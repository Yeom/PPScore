Laplacian_NNP Optimal_JJ Design_NN for_IN Image_NN Retrieval_NNP Xiaofei_NNP He_PRP Yahoo_NNP !_.
Burbank_NNP ,_, CA_NNP #####_NNP hex_NN @_IN yahoo-inc_NN ._.
com_NN Wanli_NNP Min_NNP IBM_NNP Yorktown_NNP Heights_NNP ,_, NY_NNP #####_CD wanlimin_JJ @_IN us_PRP ._.
ibm_NN ._.
com_NN Deng_NNP Cai_NNP CS_NNP Dept_NNP ._.
,_, UIUC_NNP Urbana_NNP ,_, IL_NN #####_CD dengcai2_NN @_IN cs_NNS ._.
uiuc_NN ._.
edu_NN Kun_NNP Zhou_NNP Microsoft_NNP Research_NNP Asia_NNP Beijing_NNP ,_, China_NNP kunzhou_NN @_IN microsoft_NN ._.
com_NN ABSTRACT_NN Relevance_NN feedback_NN is_VBZ a_DT powerful_JJ technique_NN to_TO enhance_VB ContentBased_NNP Image_NN Retrieval_NN -LRB-_-LRB- CBIR_NN -RRB-_-RRB- performance_NN ._.
It_PRP solicits_VBZ the_DT user_NN ''_'' s_NNS relevance_NN judgments_NNS on_IN the_DT retrieved_VBN images_NNS returned_VBN by_IN the_DT CBIR_NN systems_NNS ._.
The_DT user_NN ''_'' s_NNS labeling_VBG is_VBZ then_RB used_VBN to_TO learn_VB a_DT classifier_NN to_TO distinguish_VB between_IN relevant_JJ and_CC irrelevant_JJ images_NNS ._.
However_RB ,_, the_DT top_JJ returned_VBD images_NNS may_MD not_RB be_VB the_DT most_RBS informative_JJ ones_NNS ._.
The_DT challenge_NN is_VBZ thus_RB to_TO determine_VB which_WDT unlabeled_JJ images_NNS would_MD be_VB the_DT most_RBS informative_JJ -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, improve_VB the_DT classifier_NN the_DT most_RBS -RRB-_-RRB- if_IN they_PRP were_VBD labeled_VBN and_CC used_VBN as_IN training_NN samples_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT novel_JJ active_JJ learning_NN algorithm_NN ,_, called_VBN Laplacian_NNP Optimal_JJ Design_NN -LRB-_-LRB- LOD_NN -RRB-_-RRB- ,_, for_IN relevance_NN feedback_NN image_NN retrieval_NN ._.
Our_PRP$ algorithm_NN is_VBZ based_VBN on_IN a_DT regression_NN model_NN which_WDT minimizes_VBZ the_DT least_JJS square_JJ error_NN on_IN the_DT measured_VBN -LRB-_-LRB- or_CC ,_, labeled_VBN -RRB-_-RRB- images_NNS and_CC simultaneously_RB preserves_VBZ the_DT local_JJ geometrical_JJ structure_NN of_IN the_DT image_NN space_NN ._.
Specifically_RB ,_, we_PRP assume_VBP that_IN if_IN two_CD images_NNS are_VBP sufficiently_RB close_JJ to_TO each_DT other_JJ ,_, then_RB their_PRP$ measurements_NNS -LRB-_-LRB- or_CC ,_, labels_NNS -RRB-_-RRB- are_VBP close_RB as_RB well_RB ._.
By_IN constructing_VBG a_DT nearest_JJS neighbor_NN graph_NN ,_, the_DT geometrical_JJ structure_NN of_IN the_DT image_NN space_NN can_MD be_VB described_VBN by_IN the_DT graph_NN Laplacian_NN ._.
We_PRP discuss_VBP how_WRB results_NNS from_IN the_DT field_NN of_IN optimal_JJ experimental_JJ design_NN may_MD be_VB used_VBN to_TO guide_VB our_PRP$ selection_NN of_IN a_DT subset_NN of_IN images_NNS ,_, which_WDT gives_VBZ us_PRP the_DT most_RBS amount_VB of_IN information_NN ._.
Experimental_JJ results_NNS on_IN Corel_NNP database_NN suggest_VBP that_IN the_DT proposed_VBN approach_NN achieves_VBZ higher_JJR precision_NN in_IN relevance_NN feedback_NN image_NN retrieval_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNPS H_NN ._.
#_# ._.
#_# -LSB-_-LRB- Information_NNP storage_NN and_CC retrieval_NN -RSB-_-RRB- :_: Information_NNP search_NN and_CC retrieval-Relevance_NN feedback_NN ;_: G_NN ._.
#_# -LSB-_-LRB- Mathematics_NNS of_IN Computing_NNP -RSB-_-RRB- :_: Probability_NN and_CC Statistics-Experimental_JJ design_NN General_NNP Terms_NNS Algorithms_NNS ,_, Performance_NNP ,_, Theory_NNP 1_CD ._.
INTRODUCTION_NN In_IN many_JJ machine_NN learning_NN and_CC information_NN retrieval_NN tasks_NNS ,_, there_EX is_VBZ no_DT shortage_NN of_IN unlabeled_JJ data_NNS but_CC labels_NNS are_VBP expensive_JJ ._.
The_DT challenge_NN is_VBZ thus_RB to_TO determine_VB which_WDT unlabeled_JJ samples_NNS would_MD be_VB the_DT most_RBS informative_JJ -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, improve_VB the_DT classifier_NN the_DT most_RBS -RRB-_-RRB- if_IN they_PRP were_VBD labeled_VBN and_CC used_VBN as_IN training_NN samples_NNS ._.
This_DT problem_NN is_VBZ typically_RB called_VBN active_JJ learning_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Here_RB the_DT task_NN is_VBZ to_TO minimize_VB an_DT overall_JJ cost_NN ,_, which_WDT depends_VBZ both_DT on_IN the_DT classifier_NN accuracy_NN and_CC the_DT cost_NN of_IN data_NNS collection_NN ._.
Many_JJ real_JJ world_NN applications_NNS can_MD be_VB casted_VBN into_IN active_JJ learning_NN framework_NN ._.
Particularly_RB ,_, we_PRP consider_VBP the_DT problem_NN of_IN relevance_NN feedback_NN driven_VBN Content-Based_JJ Image_NN Retrieval_NN -LRB-_-LRB- CBIR_NN -RRB-_-RRB- -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Content-Based_JJ Image_NN Retrieval_NN has_VBZ attracted_VBN substantial_JJ interests_NNS in_IN the_DT last_JJ decade_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
It_PRP is_VBZ motivated_VBN by_IN the_DT fast_JJ growth_NN of_IN digital_JJ image_NN databases_NNS which_WDT ,_, in_IN turn_NN ,_, require_VBP efficient_JJ search_NN schemes_NNS ._.
Rather_RB than_IN describe_VBP an_DT image_NN using_VBG text_NN ,_, in_IN these_DT systems_NNS an_DT image_NN query_NN is_VBZ described_VBN using_VBG one_CD or_CC more_JJR example_NN images_NNS ._.
The_DT low_JJ level_NN visual_JJ features_NNS -LRB-_-LRB- color_NN ,_, texture_NN ,_, shape_NN ,_, etc_FW ._. -RRB-_-RRB-
are_VBP automatically_RB extracted_VBN to_TO represent_VB the_DT images_NNS ._.
However_RB ,_, the_DT low_JJ level_NN features_NNS may_MD not_RB accurately_RB characterize_VB the_DT high_JJ level_NN semantic_JJ concepts_NNS ._.
To_TO narrow_VB down_RP the_DT semantic_JJ gap_NN ,_, relevance_NN feedback_NN is_VBZ introduced_VBN into_IN CBIR_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
In_IN many_JJ of_IN the_DT current_JJ relevance_NN feedback_NN driven_VBN CBIR_NN systems_NNS ,_, the_DT user_NN is_VBZ required_VBN to_TO provide_VB his_PRP$ /_: her_PRP$ relevance_NN judgments_NNS on_IN the_DT top_JJ images_NNS returned_VBN by_IN the_DT system_NN ._.
The_DT labeled_VBN images_NNS are_VBP then_RB used_VBN to_TO train_VB a_DT classifier_NN to_TO separate_JJ images_NNS that_WDT match_VBP the_DT query_NN concept_NN from_IN those_DT that_WDT do_VBP not_RB ._.
However_RB ,_, in_IN general_JJ the_DT top_JJ returned_VBD images_NNS may_MD not_RB be_VB the_DT most_RBS informative_JJ ones_NNS ._.
In_IN the_DT worst_JJS case_NN ,_, all_PDT the_DT top_JJ images_NNS labeled_VBN by_IN the_DT user_NN may_MD be_VB positive_JJ and_CC thus_RB the_DT standard_JJ classification_NN techniques_NNS can_MD not_RB be_VB applied_VBN due_JJ to_TO the_DT lack_NN of_IN negative_JJ examples_NNS ._.
Unlike_IN the_DT standard_JJ classification_NN problems_NNS where_WRB the_DT labeled_VBN samples_NNS are_VBP pregiven_JJ ,_, in_IN relevance_NN feedback_NN image_NN retrieval_NN the_DT system_NN can_MD actively_RB select_VB the_DT images_NNS to_TO label_NN ._.
Thus_RB active_JJ learning_NN can_MD be_VB naturally_RB introduced_VBN into_IN image_NN retrieval_NN ._.
Despite_IN many_JJ existing_VBG active_JJ learning_NN techniques_NNS ,_, Support_NN Vector_NNP Machine_NN -LRB-_-LRB- SVM_NN -RRB-_-RRB- active_JJ learning_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- and_CC regression_NN based_VBN active_JJ learning_NN -LSB-_-LRB- #_# -RSB-_-RRB- have_VBP received_VBN the_DT most_RBS interests_NNS ._.
Based_VBN on_IN the_DT observation_NN that_IN the_DT closer_JJR to_TO the_DT SVM_NN boundary_NN an_DT image_NN is_VBZ ,_, the_DT less_JJR reliable_JJ its_PRP$ classification_NN is_VBZ ,_, SVM_NN active_JJ learning_NN selects_VBZ those_DT unlabeled_JJ images_NNS closest_JJS to_TO the_DT boundary_NN to_TO solicit_VB user_NN feedback_NN so_RB as_IN to_TO achieve_VB maximal_JJ refinement_NN on_IN the_DT hyperplane_NN between_IN the_DT two_CD classes_NNS ._.
The_DT major_JJ disadvantage_NN of_IN SVM_NN active_JJ learning_NN is_VBZ that_IN the_DT estimated_VBN boundary_NN may_MD not_RB be_VB accurate_JJ enough_RB ._.
Moreover_RB ,_, it_PRP may_MD not_RB be_VB applied_VBN at_IN the_DT beginning_NN of_IN the_DT retrieval_NN when_WRB there_EX is_VBZ no_DT labeled_JJ images_NNS ._.
Some_DT other_JJ SVM_NN based_VBN active_JJ learning_NN algorithms_NNS can_MD be_VB found_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN statistics_NNS ,_, the_DT problem_NN of_IN selecting_VBG samples_NNS to_TO label_NN is_VBZ typically_RB referred_VBN to_TO as_IN experimental_JJ design_NN ._.
The_DT sample_NN x_NN is_VBZ referred_VBN to_TO as_IN experiment_NN ,_, and_CC its_PRP$ label_NN y_NN is_VBZ referred_VBN to_TO as_IN measurement_NN ._.
The_DT study_NN of_IN optimal_JJ experimental_JJ design_NN -LRB-_-LRB- OED_NN -RRB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- is_VBZ concerned_VBN with_IN the_DT design_NN of_IN experiments_NNS that_WDT are_VBP expected_VBN to_TO minimize_VB variances_NNS of_IN a_DT parameterized_JJ model_NN ._.
The_DT intent_NN of_IN optimal_JJ experimental_JJ design_NN is_VBZ usually_RB to_TO maximize_VB confidence_NN in_IN a_DT given_VBN model_NN ,_, minimize_VBP parameter_NN variances_NNS for_IN system_NN identification_NN ,_, or_CC minimize_VB the_DT model_NN ''_'' s_NNS output_NN variance_NN ._.
Classical_JJ experimental_JJ design_NN approaches_NNS include_VBP A-Optimal_NNP Design_NNP ,_, D-Optimal_NNP Design_NNP ,_, and_CC E-Optimal_NNP Design_NNP ._.
All_DT of_IN these_DT approaches_NNS are_VBP based_VBN on_IN a_DT least_JJS squares_NNS regression_NN model_NN ._.
Comparing_VBG to_TO SVM_NNP based_VBN active_JJ learning_NN algorithms_NNS ,_, experimental_JJ design_NN approaches_NNS are_VBP much_RB more_RBR efficient_JJ in_IN computation_NN ._.
However_RB ,_, this_DT kind_NN of_IN approaches_NNS takes_VBZ only_RB measured_VBN -LRB-_-LRB- or_CC ,_, labeled_VBN -RRB-_-RRB- data_NNS into_IN account_NN in_IN their_PRP$ objective_JJ function_NN ,_, while_IN the_DT unmeasured_JJ -LRB-_-LRB- or_CC ,_, unlabeled_JJ -RRB-_-RRB- data_NNS is_VBZ ignored_VBN ._.
Benefit_NNP from_IN recent_JJ progresses_VBZ on_IN optimal_JJ experimental_JJ design_NN and_CC semi-supervised_JJ learning_NN ,_, in_IN this_DT paper_NN we_PRP propose_VBP a_DT novel_JJ active_JJ learning_NN algorithm_NN for_IN image_NN retrieval_NN ,_, called_VBN Laplacian_NNP Optimal_JJ Design_NN -LRB-_-LRB- LOD_NN -RRB-_-RRB- ._.
Unlike_IN traditional_JJ experimental_JJ design_NN methods_NNS whose_WP$ loss_NN functions_NNS are_VBP only_RB defined_VBN on_IN the_DT measured_VBN points_NNS ,_, the_DT loss_NN function_NN of_IN our_PRP$ proposed_VBN LOD_NNP algorithm_NN is_VBZ defined_VBN on_IN both_DT measured_VBN and_CC unmeasured_JJ points_NNS ._.
Specifically_RB ,_, we_PRP introduce_VBP a_DT locality_NN preserving_VBG regularizer_NN into_IN the_DT standard_JJ least-square-error_NN based_VBN loss_NN function_NN ._.
The_DT new_JJ loss_NN function_NN aims_VBZ to_TO find_VB a_DT classifier_NN which_WDT is_VBZ locally_RB as_IN smooth_JJ as_IN possible_JJ ._.
In_IN other_JJ words_NNS ,_, if_IN two_CD points_NNS are_VBP sufficiently_RB close_JJ to_TO each_DT other_JJ in_IN the_DT input_NN space_NN ,_, then_RB they_PRP are_VBP expected_VBN to_TO share_VB the_DT same_JJ label_NN ._.
Once_RB the_DT loss_NN function_NN is_VBZ defined_VBN ,_, we_PRP can_MD select_VB the_DT most_RBS informative_JJ data_NNS points_NNS which_WDT are_VBP presented_VBN to_TO the_DT user_NN for_IN labeling_NN ._.
It_PRP would_MD be_VB important_JJ to_TO note_VB that_IN the_DT most_RBS informative_JJ images_NNS may_MD not_RB be_VB the_DT top_JJ returned_VBD images_NNS ._.
The_DT rest_NN of_IN the_DT paper_NN is_VBZ organized_VBN as_IN follows_VBZ ._.
In_IN Section_NN 2_CD ,_, we_PRP provide_VBP a_DT brief_JJ description_NN of_IN the_DT related_JJ work_NN ._.
Our_PRP$ proposed_VBN Laplacian_NNP Optimal_JJ Design_NN algorithm_NN is_VBZ introduced_VBN in_IN Section_NN #_# ._.
In_IN Section_NN #_# ,_, we_PRP compare_VBP our_PRP$ algorithm_NN with_IN the_DT state-or-the-art_JJ algorithms_NNS and_CC present_VB the_DT experimental_JJ results_NNS on_IN image_NN retrieval_NN ._.
Finally_RB ,_, we_PRP provide_VBP some_DT concluding_VBG remarks_NNS and_CC suggestions_NNS for_IN future_JJ work_NN in_IN Section_NN #_# ._.
2_LS ._.
RELATED_JJ WORK_VBP Since_IN our_PRP$ proposed_VBN algorithm_NN is_VBZ based_VBN on_IN regression_NN framework_NN ._.
The_DT most_RBS related_JJ work_NN is_VBZ optimal_JJ experimental_JJ design_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, including_VBG A-Optimal_NNP Design_NNP ,_, D-Optimal_NNP Design_NNP ,_, and_CC EOptimal_NNP Design_NNP ._.
In_IN this_DT Section_NN ,_, we_PRP give_VBP a_DT brief_JJ description_NN of_IN these_DT approaches_NNS ._.
2_LS ._.
#_# The_DT Active_JJ Learning_NNP Problem_NNP The_NNP generic_JJ problem_NN of_IN active_JJ learning_NN is_VBZ the_DT following_VBG ._.
Given_VBN a_DT set_NN of_IN points_NNS A_NN =_JJ -LCB-_-LRB- x1_NN ,_, x2_NN ,_, ,_, xm_NN -RCB-_-RRB- in_IN Rd_NN ,_, find_VB a_DT subset_NN B_NN =_JJ -LCB-_-LRB- z1_NN ,_, z2_NN ,_, ,_, zk_NN -RCB-_-RRB- A_DT which_WDT contains_VBZ the_DT most_RBS informative_JJ points_NNS ._.
In_IN other_JJ words_NNS ,_, the_DT points_NNS zi_NN -LRB-_-LRB- i_FW =_JJ #_# ,_, ,_, k_NN -RRB-_-RRB- can_MD improve_VB the_DT classifier_NN the_DT most_RBS if_IN they_PRP are_VBP labeled_VBN and_CC used_VBN as_IN training_NN points_NNS ._.
2_LS ._.
#_# Optimal_JJ Experimental_JJ Design_NN We_PRP consider_VBP a_DT linear_JJ regression_NN model_NN y_NN =_JJ wT_NN x_NN +_CC -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB y_NN is_VBZ the_DT observation_NN ,_, x_NN is_VBZ the_DT independent_JJ variable_NN ,_, w_NN is_VBZ the_DT weight_NN vector_NN and_CC is_VBZ an_DT unknown_JJ error_NN with_IN zero_CD mean_NN ._.
Different_JJ observations_NNS have_VBP errors_NNS that_WDT are_VBP independent_JJ ,_, but_CC with_IN equal_JJ variances_NNS #_# ._.
We_PRP define_VBP f_FW -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ wT_NN x_NN to_TO be_VB the_DT learner_NN ''_'' s_NNS output_NN given_VBN input_NN x_NN and_CC the_DT weight_NN vector_NN w_NN ._.
Suppose_VB we_PRP have_VBP a_DT set_NN of_IN labeled_VBN sample_NN points_NNS -LRB-_-LRB- z1_NN ,_, y1_NN -RRB-_-RRB- ,_, ,_, -LRB-_-LRB- zk_NN ,_, yk_NN -RRB-_-RRB- ,_, where_WRB yi_NN is_VBZ the_DT label_NN of_IN zi_NN ._.
Thus_RB ,_, the_DT maximum_NN likelihood_NN estimate_NN for_IN the_DT weight_NN vector_NN ,_, w_NN ,_, is_VBZ that_IN which_WDT minimizes_VBZ the_DT sum_NN squared_VBD error_NN Jsse_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ k_NN i_FW =_JJ #_# wT_NNP zi_NNP yi_NN 2_CD -LRB-_-LRB- #_# -RRB-_-RRB- The_DT estimate_NN w_NN gives_VBZ us_PRP an_DT estimate_NN of_IN the_DT output_NN at_IN a_DT novel_JJ input_NN :_: y_NN =_JJ wT_NN x_NN ._.
By_IN Gauss-Markov_JJ theorem_NN ,_, we_PRP know_VBP that_IN w_NN w_NN has_VBZ a_DT zero_CD mean_NN and_CC a_DT covariance_NN matrix_NN given_VBN by_IN #_# H1_NN sse_NN ,_, where_WRB Hsse_NN is_VBZ the_DT Hessian_JJ of_IN Jsse_NN -LRB-_-LRB- w_NN -RRB-_-RRB- Hsse_NN =_JJ 2_CD Jsse_NN w2_NN =_JJ k_NN i_FW =_JJ #_# zizT_NN i_FW =_JJ ZZT_NN where_WRB Z_NN =_JJ -LRB-_-LRB- z1_NN ,_, z2_NN ,_, ,_, zk_NN -RRB-_-RRB- ._.
The_DT three_CD most_RBS common_JJ scalar_NN measures_NNS of_IN the_DT size_NN of_IN the_DT parameter_NN covariance_NN matrix_NN in_IN optimal_JJ experimental_JJ design_NN are_VBP :_: D-optimal_JJ design_NN :_: determinant_NN of_IN Hsse_NN ._.
A-optimal_JJ design_NN :_: trace_NN of_IN Hsse_NN ._.
E-optimal_JJ design_NN :_: maximum_NN eigenvalue_NN of_IN Hsse_NN ._.
Since_IN the_DT computation_NN of_IN the_DT determinant_NN and_CC eigenvalues_NNS of_IN a_DT matrix_NN is_VBZ much_RB more_RBR expensive_JJ than_IN the_DT computation_NN of_IN matrix_NN trace_NN ,_, A-optimal_JJ design_NN is_VBZ more_RBR efficient_JJ than_IN the_DT other_JJ two_CD ._.
Some_DT recent_JJ work_NN on_IN experimental_JJ design_NN can_MD be_VB found_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
3_LS ._.
LAPLACIAN_NNP OPTIMAL_NNP DESIGN_NN Since_IN the_DT covariance_NN matrix_NN Hsse_NN used_VBN in_IN traditional_JJ approaches_NNS is_VBZ only_RB dependent_JJ on_IN the_DT measured_VBN samples_NNS ,_, i_FW ._.
e_LS ._.
zi_NN ''_'' s_NNS ,_, these_DT approaches_NNS fail_VBP to_TO evaluate_VB the_DT expected_JJ errors_NNS on_IN the_DT unmeasured_JJ samples_NNS ._.
In_IN this_DT Section_NN ,_, we_PRP introduce_VBP a_DT novel_JJ active_JJ learning_NN algorithm_NN called_VBN Laplacian_NNP Optimal_JJ Design_NN -LRB-_-LRB- LOD_NN -RRB-_-RRB- which_WDT makes_VBZ efficient_JJ use_NN of_IN both_DT measured_VBN -LRB-_-LRB- labeled_VBN -RRB-_-RRB- and_CC unmeasured_JJ -LRB-_-LRB- unlabeled_JJ -RRB-_-RRB- samples_NNS ._.
3_LS ._.
#_# The_DT Objective_NNP Function_NN In_IN many_JJ machine_NN learning_NN problems_NNS ,_, it_PRP is_VBZ natural_JJ to_TO assume_VB that_IN if_IN two_CD points_NNS xi_NN ,_, xj_NN are_VBP sufficiently_RB close_JJ to_TO each_DT other_JJ ,_, then_RB their_PRP$ measurements_NNS -LRB-_-LRB- f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- ,_, f_FW -LRB-_-LRB- xj_NN -RRB-_-RRB- -RRB-_-RRB- are_VBP close_RB as_RB well_RB ._.
Let_VB S_NN be_VB a_DT similarity_NN matrix_NN ._.
Thus_RB ,_, a_DT new_JJ loss_NN function_NN which_WDT respects_VBZ the_DT geometrical_JJ structure_NN of_IN the_DT data_NNS space_NN can_MD be_VB defined_VBN as_IN follows_VBZ :_: J0_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ k_NN i_FW =_JJ #_# f_FW -LRB-_-LRB- zi_NN -RRB-_-RRB- yi_NN 2_CD +_CC 2_CD m_NN i_FW ,_, j_NN =_JJ #_# f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- f_FW -LRB-_-LRB- xj_NN -RRB-_-RRB- 2_CD Sij_NN -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB yi_NN is_VBZ the_DT measurement_NN -LRB-_-LRB- or_CC ,_, label_NN -RRB-_-RRB- of_IN zi_NN ._.
Note_VB that_IN ,_, the_DT loss_NN function_NN -LRB-_-LRB- #_# -RRB-_-RRB- is_VBZ essentially_RB the_DT same_JJ as_IN the_DT one_CD used_VBN in_IN Laplacian_NNP Regularized_NNP Regression_NN -LRB-_-LRB- LRR_NN ,_, -LSB-_-LRB- #_# -RSB-_-RRB- -RRB-_-RRB- ._.
However_RB ,_, LRR_NN is_VBZ a_DT passive_JJ learning_NN algorithm_NN where_WRB the_DT training_NN data_NNS is_VBZ given_VBN ._.
In_IN this_DT paper_NN ,_, we_PRP are_VBP focused_VBN on_IN how_WRB to_TO select_VB the_DT most_RBS informative_JJ data_NNS for_IN training_NN ._.
The_DT loss_NN function_NN with_IN our_PRP$ choice_NN of_IN symmetric_JJ weights_NNS Sij_NN -LRB-_-LRB- Sij_NN =_JJ Sji_NN -RRB-_-RRB- incurs_VBZ a_DT heavy_JJ penalty_NN if_IN neighboring_VBG points_NNS xi_NN and_CC xj_NN are_VBP mapped_VBN far_RB apart_RB ._.
Therefore_RB ,_, minimizing_VBG J0_NN -LRB-_-LRB- w_NN -RRB-_-RRB- is_VBZ an_DT attempt_NN to_TO ensure_VB that_IN if_IN xi_NN and_CC xj_NN are_VBP close_JJ then_RB f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- and_CC f_FW -LRB-_-LRB- xj_NN -RRB-_-RRB- are_VBP close_RB as_RB well_RB ._.
There_EX are_VBP many_JJ choices_NNS of_IN the_DT similarity_NN matrix_NN S_NN ._.
A_DT simple_JJ definition_NN is_VBZ as_IN follows_VBZ :_: Sij_NN =_JJ 1_CD ,_, if_IN xi_NN is_VBZ among_IN the_DT p_NN nearest_JJS neighbors_NNS of_IN xj_NN ,_, or_CC xj_NN is_VBZ among_IN the_DT p_NN nearest_JJS neighbors_NNS of_IN xi_NN ;_: 0_CD ,_, otherwise_RB ._.
-LRB-_-LRB- #_# -RRB-_-RRB- Let_VB D_NNP be_VB a_DT diagonal_JJ matrix_NN ,_, Dii_NN =_JJ j_NN Sij_NN ,_, and_CC L_NN =_JJ DS_NN ._.
The_DT matrix_NN L_NN is_VBZ called_VBN graph_NN Laplacian_NN in_IN spectral_JJ graph_NN theory_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Let_VB y_NN =_JJ -LRB-_-LRB- y1_NN ,_, ,_, yk_NN -RRB-_-RRB- T_NN and_CC X_NN =_JJ -LRB-_-LRB- x1_NN ,_, ,_, xm_NN -RRB-_-RRB- ._.
Following_VBG some_DT simple_JJ algebraic_JJ steps_NNS ,_, we_PRP see_VBP that_IN :_: J0_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ k_NN i_FW =_JJ #_# wT_NNP zi_NNP yi_NN 2_CD +_CC 2_CD m_NN i_FW ,_, j_NN =_JJ #_# wT_NNP xi_NNP wT_NNP xj_NN 2_CD Sij_NN =_JJ y_NN ZT_NN w_NN T_NN y_NN ZT_NN w_NN +_CC wT_NN m_NN i_FW =_JJ #_# DiixixT_NNP i_FW m_NN i_FW ,_, j_NN =_JJ #_# SijxixT_NN j_NN w_NN =_JJ yT_NN y_NN 2wT_NN Zy_NN +_CC wT_NN ZZT_NN w_NN +_CC wT_NN XDXT_NN XSXT_NN w_NN =_JJ yT_NN y_NN 2wT_NN Zy_NN +_CC wT_NN ZZT_NN +_CC XLXT_NN w_NN The_DT Hessian_JJ of_IN J0_NN -LRB-_-LRB- w_NN -RRB-_-RRB- can_MD be_VB computed_VBN as_IN follows_VBZ :_: H0_NN =_JJ 2_CD J0_NN w2_NN =_JJ ZZT_NN +_CC XLXT_NN In_IN some_DT cases_NNS ,_, the_DT matrix_NN ZZT_NN +_CC XLXT_NN is_VBZ singular_JJ -LRB-_-LRB- e_LS ._.
g_NN ._.
if_IN m_NN <_JJR d_NN -RRB-_-RRB- ._.
Thus_RB ,_, there_EX is_VBZ no_DT stable_JJ solution_NN to_TO the_DT optimization_NN problem_NN Eq_NN ._.
-LRB-_-LRB- #_# -RRB-_-RRB- ._.
A_DT common_JJ way_NN to_TO deal_VB with_IN this_DT ill-posed_JJ problem_NN is_VBZ to_TO introduce_VB a_DT Tikhonov_NNP regularizer_NN into_IN our_PRP$ loss_NN function_NN :_: J_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ k_NN i_FW =_JJ #_# wT_NNP zi_NNP yi_NN 2_CD +_CC 1_CD 2_CD m_NN i_FW ,_, j_NN =_JJ #_# wT_NNP xi_NNP wT_NNP xj_NN 2_CD Sij_NN +_CC #_# w_FW #_# -LRB-_-LRB- #_# -RRB-_-RRB- The_DT Hessian_JJ of_IN the_DT new_JJ loss_NN function_NN is_VBZ given_VBN by_IN :_: H_NN =_JJ 2_CD J_NN w2_NN =_JJ ZZT_NN +_CC 1XLXT_NN +_CC 2I_NN :_: =_JJ ZZT_NN +_CC where_WRB I_PRP is_VBZ an_DT identity_NN matrix_NN and_CC =_JJ 1XLXT_NN +_CC 2I_NN ._.
Clearly_RB ,_, H_NN is_VBZ of_IN full_JJ rank_NN ._.
Requiring_VBG that_IN the_DT gradient_NN of_IN J_NN -LRB-_-LRB- w_NN -RRB-_-RRB- with_IN respect_NN to_TO w_VB vanish_VBP gives_VBZ the_DT optimal_JJ estimate_NN w_NN :_: w_NN =_JJ H1_NN Zy_NN The_DT following_VBG proposition_NN states_VBZ the_DT bias_NN and_CC variance_NN properties_NNS of_IN the_DT estimator_NN for_IN the_DT coefficient_NN vector_NN w_NN ._.
Proposition_NN #_# ._.
#_# ._.
E_NN -LRB-_-LRB- w_NN w_NN -RRB-_-RRB- =_JJ H1_NN w_NN ,_, Cov_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ 2_CD -LRB-_-LRB- H1_NN H1_NN H1_NN -RRB-_-RRB- Proof_NN ._.
Since_IN y_NN =_JJ ZT_NN w_NN +_CC and_CC E_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ #_# ,_, it_PRP follows_VBZ that_IN E_NN -LRB-_-LRB- w_NN w_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- =_JJ H1_NN ZZT_NN w_NN w_NN =_JJ H1_NN -LRB-_-LRB- ZZT_NN +_CC -RRB-_-RRB- w_NN w_NN =_JJ -LRB-_-LRB- I_PRP H1_NN -RRB-_-RRB- w_NN w_NN =_JJ H1_NN w_NN -LRB-_-LRB- #_# -RRB-_-RRB- Notice_NNP Cov_NNP -LRB-_-LRB- y_NN -RRB-_-RRB- =_JJ #_# I_PRP ,_, the_DT covariance_NN matrix_NN of_IN w_NN has_VBZ the_DT expression_NN :_: Cov_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ H1_NN ZCov_NN -LRB-_-LRB- y_NN -RRB-_-RRB- ZT_NN H1_NN =_JJ #_# H1_NN ZZT_NN H1_NN =_JJ #_# H1_NN -LRB-_-LRB- H_NN -RRB-_-RRB- H1_NN =_JJ #_# -LRB-_-LRB- H1_NN H1_NN H1_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- Therefore_RB mean_VB squared_VBN error_NN matrix_NN for_IN the_DT coefficients_NNS w_NN is_VBZ E_NN -LRB-_-LRB- w_NN w_NN -RRB-_-RRB- -LRB-_-LRB- w_NN w_NN -RRB-_-RRB- T_NN -LRB-_-LRB- #_# -RRB-_-RRB- =_JJ H1_NN wwT_NN H1_NN +_CC #_# -LRB-_-LRB- H1_NN H1_NN H1_NN -RRB-_-RRB- -LRB-_-LRB- ##_CD -RRB-_-RRB- For_IN any_DT x_NN ,_, let_VB y_NN =_JJ wT_NN x_NN be_VB its_PRP$ predicted_VBN observation_NN ._.
The_DT expected_VBN squared_VBD prediction_NN error_NN is_VBZ E_NN -LRB-_-LRB- y_NN y_NN -RRB-_-RRB- #_# =_JJ E_NN -LRB-_-LRB- +_CC wT_NN x_NN wT_NN x_NN -RRB-_-RRB- #_# =_JJ #_# +_CC xT_NN -LSB-_-LRB- E_NN -LRB-_-LRB- w_NN w_NN -RRB-_-RRB- -LRB-_-LRB- w_NN w_NN -RRB-_-RRB- T_NN -RSB-_-RRB- x_NN =_JJ #_# +_CC xT_NN -LSB-_-LRB- H1_NN wwT_NN H1_NN +_CC #_# H1_NN #_# H1_NN H1_NN -RSB-_-RRB- x_CC Clearly_RB the_DT expected_VBN square_NN prediction_NN error_NN depends_VBZ on_IN the_DT explanatory_JJ variable_JJ x_NN ,_, therefore_RB average_JJ expected_VBN square_JJ predictive_JJ error_NN over_IN the_DT complete_JJ data_NN set_NN A_NN is_VBZ 1_CD m_NN m_NN i_FW =_JJ #_# E_NN -LRB-_-LRB- yi_NN wT_NN xi_NN -RRB-_-RRB- #_# =_SYM 1_CD m_NN m_NN i_FW =_JJ #_# xT_NNP i_FW -LSB-_-LRB- H1_NN wwT_NN H1_NN +_CC #_# H1_NN #_# H1_NN H1_NN -RSB-_-RRB- xi_NN +_CC #_# =_SYM 1_CD m_NN Tr_NN -LRB-_-LRB- XT_NN -LSB-_-LRB- #_# H1_NN +_CC H1_NN wwT_NN H1_NN #_# H1_NN H1_NN -RSB-_-RRB- X_NN -RRB-_-RRB- +_CC #_# Since_IN Tr_NN -LRB-_-LRB- XT_NN -LSB-_-LRB- H1_NN wwT_NN H1_NN #_# H1_NN H1_NN -RSB-_-RRB- X_NN -RRB-_-RRB- Tr_NN -LRB-_-LRB- #_# XT_NNP H1_NN X_NN -RRB-_-RRB- ,_, Our_PRP$ Laplacian_JJ optimality_NN criterion_NN is_VBZ thus_RB formulated_VBN by_IN minimizing_VBG the_DT trace_NN of_IN XT_NNP H1_NN X_NN ._.
Definition_NN #_# ._.
Laplacian_NNP Optimal_JJ Design_NN min_NN Z_NN =_JJ -LRB-_-LRB- z1_NN ,_, ,_, zk_NN -RRB-_-RRB- Tr_NN XT_NN ZZT_NN +_CC 1XLXT_NN +_CC 2I_NN 1_CD X_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- where_WRB z1_NN ,_, ,_, zk_NN are_VBP selected_VBN from_IN -LCB-_-LRB- x1_NN ,_, ,_, xm_NN -RCB-_-RRB- ._.
4_LS ._.
KERNEL_NN LAPLACIAN_NN OPTIMAL_JJ DESIGN_NN Canonical_JJ experimental_JJ design_NN approaches_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
A-Optimal_NNP Design_NNP ,_, D-Optimal_NNP Design_NNP ,_, and_CC E-Optimal_NN -RRB-_-RRB- only_RB consider_VB linear_JJ functions_NNS ._.
They_PRP fail_VBP to_TO discover_VB the_DT intrinsic_JJ geometry_NN in_IN the_DT data_NNS when_WRB the_DT data_NNS space_NN is_VBZ highly_RB nonlinear_JJ ._.
In_IN this_DT section_NN ,_, we_PRP describe_VBP how_WRB to_TO perform_VB Laplacian_NNP Experimental_JJ Design_NN in_IN Reproducing_VBG Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNS -RRB-_-RRB- which_WDT gives_VBZ rise_NN to_TO Kernel_NNP Laplacian_NNP Experimental_JJ Design_NN -LRB-_-LRB- KLOD_NN -RRB-_-RRB- ._.
For_IN given_VBN data_NNS points_NNS x1_NN ,_, ,_, xm_NN X_NN with_IN a_DT positive_JJ definite_JJ mercer_NN kernel_NN K_NN :_: X_NN X_NN R_NN ,_, there_EX exists_VBZ a_DT unique_JJ RKHS_NN HK_NN of_IN real_JJ valued_VBN functions_NNS on_IN X_NN ._.
Let_VB Kt_NNP -LRB-_-LRB- s_NNS -RRB-_-RRB- be_VB the_DT function_NN of_IN s_NNS obtained_VBN by_IN fixing_VBG t_NN and_CC letting_VBG Kt_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
=_JJ K_NN -LRB-_-LRB- s_NNS ,_, t_NN -RRB-_-RRB- ._.
HK_JJ consists_VBZ of_IN all_DT finite_JJ linear_JJ combinations_NNS of_IN the_DT form_NN l_NN i_FW =_JJ #_# iKti_NNS with_IN ti_NN X_NN and_CC limits_NNS of_IN such_JJ functions_NNS as_IN the_DT ti_NNS become_VBP dense_JJ in_IN X_NN ._.
We_PRP have_VBP Ks_NNS ,_, Kt_NN HK_NN =_JJ K_NN -LRB-_-LRB- s_NNS ,_, t_NN -RRB-_-RRB- ._.
4_LS ._.
#_# Derivation_NN of_IN LOD_NNP in_IN Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP Consider_VB the_DT optimization_NN problem_NN -LRB-_-LRB- #_# -RRB-_-RRB- in_IN RKHS_NN ._.
Thus_RB ,_, we_PRP seek_VBP a_DT function_NN f_FW HK_FW such_JJ that_IN the_DT following_VBG objective_NN function_NN is_VBZ minimized_VBN :_: min_NN fHK_NN k_NN i_FW =_JJ #_# f_FW -LRB-_-LRB- zi_NN -RRB-_-RRB- yi_NN 2_CD +_CC 1_CD 2_CD m_NN i_FW ,_, j_NN =_JJ #_# f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- f_FW -LRB-_-LRB- xj_NN -RRB-_-RRB- 2_CD Sij_NN +_CC #_# f_FW #_# HK_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- We_PRP have_VBP the_DT following_VBG proposition_NN ._.
Proposition_NN #_# ._.
#_# ._.
Let_VB H_NN =_JJ -LCB-_-LRB- m_NN i_FW =_JJ #_# iK_NN -LRB-_-LRB- ,_, xi_NN -RRB-_-RRB- |_CD i_FW R_NN -RCB-_-RRB- be_VB a_DT subspace_NN of_IN HK_NNP ,_, the_DT solution_NN to_TO the_DT problem_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- is_VBZ in_IN H_NN ._.
Proof_NN ._.
Let_VB H_NNP be_VB the_DT orthogonal_JJ complement_NN of_IN H_NN ,_, i_FW ._.
e_LS ._.
HK_NN =_JJ H_NN H_NN ._.
Thus_RB ,_, for_IN any_DT function_NN f_FW HK_FW ,_, it_PRP has_VBZ orthogonal_JJ decomposition_NN as_IN follows_VBZ :_: f_LS =_JJ fH_NN +_CC fH_NN Now_RB ,_, let_VB ''_'' s_NNS evaluate_VBP f_SYM at_IN xi_NN :_: f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- =_JJ f_LS ,_, Kxi_NNP HK_NNP =_JJ fH_NN +_CC fH_NN ,_, Kxi_NN HK_NN =_JJ fH_NN ,_, Kxi_NNP HK_NNP +_CC fH_NNP ,_, Kxi_NNP HK_NNP Notice_NNP that_WDT Kxi_NN H_NN while_IN fH_NN H_NN ._.
This_DT implies_VBZ that_IN fH_NN ,_, Kxi_NN HK_NN =_JJ #_# ._.
Therefore_RB ,_, f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- =_JJ fH_NN ,_, Kxi_NN HK_NN =_JJ fH_NN -LRB-_-LRB- xi_NN -RRB-_-RRB- This_DT completes_VBZ the_DT proof_NN ._.
Proposition_NN #_# ._.
#_# tells_VBZ us_PRP the_DT minimizer_NN of_IN problem_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- admits_VBZ a_DT representation_NN f_FW =_JJ m_NN i_FW =_JJ #_# iK_NN -LRB-_-LRB- ,_, xi_NN -RRB-_-RRB- ._.
Please_VB see_VB -LSB-_-LRB- #_# -RSB-_-RRB- for_IN the_DT details_NNS ._.
Let_VB :_: Rd_NN H_NN be_VB a_DT feature_NN map_NN from_IN the_DT input_NN space_NN Rd_NN to_TO H_NN ,_, and_CC K_NN -LRB-_-LRB- xi_NN ,_, xj_NN -RRB-_-RRB- =_JJ ._.
Let_VB X_NNP denote_VB the_DT data_NNS matrix_NN in_IN RKHS_NNS ,_, X_NN =_JJ -LRB-_-LRB- -LRB-_-LRB- x1_NN -RRB-_-RRB- ,_, -LRB-_-LRB- x2_NN -RRB-_-RRB- ,_, ,_, -LRB-_-LRB- xm_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Similarly_RB ,_, we_PRP define_VBP Z_NN =_JJ -LRB-_-LRB- -LRB-_-LRB- z1_NN -RRB-_-RRB- ,_, -LRB-_-LRB- z2_NN -RRB-_-RRB- ,_, ,_, -LRB-_-LRB- zk_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Thus_RB ,_, the_DT optimization_NN problem_NN in_IN RKHS_NN can_MD be_VB written_VBN as_IN follows_VBZ :_: min_NN Z_NN Tr_NN XT_NN ZZT_NN +_CC 1XLXT_NN +_CC 2I_NN 1_CD X_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- Since_IN the_DT mapping_NN function_NN is_VBZ generally_RB unknown_JJ ,_, there_EX is_VBZ no_DT direct_JJ way_NN to_TO solve_VB problem_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- ._.
In_IN the_DT following_VBG ,_, we_PRP apply_VBP kernel_NN tricks_NNS to_TO solve_VB this_DT optimization_NN problem_NN ._.
Let_VB X1_NN be_VB the_DT Moore-Penrose_NNP inverse_NN -LRB-_-LRB- also_RB known_VBN as_IN pseudo_NN inverse_NN -RRB-_-RRB- of_IN X_NN ._.
Thus_RB ,_, we_PRP have_VBP :_: XT_NN ZZT_NN +_CC 1XLXT_NN +_CC 2I_NN 1_CD X_NN =_JJ XT_NN XX1_NN ZZT_NN +_CC 1XLXT_NN +_CC 2I_NN 1_CD -LRB-_-LRB- XT_NN -RRB-_-RRB- #_# XT_NNP X_NN =_JJ XT_NN X_NN ZZT_NN X_NN +_CC 1XLXT_NN X_NN +_CC 2X_NN 1_CD -LRB-_-LRB- XT_NN -RRB-_-RRB- #_# XT_NNP X_NN =_JJ XT_NN X_NN XT_NN ZZT_NN X_NN +_CC 1XT_NN XLXT_NN X_NN +_CC 2XT_NN X_NN 1_CD XT_NN X_NN =_JJ KXX_NN KXZKZX_NN +_CC 1KXXLKXX_NN +_CC 2KXX_NN 1_CD KXX_NN where_WRB KXX_NNP is_VBZ a_DT m_NN m_NN matrix_NN -LRB-_-LRB- KXX_NN ,_, ij_NN =_JJ K_NN -LRB-_-LRB- xi_NN ,_, xj_NN -RRB-_-RRB- -RRB-_-RRB- ,_, KXZ_NN is_VBZ a_DT mk_NN matrix_NN -LRB-_-LRB- KXZ_NN ,_, ij_NN =_JJ K_NN -LRB-_-LRB- xi_NN ,_, zj_NN -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC KZX_NNP is_VBZ a_DT km_NN matrix_NN -LRB-_-LRB- KZX_NN ,_, ij_NN =_JJ K_NN -LRB-_-LRB- zi_NN ,_, xj_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Thus_RB ,_, the_DT Kernel_NNP Laplacian_NNP Optimal_JJ Design_NN can_MD be_VB defined_VBN as_IN follows_VBZ :_: Definition_NN #_# ._.
Kernel_NNP Laplacian_NNP Optimal_JJ Design_NN minZ_NN =_JJ -LRB-_-LRB- z1_NN ,_, ,_, zk_NN -RRB-_-RRB- Tr_NN KXX_NN KXZKZX_NN +_CC 1KXXLKXX_NN 2KXX_NN 1_CD KXX_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- 4_CD ._.
#_# Optimization_NNP Scheme_NNP In_IN this_DT subsection_NN ,_, we_PRP discuss_VBP how_WRB to_TO solve_VB the_DT optimization_NN problems_NNS -LRB-_-LRB- ##_NNS -RRB-_-RRB- and_CC -LRB-_-LRB- ##_CD -RRB-_-RRB- ._.
Particularly_RB ,_, if_IN we_PRP select_VBP a_DT linear_JJ kernel_NN for_IN KLOD_NNP ,_, then_RB it_PRP reduces_VBZ to_TO LOD_NNP ._.
Therefore_RB ,_, we_PRP will_MD focus_VB on_IN problem_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- in_IN the_DT following_VBG ._.
It_PRP can_MD be_VB shown_VBN that_IN the_DT optimization_NN problem_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- is_VBZ NP-hard_NN ._.
In_IN this_DT subsection_NN ,_, we_PRP develop_VBP a_DT simple_JJ sequential_JJ greedy_JJ approach_NN to_TO solve_VB -LRB-_-LRB- ##_CD -RRB-_-RRB- ._.
Suppose_VB n_NN points_NNS have_VBP been_VBN selected_VBN ,_, denoted_VBN by_IN a_DT matrix_NN Zn_NN =_JJ -LRB-_-LRB- z1_NN ,_, ,_, zn_NN -RRB-_-RRB- ._.
The_DT -LRB-_-LRB- n_NN +_CC 1_LS -RRB-_-RRB- -_: th_DT point_NN zn_NN +_CC #_# can_MD be_VB selected_VBN by_IN solving_VBG the_DT following_VBG optimization_NN problem_NN :_: max_NN Zn_NN +_CC #_# =_JJ -LRB-_-LRB- Zn_NN ,_, zn_NN +_CC #_# -RRB-_-RRB- Tr_NN KXX_NN KXZn_NN +_CC #_# KZn_NN +_CC 1X_NN +_CC 1KXXLKXX_NN +_CC 2KXX_NN 1_CD KXX_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- The_DT kernel_NN matrices_NNS KXZn_NN +_CC #_# and_CC KZn_NN +_CC 1X_NN can_MD be_VB rewritten_VBN as_IN follows_VBZ :_: KXZn_NN +_CC #_# =_JJ KXZn_NN ,_, KXzn_NN +_CC #_# ,_, KZn_NN +_CC 1X_NN =_JJ KZnX_NN Kzn_NN +_CC 1X_NN Thus_RB ,_, we_PRP have_VBP :_: KXZn_NN +_CC #_# KZn_NN +_CC 1X_NN =_JJ KXZn_NN KZnX_NN +_CC KXzn_NN +_CC #_# Kzn_NN +_CC 1X_NN We_PRP define_VBP :_: A_NN =_JJ KXZn_NN KZnX_NN +_CC 1KXXLKXX_NN +_CC 2KXX_NN A_NN is_VBZ only_RB dependent_JJ on_IN X_NN and_CC Zn_NN ._.
Thus_RB ,_, the_DT -LRB-_-LRB- n_NN +_CC #_# -RRB-_-RRB- -_: th_DT point_NN zn_NN +_CC #_# is_VBZ given_VBN by_IN :_: zn_NN +_CC #_# =_JJ arg_NN min_NN zn_NN +_CC #_# Tr_FW KXX_FW A_NN +_CC KXzn_NN +_CC #_# Kzn_NN +_CC 1X_NN 1_CD KXX_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- Each_DT time_NN we_PRP select_VBP a_DT new_JJ point_NN zn_NN +_CC #_# ,_, the_DT matrix_NN A_NN is_VBZ updated_VBN by_IN :_: A_DT A_NN +_CC KXzn_NN +_CC #_# Kzn_NN +_CC 1X_NN If_IN the_DT kernel_NN function_NN is_VBZ chosen_VBN as_IN inner_JJ product_NN K_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ x_NN ,_, y_NN ,_, then_RB HK_NN is_VBZ a_DT linear_JJ functional_JJ space_NN and_CC the_DT algorithm_NN reduces_VBZ to_TO LOD_NNP ._.
5_CD ._.
CONTENT-BASED_JJ IMAGE_NNP RETRIEVAL_NNP USING_NNP LAPLACIAN_NNP OPTIMAL_NNP DESIGN_NN In_IN this_DT section_NN ,_, we_PRP describe_VBP how_WRB to_TO apply_VB Laplacian_NNP Optimal_JJ Design_NN to_TO CBIR_NN ._.
We_PRP begin_VBP with_IN a_DT brief_JJ description_NN of_IN image_NN representation_NN using_VBG low_JJ level_NN visual_JJ features_NNS ._.
5_CD ._.
#_# Low-Level_NNP Image_NN Representation_NN Low-level_JJ image_NN representation_NN is_VBZ a_DT crucial_JJ problem_NN in_IN CBIR_NN ._.
General_NNP visual_JJ features_NNS includes_VBZ color_NN ,_, texture_NN ,_, shape_NN ,_, etc_FW ._.
Color_NNP and_CC texture_NN features_NNS are_VBP the_DT most_RBS extensively_RB used_VBN visual_JJ features_NNS in_IN CBIR_NN ._.
Compared_VBN with_IN color_NN and_CC texture_NN features_NNS ,_, shape_NN features_NNS are_VBP usually_RB described_VBN after_IN images_NNS have_VBP been_VBN segmented_JJ into_IN regions_NNS or_CC objects_NNS ._.
Since_IN robust_JJ and_CC accurate_JJ image_NN segmentation_NN is_VBZ difficult_JJ to_TO achieve_VB ,_, the_DT use_NN of_IN shape_NN features_NNS for_IN image_NN retrieval_NN has_VBZ been_VBN limited_VBN to_TO special_JJ applications_NNS where_WRB objects_NNS or_CC regions_NNS are_VBP readily_RB available_JJ ._.
In_IN this_DT work_NN ,_, We_PRP combine_VBP 64-dimensional_JJ color_NN histogram_NN and_CC 64-dimensional_JJ Color_NN Texture_NN Moment_NN -LRB-_-LRB- CTM_NN ,_, -LSB-_-LRB- ##_CD -RSB-_-RRB- -RRB-_-RRB- to_TO represent_VB the_DT images_NNS ._.
The_DT color_NN histogram_NN is_VBZ calculated_VBN using_VBG #_# #_# #_# bins_NNS in_IN HSV_NN space_NN ._.
The_DT Color_NNP Texture_NNP Moment_NNP is_VBZ proposed_VBN by_IN Yu_NNP et_FW al_FW ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- ,_, which_WDT integrates_VBZ the_DT color_NN and_CC texture_NN characteristics_NNS of_IN the_DT image_NN in_IN a_DT compact_JJ form_NN ._.
CTM_NNP adopts_VBZ local_JJ Fourier_NN transform_VB as_IN a_DT texture_NN representation_NN scheme_NN and_CC derives_VBZ eight_CD characteristic_JJ maps_NNS to_TO describe_VB different_JJ aspects_NNS of_IN co-occurrence_NN relations_NNS of_IN image_NN pixels_NNS in_IN each_DT channel_NN of_IN the_DT -LRB-_-LRB- SVcosH_NN ,_, SVsinH_NN ,_, V_NN -RRB-_-RRB- color_NN space_NN ._.
Then_RB CTM_NNP calculates_VBZ the_DT first_JJ and_CC second_JJ moment_NN of_IN these_DT maps_NNS as_IN a_DT representation_NN of_IN the_DT natural_JJ color_NN image_NN pixel_NN distribution_NN ._.
Please_VB see_VB -LSB-_-LRB- ##_CD -RSB-_-RRB- for_IN details_NNS ._.
5_CD ._.
#_# Relevance_NNP Feedback_NNP Image_NN Retrieval_NNP Relevance_NNP feedback_NN is_VBZ one_CD of_IN the_DT most_RBS important_JJ techniques_NNS to_TO narrow_VB down_RP the_DT gap_NN between_IN low_JJ level_NN visual_JJ features_NNS and_CC high_JJ level_NN semantic_JJ concepts_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Traditionally_RB ,_, the_DT user_NN ''_'' s_NNS relevance_NN feedbacks_NNS are_VBP used_VBN to_TO update_VB the_DT query_NN vector_NN or_CC adjust_VBP the_DT weighting_NN of_IN different_JJ dimensions_NNS ._.
This_DT process_NN can_MD be_VB viewed_VBN as_IN an_DT on-line_JJ learning_NN process_NN in_IN which_WDT the_DT image_NN retrieval_NN system_NN acts_VBZ as_IN a_DT learner_NN and_CC the_DT user_NN acts_VBZ as_IN a_DT teacher_NN ._.
They_PRP typical_JJ retrieval_NN process_NN is_VBZ outlined_VBN as_IN follows_VBZ :_: 1_CD ._.
The_DT user_NN submits_VBZ a_DT query_NN image_NN example_NN to_TO the_DT system_NN ._.
The_DT system_NN ranks_VBZ the_DT images_NNS in_IN database_NN according_VBG to_TO some_DT pre-defined_JJ distance_NN metric_JJ and_CC presents_VBZ to_TO the_DT user_NN the_DT top_JJ ranked_VBD images_NNS ._.
2_LS ._.
The_DT system_NN selects_VBZ some_DT images_NNS from_IN the_DT database_NN and_CC request_NN the_DT user_NN to_TO label_VB them_PRP as_IN relevant_JJ or_CC irrelevant_JJ ._.
3_LS ._.
The_DT system_NN uses_VBZ the_DT user_NN ''_'' s_VBZ provided_VBN information_NN to_TO rerank_VB the_DT images_NNS in_IN database_NN and_CC returns_NNS to_TO the_DT user_NN the_DT top_JJ images_NNS ._.
Go_VB to_TO step_VB #_# until_IN the_DT user_NN is_VBZ satisfied_JJ ._.
Our_PRP$ Laplacian_NNP Optimal_JJ Design_NN algorithm_NN is_VBZ applied_VBN in_IN the_DT second_JJ step_NN for_IN selecting_VBG the_DT most_RBS informative_JJ images_NNS ._.
Once_RB we_PRP get_VBP the_DT labels_NNS for_IN the_DT images_NNS selected_VBN by_IN LOD_NNP ,_, we_PRP apply_VBP Laplacian_NNP Regularized_NNP Regression_NN -LRB-_-LRB- LRR_NN ,_, -LSB-_-LRB- #_# -RSB-_-RRB- -RRB-_-RRB- to_TO solve_VB the_DT optimization_NN problem_NN -LRB-_-LRB- #_# -RRB-_-RRB- and_CC build_VB the_DT classifier_NN ._.
The_DT classifier_NN is_VBZ then_RB used_VBN to_TO re-rank_VB the_DT images_NNS in_IN database_NN ._.
Note_VB that_IN ,_, in_IN order_NN to_TO reduce_VB the_DT computational_JJ complexity_NN ,_, we_PRP do_VBP not_RB use_VB all_PDT the_DT unlabeled_JJ images_NNS in_IN the_DT database_NN but_CC only_RB those_DT within_IN top_JJ ###_CD returns_NNS of_IN previous_JJ iteration_NN ._.
6_CD ._.
EXPERIMENTAL_JJ RESULTS_NNS In_IN this_DT section_NN ,_, we_PRP evaluate_VBP the_DT performance_NN of_IN our_PRP$ proposed_VBN algorithm_NN on_IN a_DT large_JJ image_NN database_NN ._.
To_TO demonstrate_VB the_DT effectiveness_NN of_IN our_PRP$ proposed_VBN LOD_NNP algorithm_NN ,_, we_PRP compare_VBP it_PRP with_IN Laplacian_NNP Regularized_NNP Regression_NN -LRB-_-LRB- LRR_NN ,_, -LSB-_-LRB- #_# -RSB-_-RRB- -RRB-_-RRB- ,_, Support_NN Vector_NNP Machine_NN -LRB-_-LRB- SVM_NN -RRB-_-RRB- ,_, Support_NN Vector_NNP Machine_NNP Active_JJ Learning_NNP -LRB-_-LRB- SVMactive_JJ -RRB-_-RRB- -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, and_CC A-Optimal_NNP Design_NNP -LRB-_-LRB- AOD_NNP -RRB-_-RRB- ._.
Both_DT SVMactive_JJ ,_, AOD_NNP ,_, and_CC LOD_NNP are_VBP active_JJ learning_NN algorithms_NNS ,_, while_IN LRR_NN and_CC SVM_NN are_VBP standard_JJ classification_NN algorithms_NNS ._.
SVM_NNP only_RB makes_VBZ use_NN of_IN the_DT labeled_VBN images_NNS ,_, while_IN LRR_NN is_VBZ a_DT semi-supervised_JJ learning_NN algorithm_NN which_WDT makes_VBZ use_NN of_IN both_CC labeled_VBN and_CC unlabeled_JJ images_NNS ._.
For_IN SVMactive_JJ ,_, AOD_NNP ,_, and_CC LOD_NNP ,_, ##_CD training_NN images_NNS are_VBP selected_VBN by_IN the_DT algorithms_NNS themselves_PRP at_IN each_DT iteration_NN ._.
While_IN for_IN LRR_NN and_CC SVM_NN ,_, we_PRP use_VBP the_DT top_JJ ##_NN images_NNS as_IN training_NN data_NNS ._.
It_PRP would_MD be_VB important_JJ to_TO note_VB that_IN SVMactive_NN is_VBZ based_VBN on_IN the_DT ordinary_JJ SVM_NN ,_, LOD_NN is_VBZ based_VBN on_IN LRR_NN ,_, and_CC AOD_NNP is_VBZ based_VBN on_IN the_DT ordinary_JJ regression_NN ._.
The_DT parameters_NNS #_# and_CC #_# in_IN our_PRP$ LOD_NNP algorithm_NN are_VBP empirically_RB set_VBN to_TO be_VB #_# ._.
###_NN and_CC #_# ._.
#####_NN ._.
For_IN both_DT LRR_NN and_CC LOD_NN algorithms_NNS ,_, we_PRP use_VBP the_DT same_JJ graph_NN structure_NN and_CC set_VBD the_DT value_NN of_IN p_NN -LRB-_-LRB- number_NN of_IN nearest_JJS neighbors_NNS -RRB-_-RRB- to_TO be_VB #_# ._.
We_PRP begin_VBP with_IN a_DT simple_JJ synthetic_JJ example_NN to_TO give_VB some_DT intuition_NN about_IN how_WRB LOD_NNP works_VBZ ._.
6_CD ._.
#_# Simple_JJ Synthetic_JJ Example_NN A_DT simple_JJ synthetic_JJ example_NN is_VBZ given_VBN in_IN Figure_NNP #_# ._.
The_DT data_NN set_NN contains_VBZ two_CD circles_NNS ._.
Eight_CD points_NNS are_VBP selected_VBN by_IN AOD_NN and_CC LOD_NN ._.
As_IN can_MD be_VB seen_VBN ,_, all_PDT the_DT points_NNS selected_VBN by_IN AOD_NN are_VBP from_IN the_DT big_JJ circle_NN ,_, while_IN LOD_NN selects_VBZ four_CD points_NNS from_IN the_DT big_JJ circle_NN and_CC four_CD from_IN the_DT small_JJ circle_NN ._.
The_DT numbers_NNS beside_IN the_DT selected_VBN points_NNS denote_VBP their_PRP$ orders_NNS to_TO be_VB selected_VBN ._.
Clearly_RB ,_, the_DT points_NNS selected_VBN by_IN our_PRP$ LOD_NNP algorithm_NN can_MD better_RB represent_VB the_DT original_JJ data_NN set_NN ._.
We_PRP did_VBD not_RB compare_VB our_PRP$ algorithm_NN with_IN SVMactive_NN because_IN SVMactive_NN can_MD not_RB be_VB applied_VBN in_IN this_DT case_NN due_JJ to_TO the_DT lack_NN of_IN the_DT labeled_VBN points_NNS ._.
6_CD ._.
#_# Image_NN Retrieval_NNP Experimental_JJ Design_NN The_DT image_NN database_NN we_PRP used_VBD consists_VBZ of_IN #_# ,_, ###_CD images_NNS of_IN 79_CD semantic_JJ categories_NNS ,_, from_IN COREL_NNP data_NNS set_NN ._.
It_PRP is_VBZ a_DT large_JJ and_CC heterogeneous_JJ image_NN set_NN ._.
Each_DT image_NN is_VBZ represented_VBN as_IN a_DT 128-dimensional_JJ vector_NN as_IN described_VBN in_IN Section_NN #_# ._.
#_# ._.
Figure_NN 2_CD shows_VBZ some_DT sample_NN images_NNS ._.
To_TO exhibit_VB the_DT advantages_NNS of_IN using_VBG our_PRP$ algorithm_NN ,_, we_PRP need_VBP a_DT reliable_JJ way_NN of_IN evaluating_VBG the_DT retrieval_NN performance_NN and_CC the_DT comparisons_NNS with_IN other_JJ algorithms_NNS ._.
We_PRP list_VBP different_JJ aspects_NNS of_IN the_DT experimental_JJ design_NN below_IN ._.
6_CD ._.
#_# ._.
#_# Evaluation_NN Metrics_NNS We_PRP use_VBP precision-scope_JJ curve_NN and_CC precision_NN rate_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- to_TO evaluate_VB the_DT effectiveness_NN of_IN the_DT image_NN retrieval_NN algorithms_NNS ._.
The_DT scope_NN is_VBZ specified_VBN by_IN the_DT number_NN -LRB-_-LRB- N_NN -RRB-_-RRB- of_IN top-ranked_JJ images_NNS presented_VBN to_TO the_DT user_NN ._.
The_DT precision_NN is_VBZ the_DT ratio_NN of_IN the_DT number_NN of_IN relevant_JJ images_NNS presented_VBN to_TO the_DT user_NN to_TO the_DT -LRB-_-LRB- a_DT -RRB-_-RRB- Data_NNS set_VBN 1_CD 2_CD 3_CD 4_CD 5_CD 6_CD 7_CD 8_CD -LRB-_-LRB- b_NN -RRB-_-RRB- AOD_NN 1_CD 2_CD 3_CD 4_CD 5_CD 6_CD 7_CD 8_CD -LRB-_-LRB- c_NN -RRB-_-RRB- LOD_NN Figure_NN #_# :_: Data_NNS selection_NN by_IN active_JJ learning_NN algorithms_NNS ._.
The_DT numbers_NNS beside_IN the_DT selected_VBN points_NNS denote_VBP their_PRP$ orders_NNS to_TO be_VB selected_VBN ._.
Clearly_RB ,_, the_DT points_NNS selected_VBN by_IN our_PRP$ LOD_NNP algorithm_NN can_MD better_RB represent_VB the_DT original_JJ data_NN set_NN ._.
Note_VB that_IN ,_, the_DT SVMactive_JJ algorithm_NN can_MD not_RB be_VB applied_VBN in_IN this_DT case_NN due_JJ to_TO the_DT lack_NN of_IN labeled_VBN points_NNS ._.
-LRB-_-LRB- a_DT -RRB-_-RRB- -LRB-_-LRB- b_NN -RRB-_-RRB- -LRB-_-LRB- c_NN -RRB-_-RRB- Figure_NN #_# :_: Sample_VB images_NNS from_IN category_NN bead_NN ,_, elephant_NN ,_, and_CC ship_NN ._.
scope_NN N_NN ._.
The_DT precision-scope_JJ curve_NN describes_VBZ the_DT precision_NN with_IN various_JJ scopes_NNS and_CC thus_RB gives_VBZ an_DT overall_JJ performance_NN evaluation_NN of_IN the_DT algorithms_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT precision_NN rate_NN emphasizes_VBZ the_DT precision_NN at_IN a_DT particular_JJ value_NN of_IN scope_NN ._.
In_IN general_JJ ,_, it_PRP is_VBZ appropriate_JJ to_TO present_JJ ##_CD images_NNS on_IN a_DT screen_NN ._.
Putting_VBG more_JJR images_NNS on_IN a_DT screen_NN may_MD affect_VB the_DT quality_NN of_IN the_DT presented_VBN images_NNS ._.
Therefore_RB ,_, the_DT precision_NN at_IN top_JJ ##_NN -LRB-_-LRB- N_NN =_JJ ##_NN -RRB-_-RRB- is_VBZ especially_RB important_JJ ._.
In_IN real_JJ world_NN image_NN retrieval_NN systems_NNS ,_, the_DT query_NN image_NN is_VBZ usually_RB not_RB in_IN the_DT image_NN database_NN ._.
To_TO simulate_VB such_JJ environment_NN ,_, we_PRP use_VBP five-fold_JJ cross_NN validation_NN to_TO evaluate_VB the_DT algorithms_NNS ._.
More_RBR precisely_RB ,_, we_PRP divide_VBP the_DT whole_JJ image_NN database_NN into_IN five_CD subsets_NNS with_IN equal_JJ size_NN ._.
Thus_RB ,_, there_EX are_VBP ##_CD images_NNS per_IN category_NN in_IN each_DT subset_NN ._.
At_IN each_DT run_NN of_IN cross_NN validation_NN ,_, one_CD subset_NN is_VBZ selected_VBN as_IN the_DT query_JJ set_NN ,_, and_CC the_DT other_JJ four_CD subsets_NNS are_VBP used_VBN as_IN the_DT database_NN for_IN retrieval_NN ._.
The_DT precisionscope_JJ curve_NN and_CC precision_NN rate_NN are_VBP computed_VBN by_IN averaging_VBG the_DT results_NNS from_IN the_DT five-fold_JJ cross_NN validation_NN ._.
6_CD ._.
#_# ._.
#_# Automatic_NNP Relevance_NNP Feedback_NNP Scheme_NNP We_PRP designed_VBD an_DT automatic_JJ feedback_NN scheme_NN to_TO model_VB the_DT retrieval_NN process_NN ._.
For_IN each_DT submitted_VBN query_NN ,_, our_PRP$ system_NN retrieves_VBZ and_CC ranks_VBZ the_DT images_NNS in_IN the_DT database_NN ._.
##_NN images_NNS were_VBD selected_VBN from_IN the_DT database_NN for_IN user_NN labeling_NN and_CC the_DT label_NN information_NN is_VBZ used_VBN by_IN the_DT system_NN for_IN re-ranking_NN ._.
Note_VB that_IN ,_, the_DT images_NNS which_WDT have_VBP been_VBN selected_VBN at_IN previous_JJ iterations_NNS are_VBP excluded_VBN from_IN later_JJ selections_NNS ._.
For_IN each_DT query_NN ,_, the_DT automatic_JJ relevance_NN feedback_NN mechanism_NN is_VBZ performed_VBN for_IN four_CD iterations_NNS ._.
It_PRP is_VBZ important_JJ to_TO note_VB that_IN the_DT automatic_JJ relevance_NN feedback_NN scheme_NN used_VBN here_RB is_VBZ different_JJ from_IN the_DT ones_NNS described_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
In_IN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, the_DT top_JJ four_CD relevant_JJ and_CC irrelevant_JJ images_NNS were_VBD selected_VBN as_IN the_DT feedback_NN images_NNS ._.
However_RB ,_, this_DT may_MD not_RB be_VB practical_JJ ._.
In_IN real_JJ world_NN image_NN retrieval_NN systems_NNS ,_, it_PRP is_VBZ possible_JJ that_IN most_JJS of_IN the_DT top-ranked_JJ images_NNS are_VBP relevant_JJ -LRB-_-LRB- or_CC ,_, irrelevant_JJ -RRB-_-RRB- ._.
Thus_RB ,_, it_PRP is_VBZ difficult_JJ for_IN the_DT user_NN to_TO find_VB both_CC four_CD relevant_JJ and_CC irrelevant_JJ images_NNS ._.
It_PRP is_VBZ more_RBR reasonable_JJ for_IN the_DT users_NNS to_TO provide_VB feedback_NN information_NN only_RB on_IN the_DT ##_NN images_NNS selected_VBN by_IN the_DT system_NN ._.
6_CD ._.
#_# Image_NN Retrieval_NNP Performance_NNP In_IN real_JJ world_NN ,_, it_PRP is_VBZ not_RB practical_JJ to_TO require_VB the_DT user_NN to_TO provide_VB many_JJ rounds_NNS of_IN feedbacks_NNS ._.
The_DT retrieval_NN performance_NN after_IN the_DT first_JJ two_CD rounds_NNS of_IN feedbacks_NNS -LRB-_-LRB- especially_RB the_DT first_JJ round_NN -RRB-_-RRB- is_VBZ more_RBR important_JJ ._.
Figure_NNP #_# shows_VBZ the_DT average_JJ precision-scope_JJ curves_NNS of_IN the_DT different_JJ algorithms_NNS for_IN the_DT first_JJ two_CD feedback_NN iterations_NNS ._.
At_IN the_DT beginning_NN of_IN retrieval_NN ,_, the_DT Euclidean_JJ distances_NNS in_IN the_DT original_JJ 128-dimensional_JJ space_NN are_VBP used_VBN to_TO rank_VB the_DT images_NNS in_IN database_NN ._.
After_IN the_DT user_NN provides_VBZ relevance_NN feedbacks_NNS ,_, the_DT LRR_NNP ,_, SVM_NNP ,_, SVMactive_NNP ,_, AOD_NNP ,_, and_CC LOD_NNP algorithms_NNS are_VBP then_RB applied_VBN to_TO re-rank_VB the_DT images_NNS ._.
In_IN order_NN to_TO reduce_VB the_DT time_NN complexity_NN of_IN active_JJ learning_NN algorithms_NNS ,_, we_PRP didn_VBP ''_'' t_NN select_JJ the_DT most_RBS informative_JJ images_NNS from_IN the_DT whole_NN database_NN but_CC from_IN the_DT top_JJ ###_CD images_NNS ._.
For_IN LRR_NN and_CC SVM_NN ,_, the_DT user_NN is_VBZ required_VBN to_TO label_VB the_DT top_JJ ##_NN images_NNS ._.
For_IN SVMactive_JJ ,_, AOD_NNP ,_, and_CC LOD_NNP ,_, the_DT user_NN is_VBZ required_VBN to_TO label_VB ##_CD most_RBS informative_JJ images_NNS selected_VBN by_IN these_DT algorithms_NNS ._.
Note_VB that_IN ,_, SVMactive_NN can_MD only_RB be_VB ap_NN -LRB-_-LRB- a_DT -RRB-_-RRB- Feedback_NNP Iteration_NNP #_# -LRB-_-LRB- b_NN -RRB-_-RRB- Feedback_NN Iteration_NN #_# Figure_NNP #_# :_: The_DT average_JJ precision-scope_JJ curves_NNS of_IN different_JJ algorithms_NNS for_IN the_DT first_JJ two_CD feedback_NN iterations_NNS ._.
The_DT LOD_NNP algorithm_NN performs_VBZ the_DT best_JJS on_IN the_DT entire_JJ scope_NN ._.
Note_VB that_IN ,_, at_IN the_DT first_JJ round_NN of_IN feedback_NN ,_, the_DT SVMactive_JJ algorithm_NN can_MD not_RB be_VB applied_VBN ._.
It_PRP applies_VBZ the_DT ordinary_JJ SVM_NN to_TO build_VB the_DT initial_JJ classifier_NN ._.
-LRB-_-LRB- a_DT -RRB-_-RRB- Precision_NN at_IN Top_JJ ##_NN -LRB-_-LRB- b_NN -RRB-_-RRB- Precision_NN at_IN Top_JJ ##_NN -LRB-_-LRB- c_NN -RRB-_-RRB- Precision_NN at_IN Top_JJ ##_CD Figure_NN #_# :_: Performance_NNP evaluation_NN of_IN the_DT five_CD learning_NN algorithms_NNS for_IN relevance_NN feedback_NN image_NN retrieval_NN ._.
-LRB-_-LRB- a_DT -RRB-_-RRB- Precision_NN at_IN top_JJ ##_NNS ,_, -LRB-_-LRB- b_LS -RRB-_-RRB- Precision_NN at_IN top_JJ ##_NNS ,_, and_CC -LRB-_-LRB- c_LS -RRB-_-RRB- Precision_NN at_IN top_JJ ##_NNS ._.
As_IN can_MD be_VB seen_VBN ,_, our_PRP$ LOD_NNP algorithm_NN consistently_RB outperforms_VBZ the_DT other_JJ four_CD algorithms_NNS ._.
plied_VBN when_WRB the_DT classifier_NN is_VBZ already_RB built_VBN ._.
Therefore_RB ,_, it_PRP can_MD not_RB be_VB applied_VBN at_IN the_DT first_JJ round_NN and_CC we_PRP use_VBP the_DT standard_JJ SVM_NN to_TO build_VB the_DT initial_JJ classifier_NN ._.
As_IN can_MD be_VB seen_VBN ,_, our_PRP$ LOD_NNP algorithm_NN outperforms_VBZ the_DT other_JJ four_CD algorithms_NNS on_IN the_DT entire_JJ scope_NN ._.
Also_RB ,_, the_DT LRR_NN algorithm_NN performs_VBZ better_JJR than_IN SVM_NNP ._.
This_DT is_VBZ because_IN that_IN the_DT LRR_NN algorithm_NN makes_VBZ efficient_JJ use_NN of_IN the_DT unlabeled_JJ images_NNS by_IN incorporating_VBG a_DT locality_NN preserving_VBG regularizer_NN into_IN the_DT ordinary_JJ regression_NN objective_NN function_NN ._.
The_DT AOD_NNP algorithm_NN performs_VBZ the_DT worst_JJS ._.
As_IN the_DT scope_NN gets_VBZ larger_JJR ,_, the_DT performance_NN difference_NN between_IN these_DT algorithms_NNS gets_VBZ smaller_JJR ._.
By_IN iteratively_RB adding_VBG the_DT user_NN ''_'' s_NNS feedbacks_NNS ,_, the_DT corresponding_JJ precision_NN results_NNS -LRB-_-LRB- at_IN top_JJ ##_NNS ,_, top_JJ ##_NNS ,_, and_CC top_JJ ##_NN -RRB-_-RRB- of_IN the_DT five_CD algorithms_NNS are_VBP respectively_RB shown_VBN in_IN Figure_NNP #_# ._.
As_IN can_MD be_VB seen_VBN ,_, our_PRP$ LOD_NNP algorithm_NN performs_VBZ the_DT best_JJS in_IN all_DT the_DT cases_NNS and_CC the_DT LRR_NN algorithm_NN performs_VBZ the_DT second_JJ best_JJS ._.
Both_DT of_IN these_DT two_CD algorithms_NNS make_VBP use_NN of_IN the_DT unlabeled_JJ images_NNS ._.
This_DT shows_VBZ that_IN the_DT unlabeled_JJ images_NNS are_VBP helpful_JJ for_IN discovering_VBG the_DT intrinsic_JJ geometrical_JJ structure_NN of_IN the_DT image_NN space_NN and_CC therefore_RB enhance_VBP the_DT retrieval_NN performance_NN ._.
In_IN real_JJ world_NN ,_, the_DT user_NN may_MD not_RB be_VB willing_JJ to_TO provide_VB too_RB many_JJ relevance_NN feedbacks_NNS ._.
Therefore_RB ,_, the_DT retrieval_NN performance_NN at_IN the_DT first_JJ two_CD rounds_NNS are_VBP especially_RB important_JJ ._.
As_IN can_MD be_VB seen_VBN ,_, our_PRP$ LOD_NNP algorithm_NN achieves_VBZ #_# ._.
#_# %_NN performance_NN improvement_NN for_IN top_JJ ##_CD results_NNS ,_, #_# ._.
#_# %_NN for_IN top_JJ ##_CD results_NNS ,_, and_CC #_# ._.
#_# %_NN for_IN top_JJ ##_CD results_NNS ,_, comparing_VBG to_TO the_DT second_JJ best_JJS algorithm_NN -LRB-_-LRB- LRR_NN -RRB-_-RRB- after_IN the_DT first_JJ two_CD rounds_NNS of_IN relevance_NN feedbacks_NNS ._.
6_CD ._.
#_# Discussion_NNP Several_JJ experiments_NNS on_IN Corel_NNP database_NN have_VBP been_VBN systematically_RB performed_VBN ._.
We_PRP would_MD like_VB to_TO highlight_VB several_JJ interesting_JJ points_NNS :_: 1_CD ._.
It_PRP is_VBZ clear_JJ that_IN the_DT use_NN of_IN active_JJ learning_NN is_VBZ beneficial_JJ in_IN the_DT image_NN retrieval_NN domain_NN ._.
There_EX is_VBZ a_DT significant_JJ increase_NN in_IN performance_NN from_IN using_VBG the_DT active_JJ learning_NN methods_NNS ._.
Especially_RB ,_, out_IN of_IN the_DT three_CD active_JJ learning_VBG methods_NNS -LRB-_-LRB- SVMactive_JJ ,_, AOD_NNP ,_, LOD_NNP -RRB-_-RRB- ,_, our_PRP$ proposed_VBN LOD_NNP algorithm_NN performs_VBZ the_DT best_JJS ._.
2_LS ._.
In_IN many_JJ real_JJ world_NN applications_NNS like_IN relevance_NN feedback_NN image_NN retrieval_NN ,_, there_EX are_VBP generally_RB two_CD ways_NNS of_IN reducing_VBG labor-intensive_JJ manual_JJ labeling_NN task_NN ._.
One_CD is_VBZ active_JJ learning_NN which_WDT selects_VBZ the_DT most_RBS informative_JJ samples_NNS to_TO label_VB ,_, and_CC the_DT other_JJ is_VBZ semi-supervised_JJ learning_NN which_WDT makes_VBZ use_NN of_IN the_DT unlabeled_JJ samples_NNS to_TO enhance_VB the_DT learning_NN performance_NN ._.
Both_DT of_IN these_DT two_CD strategies_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT past_JJ -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, -LSB-_-LRB- #_# -RSB-_-RRB- ,_, -LSB-_-LRB- #_# -RSB-_-RRB- ,_, -LSB-_-LRB- #_# -RSB-_-RRB- ._.
The_DT work_NN presented_VBN in_IN this_DT paper_NN is_VBZ focused_VBN on_IN active_JJ learning_NN ,_, but_CC it_PRP also_RB takes_VBZ advantage_NN of_IN the_DT recent_JJ progresses_VBZ on_IN semi-supervised_JJ learning_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Specifically_RB ,_, we_PRP incorporate_VBP a_DT locality_NN preserving_VBG regularizer_NN into_IN the_DT standard_JJ regression_NN framework_NN and_CC find_VB the_DT most_RBS informative_JJ samples_NNS with_IN respect_NN to_TO the_DT new_JJ objective_JJ function_NN ._.
In_IN this_DT way_NN ,_, the_DT active_JJ learning_NN and_CC semi-supervised_JJ learning_NN techniques_NNS are_VBP seamlessly_RB unified_VBN for_IN learning_VBG an_DT optimal_JJ classifier_NN ._.
3_LS ._.
The_DT relevance_NN feedback_NN technique_NN is_VBZ crucial_JJ to_TO image_NN retrieval_NN ._.
For_IN all_PDT the_DT five_CD algorithms_NNS ,_, the_DT retrieval_NN performance_NN improves_VBZ with_IN more_JJR feedbacks_NNS provided_VBN by_IN the_DT user_NN ._.
7_CD ._.
CONCLUSIONS_NNS AND_CC FUTURE_NNS WORK_VBP This_DT paper_NN describes_VBZ a_DT novel_JJ active_JJ learning_NN algorithm_NN ,_, called_VBN Laplacian_NNP Optimal_JJ Design_NN ,_, to_TO enable_VB more_RBR effective_JJ relevance_NN feedback_NN image_NN retrieval_NN ._.
Our_PRP$ algorithm_NN is_VBZ based_VBN on_IN an_DT objective_JJ function_NN which_WDT simultaneously_RB minimizes_VBZ the_DT empirical_JJ error_NN and_CC preserves_VBZ the_DT local_JJ geometrical_JJ structure_NN of_IN the_DT data_NNS space_NN ._.
Using_VBG techniques_NNS from_IN experimental_JJ design_NN ,_, our_PRP$ algorithm_NN finds_VBZ the_DT most_RBS informative_JJ images_NNS to_TO label_NN ._.
These_DT labeled_VBN images_NNS and_CC the_DT unlabeled_JJ images_NNS in_IN the_DT database_NN are_VBP used_VBN to_TO learn_VB a_DT classifier_NN ._.
The_DT experimental_JJ results_NNS on_IN Corel_NN database_NN show_VBP that_IN both_CC active_JJ learning_NN and_CC semi-supervised_JJ learning_NN can_MD significantly_RB improve_VB the_DT retrieval_NN performance_NN ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT image_NN retrieval_NN problem_NN on_IN a_DT small_JJ ,_, static_JJ ,_, and_CC closed-domain_JJ image_NN data_NNS ._.
A_DT much_RB more_RBR challenging_JJ domain_NN is_VBZ the_DT World_NNP Wide_NN Web_NN -LRB-_-LRB- WWW_NN -RRB-_-RRB- ._.
For_IN Web_NN image_NN search_NN ,_, it_PRP is_VBZ possible_JJ to_TO collect_VB a_DT large_JJ amount_NN of_IN user_NN click_VBP information_NN ._.
This_DT information_NN can_MD be_VB naturally_RB used_VBN to_TO construct_VB the_DT affinity_NN graph_NN in_IN our_PRP$ algorithm_NN ._.
However_RB ,_, the_DT computational_JJ complexity_NN in_IN Web_NN scenario_NN may_MD become_VB a_DT crucial_JJ issue_NN ._.
Also_RB ,_, although_IN our_PRP$ primary_JJ interest_NN in_IN this_DT paper_NN is_VBZ focused_VBN on_IN relevance_NN feedback_NN image_NN retrieval_NN ,_, our_PRP$ results_NNS may_MD also_RB be_VB of_IN interest_NN to_TO researchers_NNS in_IN patten_NN recognition_NN and_CC machine_NN learning_NN ,_, especially_RB when_WRB a_DT large_JJ amount_NN of_IN data_NNS is_VBZ available_JJ but_CC only_RB a_DT limited_JJ samples_NNS can_MD be_VB labeled_VBN ._.
8_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
C_NN ._.
Atkinson_NNP and_CC A_NNP ._.
N_NN ._.
Donev_NNP ._.
Optimum_NNP Experimental_JJ Designs_NNS ._.
Oxford_NNP University_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- M_NN ._.
Belkin_NNP ,_, P_NN ._.
Niyogi_NNP ,_, and_CC V_NN ._.
Sindhwani_NNP ._.
Manifold_JJ regularization_NN :_: A_DT geometric_JJ framework_NN for_IN learning_VBG from_IN examples_NNS ._.
Journal_NNP of_IN Machine_NNP Learning_NNP Research_NNP ,_, #_# :_: 2399-2434_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- F_NN ._.
R_NN ._.
K_NN ._.
Chung_NNP ._.
Spectral_JJ Graph_NN Theory_NNP ,_, volume_NN ##_NN of_IN Regional_NNP Conference_NNP Series_NNP in_IN Mathematics_NNP ._.
AMS_NNP ,_, 1997_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
A_DT ._.
Cohn_NNP ,_, Z_NN ._.
Ghahramani_NNP ,_, and_CC M_NN ._.
I_PRP ._.
Jordan_NNP ._.
Active_JJ learning_NN with_IN statistical_JJ models_NNS ._.
Journal_NNP of_IN Artificial_NNP Intelligence_NNP Research_NNP ,_, #_# :_: 129-145_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
Dong_NNP and_CC B_NNP ._.
Bhanu_NNP ._.
A_DT new_JJ semi-supervised_JJ em_NN algorithm_NN for_IN image_NN retrieval_NN ._.
In_IN IEEE_NNP Conf_NNP ._.
on_IN Computer_NNP Vision_NNP and_CC Pattern_NNP Recognition_NN ,_, Madison_NNP ,_, WI_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- P_NN ._.
Flaherty_NNP ,_, M_NN ._.
I_PRP ._.
Jordan_NNP ,_, and_CC A_NN ._.
P_NN ._.
Arkin_NNP ._.
Robust_JJ design_NN of_IN biological_JJ experiments_NNS ._.
In_IN Advances_NNS in_IN Neural_NNP Information_NNP Processing_NNP Systems_NNPS ##_NN ,_, Vancouver_NNP ,_, Canada_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- K_NNP ._.
-_: S_NN ._.
Goh_NNP ,_, E_NNP ._.
Y_NN ._.
Chang_NNP ,_, and_CC W_NN ._.
-_: C_NN ._.
Lai_NNP ._.
Multimodal_JJ concept-dependent_JJ active_JJ learning_NN for_IN image_NN retrieval_NN ._.
In_IN Proceedings_NNP of_IN the_DT ACM_NNP Conference_NN on_IN Multimedia_NNP ,_, New_NNP York_NNP ,_, October_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- X_NN ._.
He_PRP ._.
Incremental_JJ semi-supervised_JJ subspace_NN learning_VBG for_IN image_NN retrieval_NN ._.
In_IN Proceedings_NNP of_IN the_DT ACM_NNP Conference_NN on_IN Multimedia_NNP ,_, New_NNP York_NNP ,_, October_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
C_NN ._.
Hoi_NNP and_CC M_NN ._.
R_NN ._.
Lyu_NNP ._.
A_DT semi-supervised_JJ active_JJ learning_NN framework_NN for_IN image_NN retrieval_NN ._.
In_IN IEEE_NNP International_NNP Conference_NNP on_IN Computer_NNP Vision_NNP and_CC Pattern_NNP Recognition_NN ,_, San_NNP Diego_NNP ,_, CA_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
P_NN ._.
Huijsmans_NNPS and_CC N_NN ._.
Sebe_NNP ._.
How_WRB to_TO complete_VB performance_NN graphs_NNS in_IN content-based_JJ image_NN retrieval_NN :_: Add_VB generality_NN and_CC normalize_VB scope_NN ._.
IEEE_NNP Transactions_NNS on_IN Pattern_NNP Analysis_NN and_CC Machine_NN Intelligence_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 245-251_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
-_: Y_NN ._.
Lin_NNP ,_, T_NN ._.
-_: L_NN ._.
Liu_NNP ,_, and_CC H_NN ._.
-_: T_NN ._.
Chen_NNP ._.
Semantic_JJ manifold_NN learning_VBG for_IN image_NN retrieval_NN ._.
In_IN Proceedings_NNP of_IN the_DT ACM_NNP Conference_NN on_IN Multimedia_NNP ,_, Singapore_NNP ,_, November_NNP ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Rui_NNP ,_, T_NN ._.
S_NN ._.
Huang_NNP ,_, M_NN ._.
Ortega_NNP ,_, and_CC S_NN ._.
Mehrotra_NNP ._.
Relevance_NN feedback_NN :_: A_DT power_NN tool_NN for_IN interative_JJ content-based_JJ image_NN retrieval_NN ._.
IEEE_NNP Transactions_NNS on_IN Circuits_NNS and_CC Systems_NNPS for_IN Video_NNP Technology_NNP ,_, #_# -LRB-_-LRB- #_# -RRB-_-RRB- ,_, ####_NN ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- A_DT ._.
W_NN ._.
Smeulders_NNP ,_, M_NN ._.
Worring_VBG ,_, S_NN ._.
Santini_NNP ,_, A_NNP ._.
Gupta_NNP ,_, and_CC R_NN ._.
Jain_NNP ._.
Content-based_JJ image_NN retrieval_NN at_IN the_DT end_NN of_IN the_DT early_JJ years_NNS ._.
IEEE_NNP Transactions_NNS on_IN Pattern_NNP Analysis_NN and_CC Machine_NN Intelligence_NNP ,_, ##_CD -LRB-_-LRB- ##_CD -RRB-_-RRB- :_: 1349-1380_CD ,_, 2000_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Tong_NNP and_CC E_NNP ._.
Chang_NNP ._.
Support_NN vector_NN machine_NN active_JJ learning_NN for_IN image_NN retrieval_NN ._.
In_IN Proceedings_NNP of_IN the_DT ninth_JJ ACM_JJ international_JJ conference_NN on_IN Multimedia_NNP ,_, pages_NNS 107-118_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- H_NN ._.
Yu_NNP ,_, M_NN ._.
Li_NNP ,_, H_NN ._.
-_: J_NN ._.
Zhang_NNP ,_, and_CC J_NN ._.
Feng_NNP ._.
Color_NNP texture_NN moments_NNS for_IN content-based_JJ image_NN retrieval_NN ._.
In_IN International_NNP Conference_NNP on_IN Image_NN Processing_NN ,_, pages_NNS 24-28_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- K_NN ._.
Yu_NNP ,_, J_NNP ._.
Bi_NNP ,_, and_CC V_NN ._.
Tresp_NNP ._.
Active_JJ learning_NN via_IN transductive_JJ experimental_JJ design_NN ._.
In_IN Proceedings_NNP of_IN the_DT 23rd_JJ International_NNP Conference_NN on_IN Machine_NN Learning_NNP ,_, Pittsburgh_NNP ,_, PA_NN ,_, ####_CD ._.
