An_DT Adversarial_NNP Environment_NNP Model_NNP for_IN Bounded_NNP Rational_NNP Agents_NNPS in_IN Zero-Sum_NNP Interactions_NNS Inon_VBP Zuckerman1_NN ,_, Sarit_NNP Kraus1_NNP ,_, Jeffrey_NNP S_NN ._.
Rosenschein2_NN ,_, Gal_NN Kaminka1_NN 1_CD Department_NNP of_IN Computer_NNP Science_NNP #_# The_DT School_NN of_IN Engineering_NNP Bar-Ilan_NNP University_NNP and_CC Computer_NNP Science_NNP Ramat-Gan_NNP ,_, Israel_NNP Hebrew_NNP University_NNP ,_, Jerusalem_NNP ,_, Israel_NNP -LCB-_-LRB- zukermi_NNS ,_, sarit_NN ,_, galk_NN -RCB-_-RRB- @_SYM cs_NNS ._.
biu_NN ._.
ac_NN ._.
il_NN jeff_NN @_IN cs_NNS ._.
huji_NNS ._.
ac_NN ._.
il_NN ABSTRACT_NN Multiagent_JJ environments_NNS are_VBP often_RB not_RB cooperative_JJ nor_CC collaborative_JJ ;_: in_IN many_JJ cases_NNS ,_, agents_NNS have_VBP conflicting_VBG interests_NNS ,_, leading_VBG to_TO adversarial_JJ interactions_NNS ._.
This_DT paper_NN presents_VBZ a_DT formal_JJ Adversarial_NNP Environment_NNP model_NN for_IN bounded_VBN rational_JJ agents_NNS operating_VBG in_IN a_DT zero-sum_JJ environment_NN ._.
In_IN such_JJ environments_NNS ,_, attempts_VBZ to_TO use_VB classical_JJ utility-based_JJ search_NN methods_NNS can_MD raise_VB a_DT variety_NN of_IN difficulties_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, implicitly_RB modeling_NN the_DT opponent_NN as_IN an_DT omniscient_JJ utility_NN maximizer_NN ,_, rather_RB than_IN leveraging_VBG a_DT more_RBR nuanced_JJ ,_, explicit_JJ opponent_NN model_NN -RRB-_-RRB- ._.
We_PRP define_VBP an_DT Adversarial_NNP Environment_NNP by_IN describing_VBG the_DT mental_JJ states_NNS of_IN an_DT agent_NN in_IN such_JJ an_DT environment_NN ._.
We_PRP then_RB present_JJ behavioral_JJ axioms_NNS that_WDT are_VBP intended_VBN to_TO serve_VB as_IN design_NN principles_NNS for_IN building_VBG such_JJ adversarial_JJ agents_NNS ._.
We_PRP explore_VBP the_DT application_NN of_IN our_PRP$ approach_NN by_IN analyzing_VBG log_NN files_NNS of_IN completed_VBN Connect-Four_NNP games_NNS ,_, and_CC present_VB an_DT empirical_JJ analysis_NN of_IN the_DT axioms_NNS ''_'' appropriateness_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNS I_PRP ._.
#_# ._.
##_NN -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Distributed_VBN Artificial_JJ Intelligence-Intelligent_JJ agents_NNS ,_, Multiagent_NNP Systems_NNPS ;_: I_PRP ._.
#_# ._.
#_# -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Knowledge_NN Representation_NN Formalisms_NNS and_CC Methods_NNS -_: Modal_JJ logic_NN General_NNP Terms_NNS Design_NN ,_, Theory_NNP 1_CD ._.
INTRODUCTION_NNP Early_NNP research_NN in_IN multiagent_JJ systems_NNS -LRB-_-LRB- MAS_NNS -RRB-_-RRB- considered_VBN cooperative_JJ groups_NNS of_IN agents_NNS ;_: because_IN individual_JJ agents_NNS had_VBD limited_VBN resources_NNS ,_, or_CC limited_JJ access_NN to_TO information_NN -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, limited_JJ processing_NN power_NN ,_, limited_JJ sensor_NN coverage_NN -RRB-_-RRB- ,_, they_PRP worked_VBD together_RB by_IN design_NN to_TO solve_VB problems_NNS that_WDT individually_RB they_PRP could_MD not_RB solve_VB ,_, or_CC at_IN least_JJS could_MD not_RB solve_VB as_IN efficiently_RB ._.
MAS_NNP research_NN ,_, however_RB ,_, soon_RB began_VBD to_TO consider_VB interacting_VBG agents_NNS with_IN individuated_JJ interests_NNS ,_, as_IN representatives_NNS of_IN different_JJ humans_NNS or_CC organizations_NNS with_IN non-identical_JJ interests_NNS ._.
When_WRB interactions_NNS are_VBP guided_VBN by_IN diverse_JJ interests_NNS ,_, participants_NNS may_MD have_VB to_TO overcome_VB disagreements_NNS ,_, uncooperative_JJ interactions_NNS ,_, and_CC even_RB intentional_JJ attempts_NNS to_TO damage_VB one_CD another_DT ._.
When_WRB these_DT types_NNS of_IN interactions_NNS occur_VBP ,_, environments_NNS require_VBP appropriate_JJ behavior_NN from_IN the_DT agents_NNS situated_VBN in_IN them_PRP ._.
We_PRP call_VBP these_DT environments_NNS Adversarial_JJ Environments_NNS ,_, and_CC call_VB the_DT clashing_NN agents_NNS Adversaries_NNS ._.
Models_NNS of_IN cooperation_NN and_CC teamwork_NN have_VBP been_VBN extensively_RB explored_VBN in_IN MAS_NNP through_IN the_DT axiomatization_NN of_IN mental_JJ states_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, -LSB-_-LRB- #_# ,_, #_# ,_, #_# -RSB-_-RRB- -RRB-_-RRB- ._.
However_RB ,_, none_NN of_IN this_DT research_NN dealt_VBN with_IN adversarial_JJ domains_NNS and_CC their_PRP$ implications_NNS for_IN agent_NN behavior_NN ._.
Our_PRP$ paper_NN addresses_NNS this_DT issue_NN by_IN providing_VBG a_DT formal_JJ ,_, axiomatized_JJ mental_JJ state_NN model_NN for_IN a_DT subset_NN of_IN adversarial_JJ domains_NNS ,_, namely_RB simple_JJ zero-sum_JJ adversarial_JJ environments_NNS ._.
Simple_JJ zero-sum_JJ encounters_NNS exist_VBP of_IN course_NN in_IN various_JJ twoplayer_NN games_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, Chess_NNP ,_, Checkers_NNP -RRB-_-RRB- ,_, but_CC they_PRP also_RB exist_VBP in_IN n-player_NN games_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, Risk_NN ,_, Diplomacy_NNP -RRB-_-RRB- ,_, auctions_NNS for_IN a_DT single_JJ good_JJ ,_, and_CC elsewhere_RB ._.
In_IN these_DT latter_JJ environments_NNS especially_RB ,_, using_VBG a_DT utility-based_JJ adversarial_JJ search_NN -LRB-_-LRB- such_JJ as_IN the_DT Min-Max_NNP algorithm_NN -RRB-_-RRB- does_VBZ not_RB always_RB provide_VB an_DT adequate_JJ solution_NN ;_: the_DT payoff_NN function_NN might_MD be_VB quite_RB complex_JJ or_CC difficult_JJ to_TO quantify_VB ,_, and_CC there_EX are_VBP natural_JJ computational_JJ limitations_NNS on_IN bounded_VBN rational_JJ agents_NNS ._.
In_IN addition_NN ,_, traditional_JJ search_NN methods_NNS -LRB-_-LRB- like_IN Min-Max_NNP -RRB-_-RRB- do_VBP not_RB make_VB use_NN of_IN a_DT model_NN of_IN the_DT opponent_NN ,_, which_WDT has_VBZ proven_VBN to_TO be_VB a_DT valuable_JJ addition_NN to_TO adversarial_JJ planning_NN -LSB-_-LRB- #_# ,_, #_# ,_, ##_NN -RSB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP develop_VBP a_DT formal_JJ ,_, axiomatized_JJ model_NN for_IN bounded_VBN rational_JJ agents_NNS that_WDT are_VBP situated_VBN in_IN a_DT zero-sum_JJ adversarial_JJ environment_NN ._.
The_DT model_NN uses_VBZ different_JJ modality_NN operators_NNS ,_, and_CC its_PRP$ main_JJ foundations_NNS are_VBP the_DT SharedPlans_NNPS -LSB-_-LRB- #_# -RSB-_-RRB- model_NN for_IN collaborative_JJ behavior_NN ._.
We_PRP explore_VBP environment_NN properties_NNS and_CC the_DT mental_JJ states_NNS of_IN agents_NNS to_TO derive_VB behavioral_JJ axioms_NNS ;_: these_DT behavioral_JJ axioms_NNS constitute_VBP a_DT formal_JJ model_NN that_WDT serves_VBZ as_IN a_DT specification_NN and_CC design_NN guideline_NN for_IN agent_NN design_NN in_IN such_JJ settings_NNS ._.
We_PRP then_RB investigate_VBP the_DT behavior_NN of_IN our_PRP$ model_NN empirically_RB using_VBG the_DT Connect-Four_NNP board_NN game_NN ._.
We_PRP show_VBP that_IN this_DT game_NN conforms_VBZ to_TO our_PRP$ environment_NN definition_NN ,_, and_CC analyze_VBP players_NNS ''_'' behavior_NN using_VBG a_DT large_JJ set_NN of_IN completed_VBN match_NN log_NN 550_CD 978-81-904262-7-5_CD -LRB-_-LRB- RPS_NN -RRB-_-RRB- c_NN ####_CD IFAAMAS_NNP files_NNS ._.
In_IN addition_NN ,_, we_PRP use_VBP the_DT results_NNS presented_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- to_TO discuss_VB the_DT importance_NN of_IN opponent_NN modeling_NN in_IN our_PRP$ ConnectFour_NNP adversarial_JJ domain_NN ._.
The_DT paper_NN proceeds_VBZ as_IN follows_VBZ ._.
Section_NN #_# presents_VBZ the_DT model_NN ''_'' s_NNS formalization_NN ._.
Section_NN #_# presents_VBZ the_DT empirical_JJ analysis_NN and_CC its_PRP$ results_NNS ._.
We_PRP discuss_VBP related_JJ work_NN in_IN Section_NN #_# ,_, and_CC conclude_VBP and_CC present_JJ future_JJ directions_NNS in_IN Section_NN #_# ._.
2_LS ._.
ADVERSARIAL_JJ ENVIRONMENTS_NNS The_DT adversarial_JJ environment_NN model_NN -LRB-_-LRB- denoted_VBN as_IN AE_NN -RRB-_-RRB- is_VBZ intended_VBN to_TO guide_VB the_DT design_NN of_IN agents_NNS by_IN providing_VBG a_DT specification_NN of_IN the_DT capabilities_NNS and_CC mental_JJ attitudes_NNS of_IN an_DT agent_NN in_IN an_DT adversarial_JJ environment_NN ._.
We_PRP focus_VBP here_RB on_IN specific_JJ types_NNS of_IN adversarial_JJ environments_NNS ,_, specified_VBN as_IN follows_VBZ :_: 1_CD ._.
Zero-Sum_NNP Interactions_NNS :_: positive_JJ and_CC negative_JJ utilities_NNS of_IN all_DT agents_NNS sum_NN to_TO zero_CD ;_: 2_CD ._.
Simple_JJ AEs_NNS :_: all_DT agents_NNS in_IN the_DT environment_NN are_VBP adversarial_JJ agents_NNS ;_: 3_LS ._.
Bilateral_JJ AEs_NNS :_: AE_NN ''_'' s_NNS with_IN exactly_RB two_CD agents_NNS ;_: 4_LS ._.
Multilateral_NNP AEs_NNP ''_'' :_: AE_NN ''_'' s_NNS of_IN three_CD or_CC more_JJR agents_NNS ._.
We_PRP will_MD work_VB on_IN both_DT bilateral_JJ and_CC multilateral_JJ instantiations_NNS of_IN zero-sum_JJ and_CC simple_JJ environments_NNS ._.
In_IN particular_JJ ,_, our_PRP$ adversarial_JJ environment_NN model_NN will_MD deal_VB with_IN interactions_NNS that_WDT consist_VBP of_IN N_NN agents_NNS -LRB-_-LRB- N_NN #_# -RRB-_-RRB- ,_, where_WRB all_DT agents_NNS are_VBP adversaries_NNS ,_, and_CC only_RB one_CD agent_NN can_MD succeed_VB ._.
Examples_NNS of_IN such_JJ environments_NNS range_VBP from_IN board_NN games_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, Chess_NNP ,_, Connect-Four_NNP ,_, and_CC Diplomacy_NNP -RRB-_-RRB- to_TO certain_JJ economic_JJ environments_NNS -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, N-bidder_NN auctions_NNS over_IN a_DT single_JJ good_JJ -RRB-_-RRB- ._.
2_LS ._.
#_# Model_NNP Overview_NNP Our_PRP$ approach_NN is_VBZ to_TO formalize_VB the_DT mental_JJ attitudes_NNS and_CC behaviors_NNS of_IN a_DT single_JJ adversarial_JJ agent_NN ;_: we_PRP consider_VBP how_WRB a_DT single_JJ agent_NN perceives_VBZ the_DT AE_NN ._.
The_DT following_VBG list_NN specifies_VBZ the_DT conditions_NNS and_CC mental_JJ states_NNS of_IN an_DT agent_NN in_IN a_DT simple_JJ ,_, zero-sum_JJ AE_NNP :_: 1_CD ._.
The_DT agent_NN has_VBZ an_DT individual_JJ intention_NN that_IN its_PRP$ own_JJ goal_NN will_MD be_VB completed_VBN ;_: 2_LS ._.
The_DT agent_NN has_VBZ an_DT individual_JJ belief_NN that_IN it_PRP and_CC its_PRP$ adversaries_NNS are_VBP pursuing_VBG full_JJ conflicting_VBG goals_NNS -LRB-_-LRB- defined_VBN below_IN -RRB-_-RRB- there_EX can_MD be_VB only_RB one_CD winner_NN ;_: 3_LS ._.
The_DT agent_NN has_VBZ an_DT individual_JJ belief_NN that_IN each_DT adversary_NN has_VBZ an_DT intention_NN to_TO complete_VB its_PRP$ own_JJ full_JJ conflicting_JJ goal_NN ;_: 4_LS ._.
The_DT agent_NN has_VBZ an_DT individual_JJ belief_NN in_IN the_DT -LRB-_-LRB- partial_JJ -RRB-_-RRB- profile_NN of_IN its_PRP$ adversaries_NNS ._.
Item_NN #_# is_VBZ required_VBN ,_, since_IN it_PRP might_MD be_VB the_DT case_NN that_IN some_DT agent_NN has_VBZ a_DT full_JJ conflicting_JJ goal_NN ,_, and_CC is_VBZ currently_RB considering_VBG adopting_VBG the_DT intention_NN to_TO complete_VB it_PRP ,_, but_CC is_VBZ ,_, as_IN of_IN yet_RB ,_, not_RB committed_VBN to_TO achieving_VBG it_PRP ._.
This_DT might_MD occur_VB because_IN the_DT agent_NN has_VBZ not_RB yet_RB deliberated_VBN about_IN the_DT effects_NNS that_WDT adopting_VBG that_IN intention_NN might_MD have_VB on_IN the_DT other_JJ intentions_NNS it_PRP is_VBZ currently_RB holding_VBG ._.
In_IN such_JJ cases_NNS ,_, it_PRP might_MD not_RB consider_VB itself_PRP to_TO even_RB be_VB in_IN an_DT adversarial_JJ environment_NN ._.
Item_NN #_# states_NNS that_IN the_DT agent_NN should_MD hold_VB some_DT belief_NN about_IN the_DT profiles_NNS of_IN its_PRP$ adversaries_NNS ._.
The_DT profile_NN represents_VBZ all_PDT the_DT knowledge_NN the_DT agent_NN has_VBZ about_IN its_PRP$ adversary_NN :_: its_PRP$ weaknesses_NNS ,_, strategic_JJ capabilities_NNS ,_, goals_NNS ,_, intentions_NNS ,_, trustworthiness_NN ,_, and_CC more_RBR ._.
It_PRP can_MD be_VB given_VBN explicitly_RB or_CC can_MD be_VB learned_VBN from_IN observations_NNS of_IN past_JJ encounters_NNS ._.
2_LS ._.
#_# Model_NNP Definitions_NNPS for_IN Mental_NNP States_NNPS We_PRP use_VBP Grosz_NNP and_CC Kraus_NNP ''_'' s_VBZ definitions_NNS of_IN the_DT modal_JJ operators_NNS ,_, predicates_VBZ ,_, and_CC meta-predicates_NNS ,_, as_IN defined_VBN in_IN their_PRP$ SharedPlan_NNP formalization_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
We_PRP recall_VBP here_RB some_DT of_IN the_DT predicates_NNS and_CC operators_NNS that_WDT are_VBP used_VBN in_IN that_DT formalization_NN :_: Int_NN ._.
To_TO -LRB-_-LRB- Ai_NNP ,_, ,_, Tn_NN ,_, T_NN ,_, C_NN -RRB-_-RRB- represents_VBZ Ai_NNP ''_'' s_VBZ intentions_NNS at_IN time_NN Tn_NN to_TO do_VB an_DT action_NN at_IN time_NN T_NN in_IN the_DT context_NN of_IN C_NN ._.
Int_NN ._.
Th_NN -LRB-_-LRB- Ai_NN ,_, prop_VBP ,_, Tn_NNP ,_, Tprop_NNP ,_, C_NNP -RRB-_-RRB- represents_VBZ Ai_NNP ''_'' s_VBZ intentions_NNS at_IN time_NN Tn_NN that_IN a_DT certain_JJ proposition_NN prop_VBP holds_VBZ at_IN time_NN Tprop_NN in_IN the_DT context_NN of_IN C_NN ._.
The_DT potential_JJ intention_NN operators_NNS ,_, Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- ..._: -RRB-_-RRB- and_CC Pot_NNP ._.
Int_NN ._.
Th_NN -LRB-_-LRB- ..._: -RRB-_-RRB- ,_, are_VBP used_VBN to_TO represent_VB the_DT mental_JJ state_NN when_WRB an_DT agent_NN considers_VBZ adopting_VBG an_DT intention_NN ,_, but_CC has_VBZ not_RB deliberated_VBN about_IN the_DT interaction_NN of_IN the_DT other_JJ intentions_NNS it_PRP holds_VBZ ._.
The_DT operator_NN Bel_NNP -LRB-_-LRB- Ai_NNP ,_, f_FW ,_, Tf_NN -RRB-_-RRB- represents_VBZ agent_NN Ai_NN believing_VBG in_IN the_DT statement_NN expressed_VBN in_IN formula_NN f_FW ,_, at_IN time_NN Tf_NN ._.
MB_NN -LRB-_-LRB- A_NN ,_, f_FW ,_, Tf_NN -RRB-_-RRB- represents_VBZ mutual_JJ belief_NN for_IN a_DT group_NN of_IN agents_NNS A_DT ._.
A_DT snapshot_NN of_IN the_DT system_NN finds_VBZ our_PRP$ environment_NN to_TO be_VB in_IN some_DT state_NN e_SYM E_NN of_IN environmental_JJ variable_JJ states_NNS ,_, and_CC each_DT adversary_NN in_IN any_DT LAi_NN L_NN of_IN possible_JJ local_JJ states_NNS ._.
At_IN any_DT given_VBN time_NN step_NN ,_, the_DT system_NN will_MD be_VB in_IN some_DT world_NN w_NN of_IN the_DT set_NN of_IN all_DT possible_JJ worlds_NNS w_VBP W_NN ,_, where_WRB w_NN =_JJ ELA1_NN LA2_NN ..._: LAn_NN ,_, and_CC n_NN is_VBZ the_DT number_NN of_IN adversaries_NNS ._.
For_IN example_NN ,_, in_IN a_DT Texas_NNP Hold_VB ''_'' em_NN poker_NN game_NN ,_, an_DT agent_NN ''_'' s_NNS local_JJ state_NN might_MD be_VB its_PRP$ own_JJ set_NN of_IN cards_NNS -LRB-_-LRB- which_WDT is_VBZ unknown_JJ to_TO its_PRP$ adversary_NN -RRB-_-RRB- while_IN the_DT environment_NN will_MD consist_VB of_IN the_DT betting_VBG pot_NN and_CC the_DT community_NN cards_NNS -LRB-_-LRB- which_WDT are_VBP visible_JJ to_TO both_DT players_NNS -RRB-_-RRB- ._.
A_DT utility_NN function_NN under_IN this_DT formalization_NN is_VBZ defined_VBN as_IN a_DT mapping_NN from_IN a_DT possible_JJ world_NN w_NN W_NN to_TO an_DT element_NN in_IN ,_, which_WDT expresses_VBZ the_DT desirability_NN of_IN the_DT world_NN ,_, from_IN a_DT single_JJ agent_NN perspective_NN ._.
We_PRP usually_RB normalize_VBP the_DT range_NN to_TO -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ,_, where_WRB #_# represents_VBZ the_DT least_JJS desirable_JJ possible_JJ world_NN ,_, and_CC 1_CD is_VBZ the_DT most_RBS desirable_JJ world_NN ._.
The_DT implementation_NN of_IN the_DT utility_NN function_NN is_VBZ dependent_JJ on_IN the_DT domain_NN in_IN question_NN ._.
The_DT following_VBG list_NN specifies_VBZ new_JJ predicates_NNS ,_, functions_NNS ,_, variables_NNS ,_, and_CC constants_NNS used_VBN in_IN conjunction_NN with_IN the_DT original_JJ definitions_NNS for_IN the_DT adversarial_JJ environment_NN formalization_NN :_: 1_CD ._.
is_VBZ a_DT null_JJ action_NN -LRB-_-LRB- the_DT agent_NN does_VBZ not_RB do_VB anything_NN -RRB-_-RRB- ._.
2_LS ._.
GAi_NN is_VBZ the_DT set_NN of_IN agent_NN Ai_NN ''_'' s_NNS goals_NNS ._.
Each_DT goal_NN is_VBZ a_DT set_NN of_IN predicates_NNS whose_WP$ satisfaction_NN makes_VBZ the_DT goal_NN complete_JJ -LRB-_-LRB- we_PRP use_VBP G_NN Ai_NNS GAi_NN to_TO represent_VB an_DT arbitrary_JJ goal_NN of_IN agent_NN Ai_NN -RRB-_-RRB- ._.
3_LS ._.
gAi_NN is_VBZ the_DT set_NN of_IN agent_NN Ai_NN ''_'' s_NNS subgoals_NNS ._.
Subgoals_NNS are_VBP predicates_NNS whose_WP$ satisfaction_NN represents_VBZ an_DT important_JJ milestone_NN toward_IN achievement_NN of_IN the_DT full_JJ goal_NN ._.
gG_NN Ai_NN gAi_NN is_VBZ the_DT set_NN of_IN subgoals_NNS that_WDT are_VBP important_JJ to_TO the_DT completion_NN of_IN goal_NN G_NN Ai_NNS -LRB-_-LRB- we_PRP will_MD use_VB g_NN G_NN Ai_NNS gG_NN Ai_NN to_TO represent_VB an_DT arbitrary_JJ subgoal_JJ -RRB-_-RRB- ._.
4_LS ._.
P_NN Aj_NN Ai_NN is_VBZ the_DT profile_NN object_NN agent_NN Ai_NNP holds_VBZ about_IN agent_NN Aj_NN ._.
5_CD ._.
CA_NNP is_VBZ a_DT general_JJ set_NN of_IN actions_NNS for_IN all_DT agents_NNS in_IN A_DT which_WDT are_VBP derived_VBN from_IN the_DT environment_NN ''_'' s_NNS constraints_NNS ._.
CAi_NNP CA_NNP is_VBZ the_DT set_NN of_IN agent_NN Ai_NN ''_'' s_NNS possible_JJ actions_NNS ._.
6_CD ._.
Do_VB -LRB-_-LRB- Ai_NNP ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- holds_VBZ when_WRB Ai_NNP performs_VBZ action_NN over_IN time_NN interval_JJ T_NN in_IN world_NN w_NN ._.
7_CD ._.
Achieve_VB -LRB-_-LRB- G_NN Ai_NNS ,_, ,_, w_NN -RRB-_-RRB- is_VBZ true_JJ when_WRB goal_NN G_NN Ai_NNS is_VBZ achieved_VBN following_VBG the_DT completion_NN of_IN action_NN in_IN world_NN w_NN W_NN ,_, where_WRB CAi_NNP ._.
8_CD ._.
Profile_NNP -LRB-_-LRB- Ai_NNP ,_, PAi_NNP Ai_NNP -RRB-_-RRB- is_VBZ true_JJ when_WRB agent_NN Ai_NN holds_VBZ an_DT object_NN profile_NN for_IN agent_NN Aj_NN ._.
Definition_NN #_# ._.
Full_JJ conflict_NN -LRB-_-LRB- FulConf_NN -RRB-_-RRB- describes_VBZ a_DT zerosum_NN interaction_NN where_WRB only_RB a_DT single_JJ goal_NN of_IN the_DT goals_NNS in_IN conflict_NN can_MD be_VB completed_VBN ._.
FulConf_NN -LRB-_-LRB- G_NN Ai_NNS ,_, G_NN Aj_NN -RRB-_-RRB- -LRB-_-LRB- CAi_NNP ,_, w_NN ,_, CAj_NN -RRB-_-RRB- -LRB-_-LRB- Achieve_VB -LRB-_-LRB- G_NN Ai_NNS ,_, ,_, w_NN -RRB-_-RRB- Achieve_NN -LRB-_-LRB- G_NN Aj_NN ,_, ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- CAj_NN ,_, w_NN ,_, CAi_NN -RRB-_-RRB- -LRB-_-LRB- Achieve_VB -LRB-_-LRB- G_NN Aj_NN ,_, ,_, w_NN -RRB-_-RRB- Achieve_NN -LRB-_-LRB- G_NN Ai_NNS ,_, ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- Definition_NN #_# ._.
Adversarial_JJ Knowledge_NN -LRB-_-LRB- AdvKnow_NN -RRB-_-RRB- is_VBZ a_DT function_NN returning_VBG a_DT value_NN which_WDT represents_VBZ the_DT amount_NN of_IN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD knowledge_NN agent_NN Ai_NNP has_VBZ on_IN the_DT profile_NN of_IN agent_NN Aj_NN ,_, at_IN time_NN Tn_NN ._.
The_DT higher_JJR the_DT value_NN ,_, the_DT more_JJR knowledge_NN agent_NN Ai_NNP has_VBZ ._.
AdvKnow_NNP :_: P_NN Aj_NN Ai_NN Tn_NN Definition_NN #_# ._.
Eval_JJ -_: This_DT evaluation_NN function_NN returns_VBZ an_DT estimated_VBN expected_VBN utility_NN value_NN for_IN an_DT agent_NN in_IN A_NN ,_, after_IN completing_VBG an_DT action_NN from_IN CA_NNP in_IN some_DT world_NN state_NN w_NN ._.
Eval_NNP :_: A_NNP CA_NNP w_NNP Definition_NNP #_# ._.
TrH_SYM -_: -LRB-_-LRB- Threshold_NN -RRB-_-RRB- is_VBZ a_DT numerical_JJ constant_JJ in_IN the_DT -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- range_NN that_IN represents_VBZ an_DT evaluation_NN function_NN -LRB-_-LRB- Eval_NN -RRB-_-RRB- threshold_NN value_NN ._.
An_DT action_NN that_WDT yields_VBZ an_DT estimated_JJ utility_NN evaluation_NN above_IN the_DT TrH_NN is_VBZ regarded_VBN as_IN a_DT highly_RB beneficial_JJ action_NN ._.
The_DT Eval_JJ value_NN is_VBZ an_DT estimation_NN and_CC not_RB the_DT real_JJ utility_NN function_NN ,_, which_WDT is_VBZ usually_RB unknown_JJ ._.
Using_VBG the_DT real_JJ utility_NN value_NN for_IN a_DT rational_JJ agent_NN would_MD easily_RB yield_VB the_DT best_JJS outcome_NN for_IN that_DT agent_NN ._.
However_RB ,_, agents_NNS usually_RB do_VBP not_RB have_VB the_DT real_JJ utility_NN functions_NNS ,_, but_CC rather_RB a_DT heuristic_NN estimate_NN of_IN it_PRP ._.
There_EX are_VBP two_CD important_JJ properties_NNS that_WDT should_MD hold_VB for_IN the_DT evaluation_NN function_NN :_: Property_NNP #_# ._.
The_DT evaluation_NN function_NN should_MD state_VB that_IN the_DT most_RBS desirable_JJ world_NN state_NN is_VBZ one_CD in_IN which_WDT the_DT goal_NN is_VBZ achieved_VBN ._.
Therefore_RB ,_, after_IN the_DT goal_NN has_VBZ been_VBN satisfied_VBN ,_, there_EX can_MD be_VB no_DT future_JJ action_NN that_WDT can_MD put_VB the_DT agent_NN in_IN a_DT world_NN state_NN with_IN higher_JJR Eval_JJ value_NN ._.
-LRB-_-LRB- Ai_NN ,_, G_NN Ai_NNS ,_, ,_, CAi_NNP ,_, w_NNP W_NNP -RRB-_-RRB- Achieve_NNP -LRB-_-LRB- G_NNP Ai_NNP ,_, ,_, w_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Ai_NN ,_, ,_, w_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Ai_NN ,_, ,_, w_NN -RRB-_-RRB- Property_NN #_# ._.
The_DT evaluation_NN function_NN should_MD project_VB an_DT action_NN that_WDT causes_VBZ a_DT completion_NN of_IN a_DT goal_NN or_CC a_DT subgoal_JJ to_TO a_DT value_NN which_WDT is_VBZ greater_JJR than_IN TrH_NN -LRB-_-LRB- a_DT highly_RB beneficial_JJ action_NN -RRB-_-RRB- ._.
-LRB-_-LRB- Ai_NNP ,_, G_NNP Ai_NNP GAi_NNP ,_, CAi_NNP ,_, w_NNP W_NNP ,_, g_NN GAi_NN gGAi_NN -RRB-_-RRB- Achieve_NN -LRB-_-LRB- G_NN Ai_NNS ,_, ,_, w_NN -RRB-_-RRB- Achieve_NN -LRB-_-LRB- g_NN GAi_NN ,_, ,_, w_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Ai_NN ,_, ,_, w_NN -RRB-_-RRB- TrH_NN ._.
Definition_NN #_# ._.
SetAction_NNP We_PRP define_VBP a_DT set_VBN action_NN -LRB-_-LRB- SetAction_NN -RRB-_-RRB- as_IN a_DT set_NN of_IN action_NN operations_NNS -LRB-_-LRB- either_CC complex_NN or_CC basic_JJ actions_NNS -RRB-_-RRB- from_IN some_DT action_NN sets_VBZ CAi_NNP and_CC CAj_NNP which_WDT ,_, according_VBG to_TO agent_NN Ai_NN ''_'' s_NNS belief_NN ,_, are_VBP attached_VBN together_RB by_IN a_DT temporal_JJ and_CC consequential_JJ relationship_NN ,_, forming_VBG a_DT chain_NN of_IN events_NNS -LRB-_-LRB- action_NN ,_, and_CC its_PRP$ following_VBG consequent_JJ action_NN -RRB-_-RRB- ._.
-LRB-_-LRB- #_# ,_, ..._: ,_, u_FW CAi_FW ,_, #_# ,_, ..._: ,_, v_LS CAj_NN ,_, w_NN W_NN -RRB-_-RRB- SetAction_NN -LRB-_-LRB- #_# ,_, ..._: ,_, u_FW ,_, #_# ,_, ..._: ,_, v_LS ,_, w_NN -RRB-_-RRB- -LRB-_-LRB- -LRB-_-LRB- Do_VBP -LRB-_-LRB- Ai_NNP ,_, #_# ,_, T1_NN ,_, w_NN -RRB-_-RRB- Do_VBP -LRB-_-LRB- Aj_NN ,_, #_# ,_, T1_NN ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- Do_VBP -LRB-_-LRB- Ai_NNP ,_, #_# ,_, T2_NN ,_, w_NN -RRB-_-RRB- ..._: Do_VBP -LRB-_-LRB- Ai_NNP ,_, u_FW ,_, Tu_FW ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- The_DT consequential_JJ relation_NN might_MD exist_VB due_JJ to_TO various_JJ environmental_JJ constraints_NNS -LRB-_-LRB- when_WRB one_CD action_NN forces_VBZ the_DT adversary_NN to_TO respond_VB with_IN a_DT specific_JJ action_NN -RRB-_-RRB- or_CC due_JJ to_TO the_DT agent_NN ''_'' s_NNS knowledge_NN about_IN the_DT profile_NN of_IN its_PRP$ adversary_NN ._.
Property_NN #_# ._.
As_IN the_DT knowledge_NN we_PRP have_VBP about_IN our_PRP$ adversary_NN increases_VBZ we_PRP will_MD have_VB additional_JJ beliefs_NNS about_IN its_PRP$ behavior_NN in_IN different_JJ situations_NNS which_WDT in_IN turn_NN creates_VBZ new_JJ set_VBN actions_NNS ._.
Formally_RB ,_, if_IN our_PRP$ AdvKnow_NNP at_IN time_NN Tn_NN +_CC #_# is_VBZ greater_JJR than_IN AdvKnow_NNP at_IN time_NN Tn_NN ,_, then_RB every_DT SetAction_NN known_VBN at_IN time_NN Tn_NN is_VBZ also_RB known_VBN at_IN time_NN Tn_NN +_CC #_# ._.
AdvKnow_NNP -LRB-_-LRB- P_NN Aj_NN Ai_NN ,_, Tn_NN +_CC #_# -RRB-_-RRB- >_JJR AdvKnow_NN -LRB-_-LRB- P_NN Aj_NN Ai_NN ,_, Tn_NN -RRB-_-RRB- -LRB-_-LRB- #_# ,_, ..._: ,_, u_FW CAi_FW ,_, #_# ,_, ..._: ,_, v_LS CAj_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, SetAction_NNP -LRB-_-LRB- #_# ,_, ..._: ,_, u_FW ,_, #_# ,_, ..._: ,_, v_LS -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, SetAction_NNP -LRB-_-LRB- #_# ,_, ..._: ,_, u_FW ,_, #_# ,_, ..._: ,_, v_LS -RRB-_-RRB- ,_, Tn_NN +_CC #_# -RRB-_-RRB- 2_CD ._.
#_# The_DT Environment_NN Formulation_NNP The_DT following_VBG axioms_NNS provide_VBP the_DT formal_JJ definition_NN for_IN a_DT simple_JJ ,_, zero-sum_JJ Adversarial_NNP Environment_NNP -LRB-_-LRB- AE_NNP -RRB-_-RRB- ._.
Satisfaction_NN of_IN these_DT axioms_NNS means_VBZ that_IN the_DT agent_NN is_VBZ situated_VBN in_IN such_JJ an_DT environment_NN ._.
It_PRP provides_VBZ specifications_NNS for_IN agent_NN Aag_NN to_TO interact_VB with_IN its_PRP$ set_NN of_IN adversaries_NNS A_DT with_IN respect_NN to_TO goals_NNS G_NN Aag_NN and_CC G_NN A_NN at_IN time_NN TCo_NN at_IN some_DT world_NN state_NN w_NN ._.
AE_NN -LRB-_-LRB- Aag_NN ,_, A_NN ,_, G_NN Aag_NN ,_, A1_NN ,_, ..._: ,_, Ak_NNP ,_, G_NNP A1_NN ,_, ..._: ,_, G_NN Ak_NN ,_, Tn_NN ,_, w_NN -RRB-_-RRB- 1_CD ._.
Aag_NNP has_VBZ an_DT Int_NN ._.
Th_NN his_PRP$ goal_NN would_MD be_VB completed_VBN :_: -LRB-_-LRB- CAag_NNP ,_, T_NN -RRB-_-RRB- Int_NN ._.
Th_NN -LRB-_-LRB- Aag_NN ,_, Achieve_VB -LRB-_-LRB- G_NN Aag_NN ,_, -RRB-_-RRB- ,_, Tn_NN ,_, T_NN ,_, AE_NN -RRB-_-RRB- 2_CD ._.
Aag_NNP believes_VBZ that_IN it_PRP and_CC each_DT of_IN its_PRP$ adversaries_NNS Ao_NNP are_VBP pursuing_VBG full_JJ conflicting_VBG goals_NNS :_: -LRB-_-LRB- Ao_NNP -LCB-_-LRB- A1_NN ,_, ..._: ,_, Ak_NNP -RCB-_-RRB- -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, FulConf_NNP -LRB-_-LRB- G_NNP Aag_NNP ,_, G_NNP Ao_NNP -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- 3_CD ._.
Aag_NNP believes_VBZ that_IN each_DT of_IN his_PRP$ adversaries_NNS in_IN Ao_NNP has_VBZ the_DT Int_NN ._.
Th_NN his_PRP$ conflicting_JJ goal_NN G_NN Aoi_NNS will_MD be_VB completed_VBN :_: -LRB-_-LRB- Ao_NNP -LCB-_-LRB- A1_NN ,_, ..._: ,_, Ak_NNP -RCB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- CAo_NNP ,_, T_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Int_NNP ._.
Th_NN -LRB-_-LRB- Ao_NN ,_, Achieve_VB -LRB-_-LRB- G_NN Ao_NN ,_, -RRB-_-RRB- ,_, TCo_NN ,_, T_NN ,_, AE_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- 4_CD ._.
Aag_NNP has_VBZ beliefs_NNS about_IN the_DT -LRB-_-LRB- partial_JJ -RRB-_-RRB- profiles_NNS of_IN its_PRP$ adversaries_NNS -LRB-_-LRB- Ao_NN -LCB-_-LRB- A1_NN ,_, ..._: ,_, Ak_NNP -RCB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- PAo_NNP Aag_NNP PAag_NNP -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Profile_NNP -LRB-_-LRB- Ao_NNP ,_, PAo_NNP Aag_NNP -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- To_TO build_VB an_DT agent_NN that_WDT will_MD be_VB able_JJ to_TO operate_VB successfully_RB within_IN such_PDT an_DT AE_NN ,_, we_PRP must_MD specify_VB behavioral_JJ guidelines_NNS for_IN its_PRP$ interactions_NNS ._.
Using_VBG a_DT naive_JJ Eval_JJ maximization_NN strategy_NN to_TO a_DT certain_JJ search_NN depth_NN will_MD not_RB always_RB yield_VB satisfactory_JJ results_NNS for_IN several_JJ reasons_NNS :_: -LRB-_-LRB- #_# -RRB-_-RRB- the_DT search_NN horizon_NN problem_NN when_WRB searching_VBG for_IN a_DT fixed_VBN depth_NN ;_: -LRB-_-LRB- #_# -RRB-_-RRB- the_DT strong_JJ assumption_NN of_IN an_DT optimally_RB rational_JJ ,_, unbounded_JJ resources_NNS adversary_NN ;_: -LRB-_-LRB- #_# -RRB-_-RRB- using_VBG an_DT estimated_VBN evaluation_NN function_NN which_WDT will_MD not_RB give_VB optimal_JJ results_NNS in_IN all_DT world_NN states_NNS ,_, and_CC can_MD be_VB exploited_VBN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
The_DT following_VBG axioms_NNS specify_VB the_DT behavioral_JJ principles_NNS that_WDT can_MD be_VB used_VBN to_TO differentiate_VB between_IN successful_JJ and_CC less_RBR successful_JJ agents_NNS in_IN the_DT above_JJ Adversarial_NNP Environment_NNP ._.
Those_DT axioms_NNS should_MD be_VB used_VBN as_IN specification_NN principles_NNS when_WRB designing_VBG and_CC implementing_VBG agents_NNS that_WDT should_MD be_VB able_JJ to_TO perform_VB well_RB in_IN such_JJ Adversarial_JJ Environments_NNS ._.
The_DT behavioral_JJ axioms_NNS represent_VBP situations_NNS in_IN which_WDT the_DT agent_NN will_MD adopt_VB potential_JJ intentions_NNS to_TO -LRB-_-LRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- ..._: -RRB-_-RRB- -RRB-_-RRB- perform_VBP an_DT action_NN ,_, which_WDT will_MD typically_RB require_VB some_DT means-end_JJ reasoning_NN to_TO select_VB a_DT possible_JJ course_NN of_IN action_NN ._.
This_DT reasoning_NN will_MD lead_VB to_TO the_DT adoption_NN of_IN an_DT Int_NN ._.
To_TO -LRB-_-LRB- ..._: -RRB-_-RRB- ._.
A1_NN ._.
Goal_NNP Achieving_NNP Axiom_NNP ._.
The_DT first_JJ axiom_NN is_VBZ the_DT simplest_JJS case_NN ;_: when_WRB the_DT agent_NN Aag_NN believes_VBZ that_IN it_PRP is_VBZ one_CD action_NN -LRB-_-LRB- -RRB-_-RRB- away_RB from_IN achieving_VBG his_PRP$ conflicting_JJ goal_NN G_NN Aag_NN ,_, it_PRP should_MD adopt_VB the_DT potential_JJ intention_NN to_TO do_VB and_CC complete_VB its_PRP$ goal_NN ._.
-LRB-_-LRB- Aag_NN ,_, CAag_NN ,_, Tn_NN ,_, T_NN ,_, w_NN W_NN -RRB-_-RRB- -LRB-_-LRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Do_VBP -LRB-_-LRB- Aag_NN ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- Achieve_NN -LRB-_-LRB- G_NN Aag_NN ,_, ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Aag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- This_DT somewhat_RB trivial_JJ behavior_NN is_VBZ the_DT first_JJ and_CC strongest_JJS axiom_NN ._.
In_IN any_DT situation_NN ,_, when_WRB the_DT agent_NN is_VBZ an_DT action_NN away_RB from_IN completing_VBG the_DT goal_NN ,_, it_PRP should_MD complete_VB the_DT action_NN ._.
Any_DT fair_JJ Eval_JJ function_NN would_MD naturally_RB classify_VB as_IN the_DT maximal_JJ value_NN action_NN -LRB-_-LRB- property_NN #_# -RRB-_-RRB- ._.
However_RB ,_, without_IN explicit_JJ axiomatization_NN of_IN such_JJ behavior_NN there_EX might_MD be_VB situations_NNS where_WRB the_DT agent_NN will_MD decide_VB on_IN taking_VBG another_DT action_NN for_IN various_JJ reasons_NNS ,_, due_JJ to_TO its_PRP$ bounded_VBN decision_NN resources_NNS ._.
A2_NN ._.
Preventive_NNP Act_NNP Axiom_NNP ._.
Being_VBG in_IN an_DT adversarial_JJ situation_NN ,_, agent_NN Aag_NN might_MD decide_VB to_TO take_VB actions_NNS that_WDT will_MD damage_VB one_CD of_IN its_PRP$ adversary_NN ''_'' s_NNS plans_VBZ to_TO complete_VB its_PRP$ goal_NN ,_, even_RB if_IN those_DT actions_NNS do_VBP not_RB explicitly_RB advance_JJ Aag_NN towards_IN its_PRP$ conflicting_JJ goal_NN G_NN Aag_NN ._.
Such_JJ preventive_JJ action_NN will_MD take_VB place_NN when_WRB agent_NN Aag_NNP has_VBZ a_DT belief_NN about_IN the_DT possibility_NN of_IN its_PRP$ adversary_NN Ao_NNP doing_VBG an_DT action_NN that_WDT will_MD give_VB it_PRP a_DT high_JJ 552_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- utility_NN evaluation_NN value_NN -LRB-_-LRB- >_JJR TrH_NN -RRB-_-RRB- ._.
Believing_VBG that_DT taking_VBG action_NN will_MD prevent_VB the_DT opponent_NN from_IN doing_VBG its_PRP$ ,_, it_PRP will_MD adopt_VB a_DT potential_JJ intention_NN to_TO do_VB ._.
-LRB-_-LRB- Aag_NN ,_, Ao_NN A_NN ,_, CAag_NN ,_, CAo_NNP ,_, Tn_NNP ,_, T_NN ,_, w_NN W_NN -RRB-_-RRB- -LRB-_-LRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Do_VBP -LRB-_-LRB- Ao_NNP ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Ao_NN ,_, ,_, w_NN -RRB-_-RRB- >_JJR TrH_NN ,_, Tn_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Do_VBP -LRB-_-LRB- Aag_NN ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- Do_VBP -LRB-_-LRB- Ao_NNP ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Aag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- This_DT axiom_NN is_VBZ a_DT basic_JJ component_NN of_IN any_DT adversarial_JJ environment_NN ._.
For_IN example_NN ,_, looking_VBG at_IN a_DT Chess_NNP board_NN game_NN ,_, a_DT player_NN could_MD realize_VB that_IN it_PRP is_VBZ about_IN to_TO be_VB checkmated_VBN by_IN its_PRP$ opponent_NN ,_, thus_RB making_VBG a_DT preventive_JJ move_NN ._.
Another_DT example_NN is_VBZ a_DT Connect_VB Four_CD game_NN :_: when_WRB a_DT player_NN has_VBZ a_DT row_NN of_IN three_CD chips_NNS ,_, its_PRP$ opponent_NN must_MD block_VB it_PRP ,_, or_CC lose_VB ._.
A_DT specific_JJ instance_NN of_IN A1_NN occurs_VBZ when_WRB the_DT adversary_NN is_VBZ one_CD action_NN away_RB from_IN achieving_VBG its_PRP$ goal_NN ,_, and_CC immediate_JJ preventive_JJ action_NN needs_VBZ to_TO be_VB taken_VBN by_IN the_DT agent_NN ._.
Formally_RB ,_, we_PRP have_VBP the_DT same_JJ beliefs_NNS as_IN stated_VBN above_IN ,_, with_IN a_DT changed_VBN belief_NN that_WDT doing_VBG action_NN will_MD cause_VB agent_NN Ao_NN to_TO achieve_VB its_PRP$ goal_NN ._.
Proposition_NN #_# :_: Prevent_VB or_CC lose_VB case_NN ._.
-LRB-_-LRB- Aag_NN ,_, Ao_NN A_NN ,_, CAag_NN ,_, CAo_NNP ,_, G_NNP Ao_NNP ,_, Tn_NNP ,_, T_NN ,_, T_NN ,_, w_NN W_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Do_VBP -LRB-_-LRB- Ao_NNP ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- Achieve_NN -LRB-_-LRB- G_NN Ao_NN ,_, ,_, w_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Do_VBP -LRB-_-LRB- Aag_NN ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- Do_VBP -LRB-_-LRB- Ao_NNP ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Aag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- Sketch_VB of_IN proof_NN :_: Proposition_NNP #_# can_MD be_VB easily_RB derived_VBN from_IN axiom_NN A1_NN and_CC the_DT property_NN #_# of_IN the_DT Eval_JJ function_NN ,_, which_WDT states_VBZ that_IN any_DT action_NN that_WDT causes_VBZ a_DT completion_NN of_IN a_DT goal_NN is_VBZ a_DT highly_RB beneficial_JJ action_NN ._.
The_DT preventive_JJ act_NN behavior_NN will_MD occur_VB implicitly_RB when_WRB the_DT Eval_JJ function_NN is_VBZ equal_JJ to_TO the_DT real_JJ world_NN utility_NN function_NN ._.
However_RB ,_, being_VBG bounded_VBN rational_JJ agents_NNS and_CC dealing_VBG with_IN an_DT estimated_VBN evaluation_NN function_NN we_PRP need_VBP to_TO explicitly_RB axiomatize_VB such_JJ behavior_NN ,_, for_IN it_PRP will_MD not_RB always_RB occur_VB implicitly_RB from_IN the_DT evaluation_NN function_NN ._.
A3_NN ._.
Suboptimal_JJ Tactical_NNP Move_VB Axiom_NN ._.
In_IN many_JJ scenarios_NNS a_DT situation_NN may_MD occur_VB where_WRB an_DT agent_NN will_MD decide_VB not_RB to_TO take_VB the_DT current_JJ most_RBS beneficial_JJ action_NN it_PRP can_MD take_VB -LRB-_-LRB- the_DT action_NN with_IN the_DT maximal_JJ utility_NN evaluation_NN value_NN -RRB-_-RRB- ,_, because_IN it_PRP believes_VBZ that_IN taking_VBG another_DT action_NN -LRB-_-LRB- with_IN lower_JJR utility_NN evaluation_NN value_NN -RRB-_-RRB- might_MD yield_VB -LRB-_-LRB- depending_VBG on_IN the_DT adversary_NN ''_'' s_NNS response_NN -RRB-_-RRB- a_DT future_JJ possibility_NN for_IN a_DT highly_RB beneficial_JJ action_NN ._.
This_DT will_MD occur_VB most_RBS often_RB when_WRB the_DT Eval_JJ function_NN is_VBZ inaccurate_JJ and_CC differs_VBZ by_IN a_DT large_JJ extent_NN from_IN the_DT Utility_NN function_NN ._.
Put_VB formally_RB ,_, agent_NN Aag_NNP believes_VBZ in_IN a_DT certain_JJ SetAction_NNP that_WDT will_MD evolve_VB according_VBG to_TO its_PRP$ initial_JJ action_NN and_CC will_MD yield_VB a_DT high_JJ beneficial_JJ value_NN -LRB-_-LRB- >_JJR TrH_NN -RRB-_-RRB- solely_RB for_IN it_PRP ._.
-LRB-_-LRB- Aag_NN ,_, Ao_NN A_NN ,_, Tn_NN ,_, w_NN W_NN -RRB-_-RRB- -LRB-_-LRB- #_# ,_, ..._: ,_, u_FW CAi_FW ,_, #_# ,_, ..._: ,_, v_LS CAj_NN ,_, T1_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, SetAction_NNP -LRB-_-LRB- #_# ,_, ..._: ,_, u_FW ,_, #_# ,_, ..._: ,_, v_LS -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Eval_NNP -LRB-_-LRB- Ao_NNP ,_, v_LS ,_, w_NN -RRB-_-RRB- <_JJR TrH_NN <_JJR Eval_NN -LRB-_-LRB- Aag_NN ,_, u_NN ,_, w_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Aag_NN ,_, #_# ,_, Tn_NN ,_, T1_NN ,_, w_NN -RRB-_-RRB- An_DT agent_NN might_MD believe_VB that_IN a_DT chain_NN of_IN events_NNS will_MD occur_VB for_IN various_JJ reasons_NNS due_JJ to_TO the_DT inevitable_JJ nature_NN of_IN the_DT domain_NN ._.
For_IN example_NN ,_, in_IN Chess_NNP ,_, we_PRP often_RB observe_VBP the_DT following_NN :_: a_DT move_NN causes_VBZ a_DT check_NN position_NN ,_, which_WDT in_IN turn_NN limits_VBZ the_DT opponent_NN ''_'' s_NNS moves_VBZ to_TO avoiding_VBG the_DT check_NN ,_, to_TO which_WDT the_DT first_JJ player_NN might_MD react_VB with_IN another_DT check_NN ,_, and_CC so_RB on_IN ._.
The_DT agent_NN might_MD also_RB believe_VB in_IN a_DT chain_NN of_IN events_NNS based_VBN on_IN its_PRP$ knowledge_NN of_IN its_PRP$ adversary_NN ''_'' s_NNS profile_NN ,_, which_WDT allows_VBZ it_PRP to_TO foresee_VB the_DT adversary_NN ''_'' s_NNS movements_NNS with_IN high_JJ accuracy_NN ._.
A4_NN ._.
Profile_NNP Detection_NN Axiom_NN ._.
The_DT agent_NN can_MD adjust_VB its_PRP$ adversary_NN ''_'' s_NNS profiles_NNS by_IN observations_NNS and_CC pattern_NN study_NN -LRB-_-LRB- specifically_RB ,_, if_IN there_EX are_VBP repeated_JJ encounters_NNS with_IN the_DT same_JJ adversary_NN -RRB-_-RRB- ._.
However_RB ,_, instead_RB of_IN waiting_VBG for_IN profile_NN information_NN to_TO be_VB revealed_VBN ,_, an_DT agent_NN can_MD also_RB initiate_VB actions_NNS that_WDT will_MD force_VB its_PRP$ adversary_NN to_TO react_VB in_IN a_DT way_NN that_WDT will_MD reveal_VB profile_NN knowledge_NN about_IN it_PRP ._.
Formally_RB ,_, the_DT axiom_NN states_VBZ that_IN if_IN all_DT actions_NNS -LRB-_-LRB- -RRB-_-RRB- are_VBP not_RB highly_RB beneficial_JJ actions_NNS -LRB-_-LRB- <_JJR TrH_NN -RRB-_-RRB- ,_, the_DT agent_NN can_MD do_VB action_NN in_IN time_NN T_NN if_IN it_PRP believes_VBZ that_IN it_PRP will_MD result_VB in_IN a_DT non-highly_JJ beneficial_JJ action_NN from_IN its_PRP$ adversary_NN ,_, which_WDT in_IN turn_NN teaches_VBZ it_PRP about_IN the_DT adversary_NN ''_'' s_NNS profile_NN ,_, i_FW ._.
e_LS ._.
,_, gives_VBZ a_DT higher_JJR AdvKnow_NNP -LRB-_-LRB- P_NN Aj_NN Ai_NN ,_, T_NN -RRB-_-RRB- ._.
-LRB-_-LRB- Aag_NN ,_, Ao_NN A_NN ,_, CAag_NN ,_, CAo_NNP ,_, Tn_NNP ,_, T_NN ,_, T_NN ,_, w_NN W_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, -LRB-_-LRB- CAag_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Aag_NN ,_, ,_, w_NN -RRB-_-RRB- <_JJR TrH_NN ,_, Tn_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Do_VBP -LRB-_-LRB- Aag_NN ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- Do_VBP -LRB-_-LRB- Ao_NNP ,_, ,_, T_NN ,_, w_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, Eval_NNP -LRB-_-LRB- Ao_NNP ,_, ,_, w_NN -RRB-_-RRB- <_JJR TrH_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, AdvKnow_NNP -LRB-_-LRB- P_NN Aj_NN Ai_NN ,_, T_NN -RRB-_-RRB- >_JJR AdvKnow_NN -LRB-_-LRB- P_NN Aj_NN Ai_NN ,_, Tn_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Aag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- For_IN example_NN ,_, going_VBG back_RB to_TO the_DT Chess_NNP board_NN game_NN scenario_NN ,_, consider_VBP starting_VBG a_DT game_NN versus_CC an_DT opponent_NN about_IN whom_WP we_PRP know_VBP nothing_NN ,_, not_RB even_RB if_IN it_PRP is_VBZ a_DT human_JJ or_CC a_DT computerized_JJ opponent_NN ._.
We_PRP might_MD start_VB playing_VBG a_DT strategy_NN that_WDT will_MD be_VB suitable_JJ versus_CC an_DT average_JJ opponent_NN ,_, and_CC adjust_VBP our_PRP$ game_NN according_VBG to_TO its_PRP$ level_NN of_IN play_NN ._.
A5_NN ._.
Alliance_NNP Formation_NN Axiom_NN The_DT following_VBG behavioral_JJ axiom_NN is_VBZ relevant_JJ only_RB in_IN a_DT multilateral_JJ instantiation_NN of_IN the_DT adversarial_JJ environment_NN -LRB-_-LRB- obviously_RB ,_, an_DT alliance_NN can_MD not_RB be_VB formed_VBN in_IN a_DT bilateral_JJ ,_, zero-sum_JJ encounter_NN -RRB-_-RRB- ._.
In_IN different_JJ situations_NNS during_IN a_DT multilateral_JJ interaction_NN ,_, a_DT group_NN of_IN agents_NNS might_MD believe_VB that_IN it_PRP is_VBZ in_IN their_PRP$ best_JJS interests_NNS to_TO form_VB a_DT temporary_JJ alliance_NN ._.
Such_PDT an_DT alliance_NN is_VBZ an_DT agreement_NN that_WDT constrains_VBZ its_PRP$ members_NNS ''_'' behavior_NN ,_, but_CC is_VBZ believed_VBN by_IN its_PRP$ members_NNS to_TO enable_VB them_PRP to_TO achieve_VB a_DT higher_JJR utility_NN value_NN than_IN the_DT one_CD achievable_JJ outside_NN of_IN the_DT alliance_NN ._.
As_IN an_DT example_NN ,_, we_PRP can_MD look_VB at_IN the_DT classical_JJ Risk_NN board_NN game_NN ,_, where_WRB each_DT player_NN has_VBZ an_DT individual_JJ goal_NN of_IN being_VBG the_DT sole_JJ conquerer_NN of_IN the_DT world_NN ,_, a_DT zero-sum_JJ game_NN ._.
However_RB ,_, in_IN order_NN to_TO achieve_VB this_DT goal_NN ,_, it_PRP might_MD be_VB strategically_RB wise_JJ to_TO make_VB short-term_JJ ceasefire_NN agreements_NNS with_IN other_JJ players_NNS ,_, or_CC to_TO join_VB forces_NNS and_CC attack_VB an_DT opponent_NN who_WP is_VBZ stronger_JJR than_IN the_DT rest_NN ._.
An_DT alliance_NN ''_'' s_NNS terms_NNS defines_VBZ the_DT way_NN its_PRP$ members_NNS should_MD act_VB ._.
It_PRP is_VBZ a_DT set_NN of_IN predicates_NNS ,_, denoted_VBN as_IN Terms_NNS ,_, that_DT is_VBZ agreed_VBN upon_IN by_IN the_DT alliance_NN members_NNS ,_, and_CC should_MD remain_VB true_JJ for_IN the_DT duration_NN of_IN the_DT alliance_NN ._.
For_IN example_NN ,_, the_DT set_NN Terms_NNS in_IN the_DT Risk_NN scenario_NN ,_, could_MD contain_VB the_DT following_VBG predicates_NNS :_: 1_CD ._.
Alliance_NNP members_NNS will_MD not_RB attack_VB each_DT other_JJ on_IN territories_NNS X_NN ,_, Y_NN and_CC Z_NN ;_: 2_LS ._.
Alliance_NNP members_NNS will_MD contribute_VB C_NN units_NNS per_IN turn_NN for_IN attacking_VBG adversary_NN Ao_NN ;_: 3_LS ._.
Members_NNS are_VBP obligated_VBN to_TO stay_VB as_IN part_NN of_IN the_DT alliance_NN until_IN time_NN Tk_NN or_CC until_IN adversary_NN ''_'' s_NNS Ao_NNP army_NN is_VBZ smaller_JJR than_IN Q_NNP ._.
The_DT set_VBN Terms_NNS specifies_VBZ inter-group_JJ constraints_NNS on_IN each_DT of_IN the_DT alliance_NN member_NN ''_'' s_NNS -LRB-_-LRB- Aal_JJ i_FW Aal_FW A_NN -RRB-_-RRB- set_NN of_IN actions_NNS Cal_JJ i_FW C_NN ._.
Definition_NN #_# ._.
Al_NNP val_SYM -_: the_DT total_JJ evaluation_NN value_NN that_WDT agent_NN Ai_NNP will_MD achieve_VB while_IN being_VBG part_NN of_IN Aal_NN is_VBZ the_DT sum_NN of_IN Evali_NNP -LRB-_-LRB- Eval_NNP for_IN Ai_NNP -RRB-_-RRB- of_IN each_DT of_IN Aal_NN j_NN Eval_JJ values_NNS after_IN taking_VBG their_PRP$ own_JJ actions_NNS -LRB-_-LRB- via_IN the_DT agent_NN -LRB-_-LRB- -RRB-_-RRB- predicate_NN -RRB-_-RRB- :_: Al_NNP val_NNP -LRB-_-LRB- Ai_NNP ,_, Cal_NNP ,_, Aal_NNP ,_, w_NN -RRB-_-RRB- =_JJ Cal_NN Evali_NN -LRB-_-LRB- Aal_NN j_NN ,_, agent_NN -LRB-_-LRB- -RRB-_-RRB- ,_, w_NN -RRB-_-RRB- Definition_NN #_# ._.
Al_NNP TrH_NNP -_: is_VBZ a_DT number_NN representing_VBG an_DT Al_NNP val_NN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD threshold_NN ;_: above_IN it_PRP ,_, the_DT alliance_NN can_MD be_VB said_VBN to_TO be_VB a_DT highly_RB beneficial_JJ alliance_NN ._.
The_DT value_NN of_IN Al_NNP TrH_NNP will_MD be_VB calculated_VBN dynamically_RB according_VBG to_TO the_DT progress_NN of_IN the_DT interaction_NN ,_, as_IN can_MD be_VB seen_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
After_IN an_DT alliance_NN is_VBZ formed_VBN ,_, its_PRP$ members_NNS are_VBP now_RB working_VBG in_IN their_PRP$ normal_JJ adversarial_JJ environment_NN ,_, as_RB well_RB as_IN according_VBG to_TO the_DT mental_JJ states_NNS and_CC axioms_NNS required_VBN for_IN their_PRP$ interactions_NNS as_IN part_NN of_IN the_DT alliance_NN ._.
The_DT following_VBG Alliance_NNP model_NN -LRB-_-LRB- AL_NN -RRB-_-RRB- specifies_VBZ the_DT conditions_NNS under_IN which_WDT the_DT group_NN Aal_NNP can_MD be_VB said_VBN to_TO be_VB in_IN an_DT alliance_NN and_CC working_VBG with_IN a_DT new_JJ and_CC constrained_VBD set_NN of_IN actions_NNS Cal_NNP ,_, at_IN time_NN Tn_NN ._.
AL_NNP -LRB-_-LRB- Aal_NNP ,_, Cal_NNP ,_, w_NN ,_, Tn_NN -RRB-_-RRB- 1_CD ._.
Aal_NNP has_VBZ a_DT MB_NN that_IN all_DT members_NNS are_VBP part_NN of_IN Aal_NNP :_: MB_NN -LRB-_-LRB- Aal_NN ,_, -LRB-_-LRB- Aal_NNP i_FW Aal_NNP -RRB-_-RRB- member_NN -LRB-_-LRB- Aal_NN i_FW ,_, Aal_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- 2_CD ._.
Aal_NNP has_VBZ a_DT MB_NN that_IN the_DT group_NN be_VB maintained_VBN :_: MB_NN -LRB-_-LRB- Aal_NN ,_, -LRB-_-LRB- Aal_NNP i_FW Aal_NNP -RRB-_-RRB- Int_NNP ._.
Th_NN -LRB-_-LRB- Ai_NN ,_, member_NN -LRB-_-LRB- Ai_NN ,_, Aal_NN -RRB-_-RRB- ,_, Tn_NN ,_, Tn_NN +_CC #_# ,_, Co_NNP -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- 3_CD ._.
Aal_NNP has_VBZ a_DT MB_NN that_WDT being_VBG members_NNS gives_VBZ them_PRP high_JJ utility_NN value_NN :_: MB_NN -LRB-_-LRB- Aal_NN ,_, -LRB-_-LRB- Aal_NNP i_FW Aal_NNP -RRB-_-RRB- Al_NNP val_NN -LRB-_-LRB- Aal_NN i_FW ,_, Cal_NNP ,_, Aal_NNP ,_, w_NN -RRB-_-RRB- Al_NNP TrH_NNP ,_, Tn_NN -RRB-_-RRB- Members_NNS ''_'' profiles_NNS are_VBP a_DT crucial_JJ part_NN of_IN successful_JJ alliances_NNS ._.
We_PRP assume_VBP that_IN agents_NNS that_WDT have_VBP more_RBR accurate_JJ profiles_NNS of_IN their_PRP$ adversaries_NNS will_MD be_VB more_RBR successful_JJ in_IN such_JJ environments_NNS ._.
Such_JJ agents_NNS will_MD be_VB able_JJ to_TO predict_VB when_WRB a_DT member_NN is_VBZ about_IN to_TO breach_VB the_DT alliance_NN ''_'' s_NNS contract_NN -LRB-_-LRB- item_NN #_# in_IN the_DT above_JJ model_NN -RRB-_-RRB- ,_, and_CC take_VB counter_JJ measures_NNS -LRB-_-LRB- when_WRB item_NN #_# will_MD falsify_VB -RRB-_-RRB- ._.
The_DT robustness_NN of_IN the_DT alliance_NN is_VBZ in_IN part_NN a_DT function_NN of_IN its_PRP$ members_NNS ''_'' trustfulness_NN measure_NN ,_, objective_JJ position_NN estimation_NN ,_, and_CC other_JJ profile_NN properties_NNS ._.
We_PRP should_MD note_VB that_IN an_DT agent_NN can_MD simultaneously_RB be_VB part_NN of_IN more_JJR than_IN one_CD alliance_NN ._.
Such_PDT a_DT temporary_JJ alliance_NN ,_, where_WRB the_DT group_NN members_NNS do_VBP not_RB have_VB a_DT joint_JJ goal_NN but_CC act_VBP collaboratively_RB for_IN the_DT interest_NN of_IN their_PRP$ own_JJ individual_JJ goals_NNS ,_, is_VBZ classified_VBN as_IN a_DT Treatment_NN Group_NN by_IN modern_JJ psychologists_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- -LRB-_-LRB- in_IN contrast_NN to_TO a_DT Task_NNP Group_NNP ,_, where_WRB its_PRP$ members_NNS have_VBP a_DT joint_JJ goal_NN -RRB-_-RRB- ._.
The_DT Shared_VBN Activity_NN model_NN as_IN presented_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- modeled_VBD Treatment_NNP Group_NNP behavior_NN using_VBG the_DT same_JJ SharedPlans_NNPS formalization_NN ._.
When_WRB comparing_VBG both_CC definitions_NNS of_IN an_DT alliance_NN and_CC a_DT Treatment_NN Group_NN we_PRP found_VBD an_DT unsurprising_JJ resemblance_NN between_IN both_DT models_NNS :_: the_DT environment_NN model_NN ''_'' s_NNS definitions_NNS are_VBP almost_RB identical_JJ ,_, and_CC their_PRP$ Selfish-Act_NNP and_CC Cooperative_NNP Act_NNP axioms_NNS conform_VB to_TO our_PRP$ adversarial_JJ agent_NN ''_'' s_NNS behavior_NN ._.
The_DT main_JJ distinction_NN between_IN both_DT models_NNS is_VBZ the_DT integration_NN of_IN a_DT Helpful-behavior_JJ act_NN axiom_NN ,_, in_IN the_DT Shared_VBN Activity_NN which_WDT can_MD not_RB be_VB part_NN of_IN ours_JJ ._.
This_DT axiom_NN states_VBZ that_IN an_DT agent_NN will_MD consider_VB taking_VBG action_NN that_WDT will_MD lower_VB its_PRP$ Eval_JJ value_NN -LRB-_-LRB- to_TO a_DT certain_JJ lower_JJR bound_VBN -RRB-_-RRB- ,_, if_IN it_PRP believes_VBZ that_IN a_DT group_NN partner_NN will_MD gain_VB a_DT significant_JJ benefit_NN ._.
Such_JJ behavior_NN can_MD not_RB occur_VB in_IN a_DT pure_JJ adversarial_JJ environment_NN -LRB-_-LRB- as_IN a_DT zero-sum_JJ game_NN is_VBZ -RRB-_-RRB- ,_, where_WRB the_DT alliance_NN members_NNS are_VBP constantly_RB on_IN watch_NN to_TO manipulate_VB their_PRP$ alliance_NN to_TO their_PRP$ own_JJ advantage_NN ._.
A6_NN ._.
Evaluation_NN Maximization_NN Axiom_NN ._.
In_IN a_DT case_NN when_WRB all_DT other_JJ axioms_NNS are_VBP inapplicable_JJ ,_, we_PRP will_MD proceed_VB with_IN the_DT action_NN that_WDT maximizes_VBZ the_DT heuristic_NN value_NN as_IN computed_JJ in_IN the_DT Eval_JJ function_NN ._.
-LRB-_-LRB- Aag_NN ,_, Ao_NN A_NN ,_, Cag_NN ,_, Tn_NN ,_, w_NN W_NN -RRB-_-RRB- Bel_NNP -LRB-_-LRB- Aag_NNP ,_, -LRB-_-LRB- Cag_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Aag_NN ,_, ,_, w_NN -RRB-_-RRB- Eval_NN -LRB-_-LRB- Aag_NN ,_, ,_, w_NN -RRB-_-RRB- ,_, Tn_NN -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Aag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- T1_NN ._.
Optimality_NN on_IN Eval_NN =_JJ Utility_NN The_DT above_IN axiomatic_JJ model_NN handles_VBZ situations_NNS where_WRB the_DT Utility_NN is_VBZ unknown_JJ and_CC the_DT agents_NNS are_VBP bounded_VBN rational_JJ agents_NNS ._.
The_DT following_VBG theorem_NN shows_VBZ that_IN in_IN bilateral_JJ interactions_NNS ,_, where_WRB the_DT agents_NNS have_VBP the_DT real_JJ Utility_NN function_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, Eval_NN =_JJ Utility_NN -RRB-_-RRB- and_CC are_VBP rational_JJ agents_NNS ,_, the_DT axioms_NNS provide_VBP the_DT same_JJ optimal_JJ result_NN as_IN classic_JJ adversarial_JJ search_NN -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, Min-Max_NNP -RRB-_-RRB- ._.
Theorem_NNP #_# ._.
Let_VB Ae_NNP ag_NN be_VB an_DT unbounded_JJ rational_JJ AE_NN agent_NN using_VBG the_DT Eval_JJ heuristic_NN evaluation_NN function_NN ,_, Au_NNP ag_NN be_VB the_DT same_JJ agent_NN using_VBG the_DT true_JJ Utility_NN function_NN ,_, and_CC Ao_NNP be_VB a_DT sole_JJ unbounded_JJ utility-based_JJ rational_JJ adversary_NN ._.
Given_VBN that_IN Eval_NN =_JJ Utility_NN :_: -LRB-_-LRB- CAu_NNP ag_NNP ,_, CAe_NNP ag_NN ,_, Tn_NN ,_, w_NN W_NN -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Au_NN ag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- Pot_NNP ._.
Int_NN ._.
To_TO -LRB-_-LRB- Ae_NN ag_NN ,_, ,_, Tn_NN ,_, T_NN ,_, w_NN -RRB-_-RRB- -LRB-_-LRB- -LRB-_-LRB- =_JJ -RRB-_-RRB- -LRB-_-LRB- Utility_NN -LRB-_-LRB- Au_NN ag_NN ,_, ,_, w_NN -RRB-_-RRB- =_JJ Eval_NN -LRB-_-LRB- Ae_NN ag_NN ,_, ,_, w_NN -RRB-_-RRB- -RRB-_-RRB- -RRB-_-RRB- Sketch_VB of_IN proof_NN -_: Given_VBN that_IN Au_NNP ag_NN has_VBZ the_DT real_JJ utility_NN function_NN and_CC unbounded_JJ resources_NNS ,_, it_PRP can_MD generate_VB the_DT full_JJ game_NN tree_NN and_CC run_VB the_DT optimal_JJ MinMax_NNP algorithm_NN to_TO choose_VB the_DT highest_JJS utility_NN value_NN action_NN ,_, which_WDT we_PRP denote_VBP by_IN ,_, ._.
The_DT proof_NN will_MD show_VB that_IN Ae_NN ag_NN ,_, using_VBG the_DT AE_NN axioms_NNS ,_, will_MD select_VB the_DT same_JJ or_CC equal_JJ utility_NN -LRB-_-LRB- when_WRB there_EX is_VBZ more_JJR than_IN one_CD action_NN with_IN the_DT same_JJ max_NN utility_NN -RRB-_-RRB- when_WRB Eval_NN =_JJ Utility_NN ._.
-LRB-_-LRB- A1_NN -RRB-_-RRB- Goal_NN achieving_VBG axiom_NN -_: suppose_VB there_EX is_VBZ an_DT such_JJ that_IN its_PRP$ completion_NN will_MD achieve_VB Au_NNP ag_NN ''_'' s_NNS goal_NN ._.
It_PRP will_MD obtain_VB the_DT highest_JJS utility_NN by_IN Min-Max_NNP for_IN Au_NNP ag_NN ._.
The_DT Ae_NN ag_NN agent_NN will_MD select_VB or_CC another_DT action_NN with_IN the_DT same_JJ utility_NN value_NN via_IN A1_NN ._.
If_IN such_JJ does_VBZ not_RB exist_VB ,_, Ae_NNP ag_NN can_MD not_RB apply_VB this_DT axiom_NN ,_, and_CC proceeds_VBZ to_TO A2_NN ._.
-LRB-_-LRB- A2_NN -RRB-_-RRB- Preventive_JJ act_NN axiom_NN -_: -LRB-_-LRB- #_# -RRB-_-RRB- Looking_VBG at_IN the_DT basic_JJ case_NN ,_, if_IN there_EX is_VBZ a_DT which_WDT leads_VBZ Ao_NNP to_TO achieve_VB its_PRP$ goal_NN ,_, then_RB a_DT preventive_JJ action_NN will_MD yield_VB the_DT highest_JJS utility_NN for_IN Au_NNP ag_NN ._.
Au_NNP ag_NN will_MD choose_VB it_PRP through_IN the_DT utility_NN ,_, while_IN Ae_NN ag_NN will_MD choose_VB it_PRP through_IN A2_NN ._.
-LRB-_-LRB- #_# -RRB-_-RRB- In_IN the_DT general_JJ case_NN ,_, is_VBZ a_DT highly_RB beneficial_JJ action_NN for_IN Ao_NNP ,_, thus_RB yields_NNS low_JJ utility_NN for_IN Au_NNP ag_NN ,_, which_WDT will_MD guide_VB it_PRP to_TO select_VB an_DT that_WDT will_MD prevent_VB ,_, while_IN Ae_NN ag_NN will_MD choose_VB it_PRP through_IN A2_NN ._.
#_# If_IN such_JJ does_VBZ not_RB exist_VB for_IN Ao_NNP ,_, then_RB A2_NN is_VBZ not_RB applicable_JJ ,_, and_CC Ae_NN ag_NN can_MD proceed_VB to_TO A3_NN ._.
-LRB-_-LRB- A3_NN -RRB-_-RRB- Suboptimal_JJ tactical_JJ move_NN axiom_NN -_: When_WRB using_VBG a_DT heuristic_NN Eval_JJ function_NN ,_, Ae_NN ag_NN has_VBZ a_DT partial_JJ belief_NN in_IN the_DT profile_NN of_IN its_PRP$ adversary_NN -LRB-_-LRB- item_NN #_# in_IN AE_NNP model_NN -RRB-_-RRB- ,_, which_WDT may_MD lead_VB it_PRP to_TO believe_VB in_IN SetActions_NNS -LRB-_-LRB- Prop1_NN -RRB-_-RRB- ._.
In_IN our_PRP$ case_NN ,_, Ae_NN ag_NN is_VBZ holding_VBG a_DT full_JJ profile_NN on_IN its_PRP$ optimal_JJ adversary_NN and_CC knows_VBZ that_IN Ao_NNP will_MD behave_VB optimally_RB according_VBG to_TO the_DT real_JJ utility_NN values_NNS on_IN the_DT complete_JJ search_NN tree_NN ,_, therefore_RB ,_, any_DT belief_NN about_IN suboptimal_JJ SetAction_NNP can_MD not_RB exist_VB ,_, yielding_VBG this_DT axiom_NN inapplicable_JJ ._.
Ae_NN ag_NN will_MD proceed_VB to_TO A4_NN ._.
-LRB-_-LRB- A4_NN -RRB-_-RRB- Profile_NNP detection_NN axiom_NN -_: Given_VBN that_IN Ae_NN ag_NN has_VBZ the_DT full_JJ profile_NN of_IN Ao_NNP ,_, none_NN of_IN Ae_NN ag_NN ''_'' s_NNS actions_NNS can_MD increase_VB its_PRP$ knowledge_NN ._.
That_DT axiom_NN will_MD not_RB be_VB applied_VBN ,_, and_CC the_DT agent_NN will_MD proceed_VB with_IN A6_NN -LRB-_-LRB- A5_NN will_MD be_VB disregarded_VBN because_IN the_DT interaction_NN is_VBZ bilateral_JJ -RRB-_-RRB- ._.
-LRB-_-LRB- A6_NN -RRB-_-RRB- Evaluation_NN maximization_NN axiom_NN -_: This_DT axiom_NN will_MD select_VB the_DT max_NN Eval_NNP for_IN Ae_NNP ag_NN ._.
Given_VBN that_IN Eval_NN =_JJ Utility_NN ,_, the_DT same_JJ that_DT was_VBD selected_VBN by_IN Au_NNP ag_NN will_MD be_VB selected_VBN ._.
3_LS ._.
EVALUATION_NN The_DT main_JJ purpose_NN of_IN our_PRP$ experimental_JJ analysis_NN is_VBZ to_TO evaluate_VB the_DT model_NN ''_'' s_NNS behavior_NN and_CC performance_NN in_IN a_DT real_JJ adversarial_JJ environment_NN ._.
This_DT section_NN investigates_VBZ whether_IN bounded_VBN 1_CD A_DT case_NN where_WRB following_VBG the_DT completion_NN of_IN there_EX exists_VBZ a_DT which_WDT gives_VBZ high_JJ utility_NN for_IN Agent_NNP Au_NNP ag_NN ,_, can_MD not_RB occur_VB because_IN Ao_NNP uses_VBZ the_DT same_JJ utility_NN ,_, and_CC ''_'' s_VBZ existence_NN will_MD cause_VB it_PRP to_TO classify_VB as_IN a_DT low_JJ utility_NN action_NN ._.
554_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- rational_JJ agents_NNS situated_VBN in_IN such_JJ adversarial_JJ environments_NNS will_MD be_VB better_JJR off_IN applying_VBG our_PRP$ suggested_VBN behavioral_JJ axioms_NNS ._.
3_LS ._.
#_# The_DT Domain_NN To_TO explore_VB the_DT use_NN of_IN the_DT above_JJ model_NN and_CC its_PRP$ behavioral_JJ axioms_NNS ,_, we_PRP decided_VBD to_TO use_VB the_DT Connect-Four_NNP game_NN as_IN our_PRP$ adversarial_JJ environment_NN ._.
Connect-Four_NNP is_VBZ a_DT 2-player_NN ,_, zerosum_NN game_NN which_WDT is_VBZ played_VBN using_VBG a_DT 6x7_JJ matrix-like_JJ board_NN ._.
Each_DT turn_NN ,_, a_DT player_NN drops_VBZ a_DT disc_NN into_IN one_CD of_IN the_DT #_# columns_NNS -LRB-_-LRB- the_DT set_NN of_IN ##_CD discs_NNS is_VBZ usually_RB colored_VBN yellow_JJ for_IN player_NN #_# and_CC red_JJ for_IN player_NN #_# ;_: we_PRP will_MD use_VB White_NNP and_CC Black_NNP respectively_RB to_TO avoid_VB confusion_NN -RRB-_-RRB- ._.
The_DT winner_NN is_VBZ the_DT first_JJ player_NN to_TO complete_VB a_DT horizontal_JJ ,_, vertical_JJ ,_, or_CC diagonal_JJ set_NN of_IN four_CD discs_NNS with_IN its_PRP$ color_NN ._.
On_IN very_RB rare_JJ occasions_NNS ,_, the_DT game_NN might_MD end_VB in_IN a_DT tie_NN if_IN all_DT the_DT empty_JJ grids_NNS are_VBP filled_VBN ,_, but_CC no_DT player_NN managed_VBD to_TO create_VB a_DT 4-disc_JJ set_NN ._.
The_DT Connect-Four_NNP game_NN was_VBD solved_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, where_WRB it_PRP is_VBZ shown_VBN that_IN the_DT first_JJ player_NN -LRB-_-LRB- playing_NN with_IN the_DT white_JJ discs_NNS -RRB-_-RRB- can_MD force_VB a_DT win_VB by_IN starting_VBG in_IN the_DT middle_JJ column_NN -LRB-_-LRB- column_NN #_# -RRB-_-RRB- and_CC playing_VBG optimally_RB However_RB ,_, the_DT optimal_JJ strategy_NN is_VBZ very_RB complex_JJ ,_, and_CC difficult_JJ to_TO follow_VB even_RB for_IN complex_JJ bounded_VBN rational_JJ agents_NNS ,_, such_JJ as_IN human_JJ players_NNS ._.
Before_IN we_PRP can_MD proceed_VB checking_VBG agent_NN behavior_NN ,_, we_PRP must_MD first_RB verify_VB that_IN the_DT domain_NN conforms_VBZ to_TO the_DT adversarial_JJ environment_NN ''_'' s_NNS definition_NN as_IN given_VBN above_IN -LRB-_-LRB- which_WDT the_DT behavioral_JJ axioms_NNS are_VBP based_VBN on_IN -RRB-_-RRB- ._.
First_RB ,_, when_WRB playing_VBG a_DT Connect-Four_NNP game_NN ,_, the_DT agent_NN has_VBZ an_DT intention_NN to_TO win_VB the_DT game_NN -LRB-_-LRB- item_NN #_# -RRB-_-RRB- ._.
Second_JJ -LRB-_-LRB- item_NN #_# -RRB-_-RRB- ,_, our_PRP$ agent_NN believes_VBZ that_IN in_IN Connect-Four_NNP there_RB can_MD only_RB be_VB one_CD winner_NN -LRB-_-LRB- or_CC no_DT winner_NN at_IN all_DT in_IN the_DT rare_JJ occurrence_NN of_IN a_DT tie_NN -RRB-_-RRB- ._.
In_IN addition_NN ,_, our_PRP$ agent_NN believes_VBZ that_IN its_PRP$ opponent_NN to_TO the_DT game_NN will_MD try_VB to_TO win_VB -LRB-_-LRB- item_NN #_# -RRB-_-RRB- ,_, and_CC we_PRP hope_VBP it_PRP has_VBZ some_DT partial_JJ knowledge_NN -LRB-_-LRB- item_NN #_# -RRB-_-RRB- about_IN its_PRP$ adversary_NN -LRB-_-LRB- this_DT knowledge_NN can_MD vary_VB from_IN nothing_NN ,_, through_IN simple_JJ facts_NNS such_JJ as_IN age_NN ,_, to_TO strategies_NNS and_CC weaknesses_NNS -RRB-_-RRB- ._.
Of_IN course_NN ,_, not_RB all_DT Connect-Four_NNP encounters_NNS are_VBP adversarial_JJ ._.
For_IN example_NN ,_, when_WRB a_DT parent_NN is_VBZ playing_VBG the_DT game_NN with_IN its_PRP$ child_NN ,_, the_DT following_VBG situation_NN might_MD occur_VB :_: the_DT child_NN ,_, having_VBG a_DT strong_JJ incentive_NN to_TO win_VB ,_, treats_VBZ the_DT environment_NN as_IN adversarial_JJ -LRB-_-LRB- it_PRP intends_VBZ to_TO win_VB ,_, understands_VBZ that_IN there_EX can_MD only_RB be_VB one_CD winner_NN ,_, and_CC believes_VBZ that_IN its_PRP$ parent_NN is_VBZ trying_VBG to_TO beat_VB him_PRP -RRB-_-RRB- ._.
However_RB ,_, the_DT parent_NN ''_'' s_NNS point_NN of_IN view_NN might_MD see_VB the_DT environment_NN as_IN an_DT educational_JJ one_CD ,_, where_WRB its_PRP$ goal_NN is_VBZ not_RB to_TO win_VB the_DT game_NN ,_, but_CC to_TO cause_VB enjoyment_NN or_CC practice_NN strategic_JJ reasoning_NN ._.
In_IN such_PDT an_DT educational_JJ environment_NN ,_, a_DT new_JJ set_NN of_IN behavioral_JJ axioms_NNS might_MD be_VB more_RBR beneficial_JJ to_TO the_DT parent_NN ''_'' s_NNS goals_NNS than_IN our_PRP$ suggested_VBN adversarial_JJ behavioral_JJ axioms_NNS ._.
3_LS ._.
#_# Axiom_NNP Analysis_NNP After_IN showing_VBG that_IN the_DT Connect-Four_NNP game_NN is_VBZ indeed_RB a_DT zero-sum_JJ ,_, bilateral_JJ adversarial_JJ environment_NN ,_, the_DT next_JJ step_NN is_VBZ to_TO look_VB at_IN players_NNS ''_'' behaviors_NNS during_IN the_DT game_NN and_CC check_VB whether_IN behaving_VBG according_VBG to_TO our_PRP$ model_NN does_VBZ improve_VB performance_NN ._.
To_TO do_VB so_RB we_PRP have_VBP collected_VBN log_NN files_NNS from_IN completed_VBN Connect-Four_NNP games_NNS that_WDT were_VBD played_VBN by_IN human_JJ players_NNS over_IN the_DT Internet_NN ._.
Our_PRP$ collected_VBN log_NN file_NN data_NNS came_VBD from_IN Play_NNP by_IN eMail_NNP -LRB-_-LRB- PBeM_NNP -RRB-_-RRB- sites_NNS ._.
These_DT are_VBP web_NN sites_NNS that_WDT host_VBP email_NN games_NNS ,_, where_WRB each_DT move_NN is_VBZ taken_VBN by_IN an_DT email_NN exchange_NN between_IN the_DT server_NN and_CC the_DT players_NNS ._.
Many_JJ such_JJ sites_NNS ''_'' archives_NNS contain_VBP real_JJ competitive_JJ interactions_NNS ,_, and_CC also_RB maintain_VBP a_DT ranking_JJ system_NN for_IN their_PRP$ members_NNS ._.
Most_JJS of_IN the_DT data_NNS we_PRP used_VBD can_MD be_VB found_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
As_IN can_MD be_VB learned_VBN from_IN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, Connect-Four_NNP has_VBZ an_DT optimal_JJ strategy_NN and_CC a_DT considerable_JJ advantage_NN for_IN the_DT player_NN who_WP starts_VBZ the_DT game_NN -LRB-_-LRB- which_WDT we_PRP call_VBP the_DT White_NNP player_NN -RRB-_-RRB- ._.
We_PRP will_MD concentrate_VB in_IN our_PRP$ analysis_NN on_IN the_DT second_JJ player_NN ''_'' s_NNS moves_NNS -LRB-_-LRB- to_TO be_VB called_VBN Black_NNP -RRB-_-RRB- ._.
The_DT White_NNP player_NN ,_, being_VBG the_DT first_JJ to_TO act_VB ,_, has_VBZ the_DT so-called_JJ initiative_NN advantage_NN ._.
Having_VBG the_DT advantage_NN and_CC a_DT good_JJ strategy_NN will_MD keep_VB the_DT Black_JJ player_NN busy_JJ reacting_VBG to_TO its_PRP$ moves_NNS ,_, instead_RB of_IN initiating_VBG threats_NNS ._.
A_DT threat_NN is_VBZ a_DT combination_NN of_IN three_CD discs_NNS of_IN the_DT same_JJ color_NN ,_, with_IN an_DT empty_JJ spot_NN for_IN the_DT fourth_JJ winning_VBG disk_NN ._.
An_DT open_JJ threat_NN is_VBZ a_DT threat_NN that_WDT can_MD be_VB realized_VBN in_IN the_DT opponent_NN ''_'' s_NNS next_JJ move_NN ._.
In_IN order_NN for_IN the_DT Black_JJ player_NN to_TO win_VB ,_, it_PRP must_MD somehow_RB turn_VB the_DT tide_NN ,_, take_VB the_DT advantage_NN and_CC start_VB presenting_VBG threats_NNS to_TO the_DT White_NNP player_NN ._.
We_PRP will_MD explore_VB Black_JJ players_NNS ''_'' behavior_NN and_CC their_PRP$ conformance_NN to_TO our_PRP$ axioms_NNS ._.
To_TO do_VB so_RB ,_, we_PRP built_VBD an_DT application_NN that_WDT reads_VBZ log_NN files_NNS and_CC analyzes_VBZ the_DT Black_JJ player_NN ''_'' s_NNS moves_NNS ._.
The_DT application_NN contains_VBZ two_CD main_JJ components_NNS :_: -LRB-_-LRB- #_# -RRB-_-RRB- a_DT Min-Max_NNP algorithm_NN for_IN evaluation_NN of_IN moves_NNS ;_: -LRB-_-LRB- #_# -RRB-_-RRB- open_JJ threats_NNS detector_NN for_IN the_DT discovering_VBG of_IN open_JJ threats_NNS ._.
The_DT Min-Max_NNP algorithm_NN will_MD work_VB to_TO a_DT given_VBN depth_NN ,_, d_NN and_CC for_IN each_DT move_NN will_MD output_NN the_DT heuristic_NN value_NN for_IN the_DT next_JJ action_NN taken_VBN by_IN the_DT player_NN as_IN written_VBN in_IN the_DT log_NN file_NN ,_, h_NN -LRB-_-LRB- -RRB-_-RRB- ,_, alongside_IN the_DT maximum_NN heuristic_NN value_NN ,_, maxh_NN -LRB-_-LRB- -RRB-_-RRB- ,_, that_WDT could_MD be_VB achieved_VBN prior_RB to_TO taking_VBG the_DT move_NN -LRB-_-LRB- obviously_RB ,_, if_IN h_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ maxh_NN -LRB-_-LRB- -RRB-_-RRB- ,_, then_RB the_DT player_NN did_VBD not_RB do_VB the_DT optimal_JJ move_NN heuristically_RB -RRB-_-RRB- ._.
The_DT threat_NN detector_NN ''_'' s_NNS job_NN is_VBZ to_TO notify_VB if_IN some_DT action_NN was_VBD taken_VBN in_IN order_NN to_TO block_VB an_DT open_JJ threat_NN -LRB-_-LRB- not_RB blocking_VBG an_DT open_JJ threat_NN will_MD probably_RB cause_VB the_DT player_NN to_TO lose_VB in_IN the_DT opponent_NN ''_'' s_NNS next_JJ move_NN -RRB-_-RRB- ._.
The_DT heuristic_NN function_NN used_VBN by_IN Min-Max_NNP to_TO evaluate_VB the_DT player_NN ''_'' s_NNS utility_NN is_VBZ the_DT following_JJ function_NN ,_, which_WDT is_VBZ simple_JJ to_TO compute_VB ,_, yet_RB provides_VBZ a_DT reasonable_JJ challenge_NN to_TO human_JJ opponents_NNS :_: Definition_NN #_# ._.
Let_NNP Group_NNP be_VB an_DT adjacent_JJ set_NN of_IN four_CD squares_NNS that_WDT are_VBP horizontal_JJ ,_, vertical_JJ ,_, or_CC diagonal_JJ ._.
Groupn_NNP b_NN -LRB-_-LRB- Groupn_NN w_NN -RRB-_-RRB- be_VB a_DT Group_NN with_IN n_NN pieces_NNS of_IN the_DT black_JJ -LRB-_-LRB- white_JJ -RRB-_-RRB- color_NN and_CC 4n_NN empty_JJ squares_NNS ._.
h_NN =_JJ -LRB-_-LRB- -LRB-_-LRB- Group1_NN b_NN -RRB-_-RRB- +_CC -LRB-_-LRB- Group2_NN b_NN -RRB-_-RRB- +_CC -LRB-_-LRB- Group3_NN b_NN -RRB-_-RRB- +_CC -LRB-_-LRB- Group4_NN b_NN -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- -LRB-_-LRB- Group1_NN w_NN -RRB-_-RRB- +_CC -LRB-_-LRB- Group2_NN w_NN -RRB-_-RRB- +_CC -LRB-_-LRB- Group3_NN w_NN -RRB-_-RRB- +_CC -LRB-_-LRB- Group4_NN w_NN -RRB-_-RRB- -RRB-_-RRB- The_DT values_NNS of_IN ,_, and_CC can_MD vary_VB to_TO form_VB any_DT desired_VBN linear_JJ combination_NN ;_: however_RB ,_, it_PRP is_VBZ important_JJ to_TO value_VB them_PRP with_IN the_DT <_JJR <_JJR ordering_VBG in_IN mind_NN -LRB-_-LRB- we_PRP used_VBD #_# ,_, #_# ,_, and_CC #_# as_IN their_PRP$ respective_JJ values_NNS -RRB-_-RRB- ._.
Groups_NNS of_IN #_# discs_NNS of_IN the_DT same_JJ color_NN means_VBZ victory_NN ,_, thus_RB discovery_NN of_IN such_PDT a_DT group_NN will_MD result_VB in_IN to_TO ensure_VB an_DT extreme_JJ value_NN ._.
We_PRP now_RB use_VBP our_PRP$ estimated_VBN evaluation_NN function_NN to_TO evaluate_VB the_DT Black_JJ player_NN ''_'' s_NNS actions_NNS during_IN the_DT Connect-Four_NNP adversarial_JJ interaction_NN ._.
Each_DT game_NN from_IN the_DT log_NN file_NN was_VBD input_NN into_IN the_DT application_NN ,_, which_WDT processed_VBN and_CC output_NN a_DT reformatted_VBN log_NN file_NN containing_VBG the_DT h_NN value_NN of_IN the_DT current_JJ move_NN ,_, the_DT maxh_JJ value_NN that_WDT could_MD be_VB achieved_VBN ,_, and_CC a_DT notification_NN if_IN an_DT open_JJ threat_NN was_VBD detected_VBN ._.
A_DT total_NN of_IN ###_CD games_NNS were_VBD analyzed_VBN -LRB-_-LRB- ##_CD with_IN White_NNP winning_VBG ,_, and_CC ##_NN with_IN Black_JJ winning_NN -RRB-_-RRB- ._.
A_DT few_JJ additional_JJ games_NNS were_VBD manually_RB ignored_VBN in_IN the_DT experiment_NN ,_, due_JJ to_TO these_DT problems_NNS :_: a_DT player_NN abandoning_VBG the_DT game_NN while_IN the_DT outcome_NN is_VBZ not_RB final_JJ ,_, or_CC a_DT blunt_JJ irrational_JJ move_NN in_IN the_DT early_JJ stages_NNS of_IN the_DT game_NN -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, not_RB blocking_VBG an_DT obvious_JJ winning_VBG group_NN in_IN the_DT first_JJ opening_NN moves_NNS -RRB-_-RRB- ._.
In_IN addition_NN ,_, a_DT single_JJ tie_NN game_NN was_VBD also_RB removed_VBN ._.
The_DT simulator_NN was_VBD run_VBN to_TO a_DT search_NN depth_NN of_IN #_# moves_NNS ._.
We_PRP now_RB proceed_VBP to_TO analyze_VB the_DT games_NNS with_IN respect_NN to_TO each_DT behavioral_JJ axiom_NN ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD Table_NNP #_# :_: Average_JJ heuristic_NN difference_NN analysis_NN Black_JJ losses_NNS Black_JJ Won_NNP Avg_NNP ''_'' minh_SYM -_: ##_NN ._.
##_NN -12_CD ._.
##_NN Avg_NN ''_'' #_# lowest_JJS h_NN moves_NNS -LRB-_-LRB- min3_CD h_NN -RRB-_-RRB- -_: ##_NN ._.
##_NN -8_CD ._.
##_NN 3_CD ._.
#_# ._.
#_# Affirming_VBG the_DT Suboptimal_JJ tactical_JJ move_NN axiom_NN The_DT following_VBG section_NN presents_VBZ the_DT heuristic_NN evaluations_NNS of_IN the_DT Min-Max_NNP algorithm_NN for_IN each_DT action_NN ,_, and_CC checks_NNS the_DT amount_NN and_CC extent_NN of_IN suboptimal_JJ tactical_JJ actions_NNS and_CC their_PRP$ implications_NNS on_IN performance_NN ._.
Table_NNP #_# shows_VBZ results_NNS and_CC insights_NNS from_IN the_DT games_NNS ''_'' heuristic_NN analysis_NN ,_, when_WRB search_NN depth_NN equals_VBZ #_# -LRB-_-LRB- this_DT search_NN depth_NN was_VBD selected_VBN for_IN the_DT results_NNS to_TO be_VB comparable_JJ to_TO -LSB-_-LRB- #_# -RSB-_-RRB- ,_, see_VB Section_NN #_# ._.
#_# ._.
#_# -RRB-_-RRB- ._.
The_DT table_NN ''_'' s_NNS heuristic_NN data_NNS is_VBZ the_DT difference_NN between_IN the_DT present_JJ maximal_JJ heuristic_NN value_NN and_CC the_DT heuristic_NN value_NN of_IN the_DT action_NN that_WDT was_VBD eventually_RB taken_VBN by_IN the_DT player_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT closer_JJR the_DT number_NN is_VBZ to_TO #_# ,_, the_DT closer_JJR the_DT action_NN was_VBD to_TO the_DT maximum_NN heuristic_NN action_NN -RRB-_-RRB- ._.
The_DT first_JJ row_NN presents_VBZ the_DT difference_NN values_NNS of_IN the_DT action_NN that_WDT had_VBD the_DT maximal_JJ difference_NN value_NN among_IN all_PDT the_DT Black_JJ player_NN ''_'' s_NNS actions_NNS in_IN a_DT given_VBN game_NN ,_, as_IN averaged_VBN over_IN all_DT Black_JJ ''_'' s_NNS winning_VBG and_CC losing_VBG games_NNS ._.
In_IN games_NNS in_IN which_WDT the_DT Black_JJ player_NN loses_VBZ ,_, its_PRP$ average_JJ difference_NN value_NN was_VBD -_: ##_NN ._.
##_NN ,_, while_IN in_IN games_NNS in_IN which_WDT the_DT Black_JJ player_NN won_VBD ,_, its_PRP$ average_NN was_VBD -_: ##_NN ._.
##_NN ._.
The_DT second_JJ row_NN expands_VBZ the_DT analysis_NN by_IN considering_VBG the_DT #_# highest_JJS heuristic_NN difference_NN actions_NNS ,_, and_CC averaging_VBG them_PRP ._.
In_IN that_DT case_NN ,_, we_PRP notice_VBP an_DT average_JJ heuristic_NN difference_NN of_IN #_# points_NNS between_IN games_NNS which_WDT the_DT Black_JJ player_NN loses_VBZ and_CC games_NNS in_IN which_WDT it_PRP wins_VBZ ._.
Nevertheless_RB ,_, the_DT importance_NN of_IN those_DT numbers_NNS is_VBZ that_IN they_PRP allowed_VBD us_PRP to_TO take_VB an_DT educated_VBN guess_NN on_IN a_DT threshold_NN number_NN of_IN ##_NN ._.
#_# ,_, as_IN the_DT value_NN of_IN the_DT TrH_NN constant_JJ ,_, which_WDT differentiates_VBZ between_IN normal_JJ actions_NNS and_CC highly_RB beneficial_JJ ones_NNS ._.
After_IN finding_VBG an_DT approximated_VBN TrH_NN constant_NN ,_, we_PRP can_MD proceed_VB with_IN an_DT analysis_NN of_IN the_DT importance_NN of_IN suboptimal_JJ moves_NNS ._.
To_TO do_VB so_RB we_PRP took_VBD the_DT subset_NN of_IN games_NNS in_IN which_WDT the_DT minimum_NN heuristic_NN difference_NN value_NN for_IN Black_JJ ''_'' s_NNS actions_NNS was_VBD ##_CD ._.
#_# ._.
As_IN presented_VBN in_IN Table_NNP #_# ,_, we_PRP can_MD see_VB the_DT different_JJ min3_NN h_NN average_NN of_IN the_DT #_# largest_JJS ranges_NNS and_CC the_DT respective_JJ percentage_NN of_IN games_NNS won_VBN ._.
The_DT first_JJ row_NN shows_VBZ that_IN the_DT Black_JJ player_NN won_VBD only_RB ##_CD %_NN of_IN the_DT games_NNS in_IN which_WDT the_DT average_NN of_IN its_PRP$ #_# highest_JJS heuristically_RB difference_NN actions_NNS -LRB-_-LRB- min3_CD h_NN -RRB-_-RRB- was_VBD smaller_JJR than_IN the_DT suggested_VBN threshold_NN ,_, TrH_NN =_JJ ##_CD ._.
#_# ._.
The_DT second_JJ row_NN shows_VBZ a_DT surprising_JJ result_NN :_: it_PRP seems_VBZ that_IN when_WRB min3_NN h_NN >_JJR #_# the_DT Black_JJ player_NN rarely_RB wins_VBZ ._.
Intuition_NN would_MD suggest_VB that_IN games_NNS in_IN which_WDT the_DT action_NN evaluation_NN values_NNS were_VBD closer_JJR to_TO the_DT maximal_JJ values_NNS will_MD result_VB in_IN more_JJR winning_VBG games_NNS for_IN Black_NNP ._.
However_RB ,_, it_PRP seems_VBZ that_IN in_IN the_DT Connect-Four_JJ domain_NN ,_, merely_RB responding_VBG with_IN somewhat_RB easily_RB expected_VBN actions_NNS ,_, without_IN initiating_VBG a_DT few_JJ surprising_JJ and_CC suboptimal_JJ moves_NNS ,_, does_VBZ not_RB yield_VB good_JJ results_NNS ._.
The_DT last_JJ row_NN sums_NNS up_IN the_DT main_JJ insights_NNS from_IN the_DT analysis_NN ;_: most_JJS of_IN Black_JJ ''_'' s_NNS wins_NNS -LRB-_-LRB- ##_CD %_NN -RRB-_-RRB- came_VBD when_WRB its_PRP$ min3_NN h_NN was_VBD in_IN the_DT range_NN of_IN -_: ##_NN ._.
#_# to_TO -4_CD ._.
A_DT close_JJ inspection_NN of_IN those_DT Black_JJ winning_VBG games_NNS shows_VBZ the_DT following_VBG pattern_NN behind_IN the_DT numbers_NNS :_: after_IN standard_JJ opening_NN moves_NNS ,_, Black_JJ suddenly_RB drops_VBZ a_DT disc_NN into_IN an_DT isolated_VBN column_NN ,_, which_WDT seems_VBZ a_DT waste_NN of_IN a_DT move_NN ._.
White_NNP continues_VBZ to_TO build_VB its_PRP$ threats_NNS ,_, while_IN usually_RB disregarding_VBG Black_JJ ''_'' s_NNS last_JJ move_NN ,_, which_WDT in_IN turn_NN uses_VBZ the_DT isolated_VBN disc_NN as_IN an_DT anchor_NN for_IN a_DT future_JJ winning_VBG threat_NN ._.
The_DT results_NNS show_VBP that_IN it_PRP was_VBD beneficial_JJ for_IN the_DT Black_JJ player_NN Table_NNP #_# :_: Black_JJ ''_'' s_NNS winnings_NNS percentages_NNS %_NN of_IN games_NNS min3_CD h_NN <_JJR ##_CD ._.
#_# ##_CD %_NN min3_CD h_NN >_JJR #_# #_# %_NN 11_CD ._.
#_# min3_CD h_NN #_# ##_CD %_NN to_TO take_VB suboptimal_JJ actions_NNS and_CC not_RB give_VB the_DT current_JJ highest_JJS possible_JJ heuristic_NN value_NN ,_, but_CC will_MD not_RB be_VB too_RB harmful_JJ for_IN its_PRP$ position_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, will_MD not_RB give_VB high_JJ beneficial_JJ value_NN to_TO its_PRP$ adversary_NN -RRB-_-RRB- ._.
As_IN it_PRP turned_VBD out_RP ,_, learning_VBG the_DT threshold_NN is_VBZ an_DT important_JJ aspect_NN of_IN success_NN :_: taking_VBG wildly_RB risky_JJ moves_NNS -LRB-_-LRB- min3_CD h_NN <_JJR ##_CD ._.
#_# -RRB-_-RRB- or_CC trying_VBG to_TO avoid_VB them_PRP -LRB-_-LRB- min3_NN h_NN >_JJR #_# -RRB-_-RRB- reduces_VBZ the_DT Black_JJ player_NN ''_'' s_NNS winning_VBG chances_NNS by_IN a_DT large_JJ margin_NN ._.
3_LS ._.
#_# ._.
#_# Affirming_VBG the_DT Profile_NNP Monitoring_NN Axiom_NN In_IN the_DT task_NN of_IN showing_VBG the_DT importance_NN of_IN monitoring_VBG one_CD ''_'' s_VBZ adversaries_NNS ''_'' profiles_NNS ,_, our_PRP$ log_NN files_NNS could_MD not_RB be_VB used_VBN because_IN they_PRP did_VBD not_RB contain_VB repeated_JJ interactions_NNS between_IN players_NNS ,_, which_WDT are_VBP needed_VBN to_TO infer_VB the_DT players_NNS ''_'' knowledge_NN about_IN their_PRP$ adversaries_NNS ._.
However_RB ,_, the_DT importance_NN of_IN opponent_NN modeling_NN and_CC its_PRP$ use_NN in_IN attaining_VBG tactical_JJ advantages_NNS was_VBD already_RB studied_VBN in_IN various_JJ domains_NNS -LRB-_-LRB- -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- are_VBP good_JJ examples_NNS -RRB-_-RRB- ._.
In_IN a_DT recent_JJ paper_NN ,_, Markovitch_NNP and_CC Reger_NNP -LSB-_-LRB- #_# -RSB-_-RRB- explored_VBD the_DT notion_NN of_IN learning_NN and_CC exploitation_NN of_IN opponent_NN weakness_NN in_IN competitive_JJ interactions_NNS ._.
They_PRP apply_VBP simple_JJ learning_NN strategies_NNS by_IN analyzing_VBG examples_NNS from_IN past_JJ interactions_NNS in_IN a_DT specific_JJ domain_NN ._.
They_PRP also_RB used_VBD the_DT Connect-Four_NNP adversarial_JJ domain_NN ,_, which_WDT can_MD now_RB be_VB used_VBN to_TO understand_VB the_DT importance_NN of_IN monitoring_VBG the_DT adversary_NN ''_'' s_NNS profile_NN ._.
Following_VBG the_DT presentation_NN of_IN their_PRP$ theoretical_JJ model_NN ,_, they_PRP describe_VBP an_DT extensive_JJ empirical_JJ study_NN and_CC check_VB the_DT agent_NN ''_'' s_NNS performance_NN after_IN learning_VBG the_DT weakness_NN model_NN with_IN past_JJ examples_NNS ._.
One_CD of_IN the_DT domains_NNS used_VBN as_IN a_DT competitive_JJ environment_NN was_VBD the_DT same_JJ Connect-Four_NN game_NN -LRB-_-LRB- Checkers_NNS was_VBD the_DT second_JJ domain_NN -RRB-_-RRB- ._.
Their_PRP$ heuristic_NN function_NN was_VBD identical_JJ to_TO ours_JJ with_IN three_CD different_JJ variations_NNS -LRB-_-LRB- H1_NN ,_, H2_NN ,_, and_CC H3_NN -RRB-_-RRB- that_WDT are_VBP distinguished_VBN from_IN one_CD another_DT in_IN their_PRP$ linear_JJ combination_NN coefficient_NN values_NNS ._.
The_DT search_NN depth_NN for_IN the_DT players_NNS was_VBD 3_CD -LRB-_-LRB- as_IN in_IN our_PRP$ analysis_NN -RRB-_-RRB- ._.
Their_PRP$ extensive_JJ experiments_NNS check_VBP and_CC compare_VBP various_JJ learning_VBG strategies_NNS ,_, risk_NN factors_NNS ,_, predefined_VBN feature_NN sets_NNS and_CC usage_NN methods_NNS ._.
The_DT bottom_JJ line_NN is_VBZ that_IN the_DT Connect-Four_JJ domain_NN shows_VBZ an_DT improvement_NN from_IN a_DT #_# ._.
###_CD winning_VBG rate_NN before_IN modeling_NN to_TO a_DT #_# ._.
##_NN after_IN modeling_NN -LRB-_-LRB- page_NN ##_NN -RRB-_-RRB- ._.
Their_PRP$ conclusions_NNS ,_, showing_VBG improved_JJ performance_NN when_WRB holding_VBG and_CC using_VBG the_DT adversary_NN ''_'' s_NNS model_NN ,_, justify_VB the_DT effort_NN to_TO monitor_VB the_DT adversary_NN profile_NN for_IN continuous_JJ and_CC repeated_JJ interactions_NNS ._.
An_DT additional_JJ point_NN that_WDT came_VBD up_RP in_IN their_PRP$ experiments_NNS is_VBZ the_DT following_NN :_: after_IN the_DT opponent_NN weakness_NN model_NN has_VBZ been_VBN learned_VBN ,_, the_DT authors_NNS describe_VBP different_JJ methods_NNS of_IN integrating_VBG the_DT opponent_NN weakness_NN model_NN into_IN the_DT agent_NN ''_'' s_NNS decision_NN strategy_NN ._.
Nevertheless_RB ,_, regardless_RB of_IN the_DT specific_JJ method_NN they_PRP chose_VBD to_TO work_VB with_IN ,_, all_DT integration_NN methods_NNS might_MD cause_VB the_DT agent_NN to_TO take_VB suboptimal_JJ decisions_NNS ;_: it_PRP might_MD cause_VB the_DT agent_NN to_TO prefer_VB actions_NNS that_WDT are_VBP suboptimal_JJ at_IN the_DT present_JJ decision_NN junction_NN ,_, but_CC which_WDT might_MD cause_VB the_DT opponent_NN to_TO react_VB in_IN accordance_NN with_IN its_PRP$ weakness_NN model_NN -LRB-_-LRB- as_IN represented_VBN by_IN our_PRP$ agent_NN -RRB-_-RRB- which_WDT in_IN turn_NN will_MD be_VB beneficial_JJ for_IN us_PRP in_IN the_DT future_NN ._.
The_DT agent_NN ''_'' s_NNS behavior_NN ,_, as_IN demonstrated_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- further_RBR confirms_VBZ and_CC strengthens_VBZ our_PRP$ Suboptimal_JJ Tactical_NNP Axiom_NNP as_IN discussed_VBN in_IN the_DT previous_JJ section_NN ._.
556_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- 3_CD ._.
#_# ._.
#_# Additional_JJ Insights_NNPS The_DT need_NN for_IN the_DT Goal_NNP Achieving_NNP ,_, Preventive_NNP Act_NNP ,_, and_CC Evaluation_NN Maximization_NN axioms_NNS are_VBP obvious_JJ ,_, and_CC need_VB no_DT further_JJ verification_NN ._.
However_RB ,_, even_RB with_IN respect_NN to_TO those_DT axioms_NNS ,_, a_DT few_JJ interesting_JJ insights_NNS came_VBD up_RP in_IN the_DT log_NN analysis_NN ._.
The_DT Goal_NNP achieving_VBG and_CC Preventive_NNP Act_NNP axioms_NNS ,_, though_IN theoretically_RB trivial_JJ ,_, seem_VBP to_TO provide_VB some_DT challenge_NN to_TO a_DT human_JJ player_NN ._.
In_IN the_DT initial_JJ inspection_NN of_IN the_DT logs_NNS ,_, we_PRP encountered_VBD few_JJ games2_NN where_WRB a_DT player_NN ,_, for_IN inexplicable_JJ reasons_NNS ,_, did_VBD not_RB block_VB the_DT other_JJ from_IN winning_VBG or_CC failed_VBN to_TO execute_VB its_PRP$ own_JJ winning_VBG move_NN ._.
We_PRP can_MD blame_VB those_DT faults_NNS on_IN the_DT human_JJ ''_'' s_NNS lack_VBP of_IN attention_NN ,_, or_CC a_DT typing_NN error_NN in_IN its_PRP$ move_NN reply_NN ;_: nevertheless_RB ,_, those_DT errors_NNS might_MD occur_VB in_IN bounded_VBN rational_JJ agents_NNS ,_, and_CC the_DT appropriate_JJ behavior_NN needs_VBZ to_TO be_VB axiomatized_VBN ._.
A_DT typical_JJ Connect-Four_NNP game_NN revolves_VBZ around_IN generating_VBG threats_NNS and_CC blocking_VBG them_PRP ._.
In_IN our_PRP$ analysis_NN we_PRP looked_VBD for_IN explicit_JJ preventive_JJ actions_NNS ,_, i_FW ._.
e_LS ._.
,_, moves_VBZ that_IN block_VBP a_DT group_NN of_IN 3_CD discs_NNS ,_, or_CC that_DT remove_VB a_DT future_JJ threat_NN -LRB-_-LRB- in_IN our_PRP$ limited_JJ search_NN horizon_NN -RRB-_-RRB- ._.
We_PRP found_VBD that_IN in_IN ##_CD %_NN of_IN the_DT total_JJ games_NNS there_EX was_VBD at_IN least_JJS one_CD preventive_JJ action_NN taken_VBN by_IN the_DT Black_JJ player_NN ._.
It_PRP was_VBD also_RB found_VBN that_IN Black_JJ averaged_VBD #_# ._.
#_# preventive_JJ actions_NNS per_IN game_NN on_IN the_DT games_NNS in_IN which_WDT it_PRP lost_VBD ,_, while_IN averaging_VBG #_# ._.
#_# preventive_JJ actions_NNS per_IN game_NN when_WRB winning_VBG ._.
It_PRP seems_VBZ that_IN Black_JJ requires_VBZ #_# or_CC #_# preventive_JJ actions_NNS to_TO build_VB its_PRP$ initial_JJ taking_VBG position_NN ,_, before_IN starting_VBG to_TO present_JJ threats_NNS ._.
If_IN it_PRP did_VBD not_RB manage_VB to_TO win_VB ,_, it_PRP will_MD usually_RB prevent_VB an_DT extra_JJ threat_NN or_CC two_CD before_IN succumbing_VBG to_TO White_NNP ._.
4_LS ._.
RELATED_JJ WORK_VBP Much_JJ research_NN deals_NNS with_IN the_DT axiomatization_NN of_IN teamwork_NN and_CC mental_JJ states_NNS of_IN individuals_NNS :_: some_DT models_NNS use_VBP knowledge_NN and_CC belief_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, others_NNS have_VBP models_NNS of_IN goals_NNS and_CC intentions_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
However_RB ,_, all_PDT these_DT formal_JJ theories_NNS deal_VBP with_IN agent_NN teamwork_NN and_CC cooperation_NN ._.
As_IN far_RB as_IN we_PRP know_VBP ,_, our_PRP$ model_NN is_VBZ the_DT first_JJ to_TO provide_VB a_DT formalized_VBN model_NN for_IN explicit_JJ adversarial_JJ environments_NNS and_CC agents_NNS ''_'' behavior_NN in_IN it_PRP ._.
The_DT classical_JJ Min-Max_NNP adversarial_JJ search_NN algorithm_NN was_VBD the_DT first_JJ attempt_NN to_TO integrate_VB the_DT opponent_NN into_IN the_DT search_NN space_NN with_IN a_DT weak_JJ assumption_NN of_IN an_DT optimally_RB playing_VBG opponent_NN ._.
Since_IN then_RB ,_, much_JJ effort_NN has_VBZ gone_VBN into_IN integrating_VBG the_DT opponent_NN model_NN into_IN the_DT decision_NN procedure_NN to_TO predict_VB future_JJ behavior_NN ._.
The_DT M_NN algorithm_NN presented_VBN by_IN Carmel_NNP and_CC Markovitch_NNP -LSB-_-LRB- #_# -RSB-_-RRB- showed_VBD a_DT method_NN of_IN incorporating_VBG opponent_NN models_NNS into_IN adversary_NN search_NN ,_, while_IN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- they_PRP used_VBD learning_VBG to_TO provide_VB a_DT more_RBR accurate_JJ opponent_NN model_NN in_IN a_DT 2player_JJR repeated_VBN game_NN environment_NN ,_, where_WRB agents_NNS ''_'' strategies_NNS were_VBD modeled_VBN as_IN finite_JJ automata_NN ._.
Additional_JJ Adversarial_JJ planning_NN work_NN was_VBD done_VBN by_IN Willmott_NNP et_FW al_FW ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- ,_, which_WDT provided_VBD an_DT adversarial_JJ planning_NN approach_NN to_TO the_DT game_NN of_IN GO_NN ._.
The_DT research_NN mentioned_VBN above_IN dealt_VBN with_IN adversarial_JJ search_NN and_CC the_DT integration_NN of_IN opponent_NN models_NNS into_IN classical_JJ utilitybased_JJ search_NN methods_NNS ._.
That_DT work_NN shows_VBZ the_DT importance_NN of_IN opponent_NN modeling_NN and_CC the_DT ability_NN to_TO exploit_VB it_PRP to_TO an_DT agent_NN ''_'' s_NNS advantage_NN ._.
However_RB ,_, the_DT basic_JJ limitations_NNS of_IN those_DT search_NN methods_NNS still_RB apply_VB ;_: our_PRP$ model_NN tries_VBZ to_TO overcome_VB those_DT limitations_NNS by_IN presenting_VBG a_DT formal_JJ model_NN for_IN a_DT new_JJ ,_, mental_JJ state-based_JJ adversarial_JJ specification_NN ._.
5_CD ._.
CONCLUSIONS_NNS We_PRP presented_VBD an_DT Adversarial_NNP Environment_NNP model_NN for_IN a_DT 2_CD These_DT were_VBD later_RB removed_VBN from_IN the_DT final_JJ analysis_NN ._.
bounded_VBN rational_JJ agent_NN that_WDT is_VBZ situated_VBN in_IN an_DT N-player_NN ,_, zerosum_NN environment_NN ._.
We_PRP used_VBD the_DT SharedPlans_NNPS formalization_NN to_TO define_VB the_DT model_NN and_CC the_DT axioms_NNS that_WDT agents_NNS can_MD apply_VB as_IN behavioral_JJ guidelines_NNS ._.
The_DT model_NN is_VBZ meant_VBN to_TO be_VB used_VBN as_IN a_DT guideline_NN for_IN designing_VBG agents_NNS that_WDT need_VBP to_TO operate_VB in_IN such_JJ adversarial_JJ environments_NNS ._.
We_PRP presented_VBD empirical_JJ results_NNS ,_, based_VBN on_IN ConnectFour_NNP log_NN file_NN analysis_NN ,_, that_WDT exemplify_VBP the_DT model_NN and_CC the_DT axioms_NNS for_IN a_DT bilateral_JJ instance_NN of_IN the_DT environment_NN ._.
The_DT results_NNS we_PRP presented_VBD are_VBP a_DT first_JJ step_NN towards_IN an_DT expanded_VBN model_NN that_WDT will_MD cover_VB all_DT types_NNS of_IN adversarial_JJ environments_NNS ,_, for_IN example_NN ,_, environments_NNS that_WDT are_VBP non-zero-sum_JJ ,_, and_CC environments_NNS that_WDT contain_VBP natural_JJ agents_NNS that_WDT are_VBP not_RB part_NN of_IN the_DT direct_JJ conflict_NN ._.
Those_DT challenges_NNS and_CC more_JJR will_MD be_VB dealt_VBN with_IN in_IN future_JJ research_NN ._.
6_CD ._.
ACKNOWLEDGMENT_NNP This_DT research_NN was_VBD supported_VBN in_IN part_NN by_IN Israel_NNP Science_NNP Foundation_NNP grants_NNS #_# ####_CD /_: ##_CD and_CC #_# ###_CD /_: ##_CD ._.
7_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- L_NN ._.
V_NN ._.
Allis_NNP ._.
A_DT knowledge-based_JJ approach_NN of_IN Connect-Four_NNP -_: the_DT game_NN is_VBZ solved_VBN :_: White_NNP wins_VBZ ._.
Master_NN ''_'' s_NNS thesis_NN ,_, Free_NNP University_NNP ,_, Amsterdam_NNP ,_, The_DT Netherlands_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Carmel_NNP and_CC S_NN ._.
Markovitch_NNP ._.
Incorporating_VBG opponent_NN models_NNS into_IN adversary_NN search_NN ._.
In_IN Proceedings_NNP of_IN the_DT Thirteenth_NNP National_NNP Conference_NNP on_IN Artificial_NNP Intelligence_NNP ,_, pages_NNS 120-125_CD ,_, Portland_NNP ,_, OR_NN ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Carmel_NNP and_CC S_NN ._.
Markovitch_NNP ._.
Opponent_NN modeling_NN in_IN multi-agent_JJ systems_NNS ._.
In_IN G_NN ._.
Wei_NNP and_CC S_NN ._.
Sen_NNP ,_, editors_NNS ,_, Adaptation_NN and_CC Learning_NNP in_IN Multi-Agent_NNP Systems_NNPS ,_, pages_NNS 40-52_CD ._.
Springer-Verlag_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- B_NN ._.
J_NN ._.
Grosz_NNP and_CC S_NN ._.
Kraus_NNP ._.
Collaborative_JJ plans_NNS for_IN complex_JJ group_NN action_NN ._.
Artificial_JJ Intelligence_NNP ,_, 86_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 269-357_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- M_NN ._.
Hadad_NNP ,_, G_NNP ._.
Kaminka_NNP ,_, G_NNP ._.
Armon_NNP ,_, and_CC S_NN ._.
Kraus_NNP ._.
Supporting_VBG collaborative_JJ activity_NN ._.
In_IN Proc_NNP ._.
of_IN AAAI-2005_NN ,_, pages_NNS 83-88_CD ,_, Pittsburgh_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- http_NN :_: /_: /_: www_NN ._.
gamerz_NN ._.
net_JJ /_: pbmserv_NN /_: ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
Kraus_NNP and_CC D_NNP ._.
Lehmann_NNP ._.
Designing_NNP and_CC building_VBG a_DT negotiating_VBG automated_VBN agent_NN ._.
Computational_JJ Intelligence_NNP ,_, ##_CD :_: 132-171_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- H_NN ._.
J_NN ._.
Levesque_NNP ,_, P_NN ._.
R_NN ._.
Cohen_NNP ,_, and_CC J_NN ._.
H_NN ._.
T_NN ._.
Nunes_NNP ._.
On_IN acting_VBG together_RB ._.
In_IN Proc_NNP ._.
of_IN AAAI-90_NN ,_, pages_NNS 94-99_CD ,_, Boston_NNP ,_, MA_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
Markovitch_NNP and_CC R_NN ._.
Reger_NNP ._.
Learning_NNP and_CC exploiting_VBG relative_JJ weaknesses_NNS of_IN opponent_NN agents_NNS ._.
Autonomous_JJ Agents_NNS and_CC Multi-Agent_NNP Systems_NNPS ,_, ##_NN -LRB-_-LRB- #_# -RRB-_-RRB- :_: 103-130_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
M_NN ._.
Ronald_NNP Fagin_NNP ,_, Joseph_NNP Y_NN ._.
Halpern_NNP and_CC M_NN ._.
Y_NN ._.
Vardi_NNP ._.
Reasoning_NN about_IN knowledge_NN ._.
MIT_NNP Press_NNP ,_, Cambridge_NNP ,_, Mass_NNP ._.
,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- P_NN ._.
Thagard_NNP ._.
Adversarial_JJ problem_NN solving_VBG :_: Modeling_VBG an_DT oponent_NN using_VBG explanatory_JJ coherence_NN ._.
Cognitive_JJ Science_NN ,_, ##_NN -LRB-_-LRB- #_# -RRB-_-RRB- :_: 123-149_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- R_NN ._.
W_NN ._.
Toseland_NNP and_CC R_NN ._.
F_NN ._.
Rivas_NNP ._.
An_DT Introduction_NN to_TO Group_NNP Work_NNP Practice_NNP ._.
Prentice_NNP Hall_NNP ,_, Englewood_NNP Cliffs_NNPS ,_, NJ_NNP ,_, 2nd_JJ edition_NN edition_NN ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Willmott_NNP ,_, J_NNP ._.
Richardson_NNP ,_, A_NNP ._.
Bundy_NNP ,_, and_CC J_NN ._.
Levine_NNP ._.
An_DT adversarial_JJ planning_NN approach_NN to_TO Go_VB ._.
Lecture_NNP Notes_NNP in_IN Computer_NNP Science_NNP ,_, ####_CD :_: 93-112_CD ,_, ####_CD ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD
