Feature_NN Representation_NN for_IN Effective_JJ Action-Item_NNP Detection_NN Paul_NNP N_NNP ._.
Bennett_NNP Computer_NNP Science_NNP Department_NNP Carnegie_NNP Mellon_NNP University_NNP Pittsburgh_NNP ,_, PA_NN #####_CD pbennett_NN +_CC @_SYM cs_NNS ._.
cmu_NN ._.
edu_NNP Jaime_NNP Carbonell_NNP Language_NNP Technologies_NNP Institute_NNP ._.
Carnegie_NNP Mellon_NNP University_NNP Pittsburgh_NNP ,_, PA_NN #####_CD jgc_NN +_CC @_SYM cs_NNS ._.
cmu_NN ._.
edu_NN ABSTRACT_NN E-mail_NN users_NNS face_VBP an_DT ever-growing_JJ challenge_NN in_IN managing_VBG their_PRP$ inboxes_NNS due_JJ to_TO the_DT growing_VBG centrality_NN of_IN email_NN in_IN the_DT workplace_NN for_IN task_NN assignment_NN ,_, action_NN requests_NNS ,_, and_CC other_JJ roles_NNS beyond_IN information_NN dissemination_NN ._.
Whereas_IN Information_NNP Retrieval_NNP and_CC Machine_NNP Learning_NNP techniques_NNS are_VBP gaining_VBG initial_JJ acceptance_NN in_IN spam_NN filtering_VBG and_CC automated_VBN folder_NN assignment_NN ,_, this_DT paper_NN reports_VBZ on_IN a_DT new_JJ task_NN :_: automated_VBN action-item_NN detection_NN ,_, in_IN order_NN to_TO flag_NN emails_NNS that_WDT require_VBP responses_NNS ,_, and_CC to_TO highlight_VB the_DT specific_JJ passage_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- indicating_VBG the_DT request_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- for_IN action_NN ._.
Unlike_IN standard_JJ topic-driven_JJ text_NN classification_NN ,_, action-item_NN detection_NN requires_VBZ inferring_VBG the_DT sender_NN ''_'' s_NNS intent_NN ,_, and_CC as_IN such_JJ responds_VBZ less_JJR well_RB to_TO pure_JJ bag-of-words_NNS classification_NN ._.
However_RB ,_, using_VBG enriched_JJ feature_NN sets_NNS ,_, such_JJ as_IN n-grams_NN -LRB-_-LRB- up_RB to_TO n_NN =_JJ #_# -RRB-_-RRB- with_IN chi-squared_JJ feature_NN selection_NN ,_, and_CC contextual_JJ cues_NNS for_IN action-item_NN location_NN improve_VB performance_NN by_IN up_RB to_TO ##_CD %_NN over_IN unigrams_NNS ,_, using_VBG in_IN both_DT cases_NNS state_NN of_IN the_DT art_NN classifiers_NNS such_JJ as_IN SVMs_NNS with_IN automated_VBN model_NN selection_NN via_IN embedded_JJ cross-validation_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNPS H_NN ._.
#_# ._.
#_# -LSB-_-LRB- Information_NNP Storage_NNP and_CC Retrieval_NNP -RSB-_-RRB- :_: Information_NNP Search_VB and_CC Retrieval_NNP ;_: I_PRP ._.
#_# ._.
#_# -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Learning_NNP ;_: I_PRP ._.
#_# ._.
#_# -LSB-_-LRB- Pattern_NNP Recognition_NN -RSB-_-RRB- :_: Applications_NNS General_NNP Terms_NNS Experimentation_NN 1_CD ._.
INTRODUCTION_NN E-mail_NN users_NNS are_VBP facing_VBG an_DT increasingly_RB difficult_JJ task_NN of_IN managing_VBG their_PRP$ inboxes_NNS in_IN the_DT face_NN of_IN mounting_VBG challenges_NNS that_IN result_VBP from_IN rising_VBG e-mail_JJ usage_NN ._.
This_DT includes_VBZ prioritizing_VBG e-mails_NNS over_IN a_DT range_NN of_IN sources_NNS from_IN business_NN partners_NNS to_TO family_NN members_NNS ,_, filtering_VBG and_CC reducing_VBG junk_NN e-mail_NN ,_, and_CC quickly_RB managing_VBG requests_NNS that_WDT demand_VBP From_IN :_: Henry_NNP Hutchins_NNP To_TO :_: Sara_NNP Smith_NNP ;_: Joe_NNP Johnson_NNP ;_: William_NNP Woolings_NNP Subject_NNP :_: meeting_NN with_IN prospective_JJ customers_NNS Sent_VBD :_: Fri_NNP ##_CD /_: ##_CD /_: ####_CD #_# :_: ##_CD AM_NNP Hi_NNP All_NNP ,_, I_PRP ''_'' d_NN like_IN to_TO remind_VB all_DT of_IN you_PRP that_IN the_DT group_NN from_IN GRTY_NNP will_MD be_VB visiting_VBG us_PRP next_IN Friday_NNP at_IN #_# :_: ##_CD p_NN ._.
m_NN ._.
The_DT current_JJ schedule_NN looks_VBZ like_IN this_DT :_: +_CC #_# :_: ##_VB a_DT ._.
m_NN ._.
Informal_JJ Breakfast_NN and_CC Discussion_NN in_IN Cafeteria_NNP +_CC ##_CD :_: ##_CD a_DT ._.
m_NN ._.
Company_NN Overview_NN +_CC ##_CD :_: ##_CD a_DT ._.
m_NN ._.
Individual_JJ Meetings_NNS -LRB-_-LRB- Continue_NNP Over_IN Lunch_NNP -RRB-_-RRB- +_CC #_# :_: ##_CD p_NN ._.
m_NN ._.
Tour_NN of_IN Facilities_NNP +_CC #_# :_: ##_CD p_NN ._.
m_NN ._.
Sales_NNS Pitch_NNP In_IN order_NN to_TO have_VB this_DT go_VB off_RP smoothly_RB ,_, I_PRP would_MD like_VB to_TO practice_VB the_DT presentation_NN well_RB in_IN advance_NN ._.
As_IN a_DT result_NN ,_, I_PRP will_MD need_VB each_DT of_IN your_PRP$ parts_NNS by_IN Wednesday_NNP ._.
Keep_VB up_RP the_DT good_JJ work_NN !_.
-_: Henry_NNP Figure_NNP #_# :_: An_DT E-mail_NN with_IN emphasized_VBN Action-Item_NNP ,_, an_DT explicit_JJ request_NN that_WDT requires_VBZ the_DT recipient_JJ ''_'' s_NNS attention_NN or_CC action_NN ._.
the_DT receiver_NN ''_'' s_NNS attention_NN or_CC action_NN ._.
Automated_VBN action-item_NN detection_NN targets_VBZ the_DT third_JJ of_IN these_DT problems_NNS by_IN attempting_VBG to_TO detect_VB which_WDT e-mails_NNS require_VBP an_DT action_NN or_CC response_NN with_IN information_NN ,_, and_CC within_IN those_DT e-mails_NNS ,_, attempting_VBG to_TO highlight_VB the_DT sentence_NN -LRB-_-LRB- or_CC other_JJ passage_NN length_NN -RRB-_-RRB- that_WDT directly_RB indicates_VBZ the_DT action_NN request_NN ._.
Such_PDT a_DT detection_NN system_NN can_MD be_VB used_VBN as_IN one_CD part_NN of_IN an_DT e-mail_JJ agent_NN which_WDT would_MD assist_VB a_DT user_NN in_IN processing_NN important_JJ e-mails_NNS quicker_JJR than_IN would_MD have_VB been_VBN possible_JJ without_IN the_DT agent_NN ._.
We_PRP view_VBP action-item_NN detection_NN as_IN one_CD necessary_JJ component_NN of_IN a_DT successful_JJ e-mail_JJ agent_NN which_WDT would_MD perform_VB spam_NN detection_NN ,_, action-item_NN detection_NN ,_, topic_NN classification_NN and_CC priority_NN ranking_NN ,_, among_IN other_JJ functions_NNS ._.
The_DT utility_NN of_IN such_PDT a_DT detector_NN can_MD manifest_VB as_IN a_DT method_NN of_IN prioritizing_VBG e-mails_NNS according_VBG to_TO task-oriented_JJ criteria_NNS other_JJ than_IN the_DT standard_JJ ones_NNS of_IN topic_NN and_CC sender_NN or_CC as_IN a_DT means_NN of_IN ensuring_VBG that_IN the_DT email_NN user_NN hasn_NN ''_'' t_NN dropped_VBD the_DT proverbial_JJ ball_NN by_IN forgetting_VBG to_TO address_VB an_DT action_NN request_NN ._.
Action-item_NN detection_NN differs_VBZ from_IN standard_JJ text_NN classification_NN in_IN two_CD important_JJ ways_NNS ._.
First_RB ,_, the_DT user_NN is_VBZ interested_JJ both_CC in_IN detecting_VBG whether_IN an_DT email_NN contains_VBZ action_NN items_NNS and_CC in_IN locating_VBG exactly_RB where_WRB these_DT action_NN item_NN requests_NNS are_VBP contained_VBN within_IN the_DT email_NN body_NN ._.
In_IN contrast_NN ,_, standard_JJ text_NN categorization_NN merely_RB assigns_VBZ a_DT topic_NN label_NN to_TO each_DT text_NN ,_, whether_IN that_DT label_NN corresponds_VBZ to_TO an_DT e-mail_JJ folder_NN or_CC a_DT controlled_JJ indexing_NN vocabulary_NN -LSB-_-LRB- ##_CD ,_, ##_CD ,_, ##_CD -RSB-_-RRB- ._.
Second_RB ,_, action-item_NN detection_NN attempts_VBZ to_TO recover_VB the_DT email_NN sender_NN ''_'' s_NNS intent_NN -_: whether_IN she_PRP means_VBZ to_TO elicit_VB response_NN or_CC action_NN on_IN the_DT part_NN of_IN the_DT receiver_NN ;_: note_NN that_IN for_IN this_DT task_NN ,_, classifiers_NNS using_VBG only_RB unigrams_VBZ as_IN features_NNS do_VBP not_RB perform_VB optimally_RB ,_, as_IN evidenced_VBN in_IN our_PRP$ results_NNS below_IN ._.
Instead_RB we_PRP find_VBP that_IN we_PRP need_VBP more_RBR information-laden_JJ features_NNS such_JJ as_IN higher-order_JJ n-grams_NNS ._.
Text_VB categorization_NN by_IN topic_NN ,_, on_IN the_DT other_JJ hand_NN ,_, works_VBZ very_RB well_RB using_VBG just_RB individual_JJ words_NNS as_IN features_NNS -LSB-_-LRB- #_# ,_, #_# ,_, ##_NN ,_, ##_NN -RSB-_-RRB- ._.
In_IN fact_NN ,_, genre-classification_NN ,_, which_WDT one_PRP would_MD think_VB may_MD require_VB more_JJR than_IN a_DT bag-of-words_JJ approach_NN ,_, also_RB works_VBZ quite_RB well_RB using_VBG just_RB unigram_JJ features_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Topic_NN detection_NN and_CC tracking_NN -LRB-_-LRB- TDT_NN -RRB-_-RRB- ,_, also_RB works_VBZ well_RB with_IN unigram_NN feature_NN sets_VBZ -LSB-_-LRB- #_# ,_, ##_NN -RSB-_-RRB- ._.
We_PRP believe_VBP that_IN action-item_NN detection_NN is_VBZ one_CD of_IN the_DT first_JJ clear_JJ instances_NNS of_IN an_DT IR-related_JJ task_NN where_WRB we_PRP must_MD move_VB beyond_IN bag-of-words_NNS to_TO achieve_VB high_JJ performance_NN ,_, albeit_IN not_RB too_RB far_RB ,_, as_IN bag-of-n-grams_NNS seem_VBP to_TO suffice_VB ._.
We_PRP first_RB review_VB related_JJ work_NN for_IN similar_JJ text_NN classification_NN problems_NNS such_JJ as_IN e-mail_JJ priority_NN ranking_NN and_CC speech_NN act_VBP identification_NN ._.
Then_RB we_PRP more_RBR formally_RB define_VB the_DT action-item_NN detection_NN problem_NN ,_, discuss_VBP the_DT aspects_NNS that_WDT distinguish_VBP it_PRP from_IN more_JJR common_JJ problems_NNS like_IN topic_NN classification_NN ,_, and_CC highlight_VB the_DT challenges_NNS in_IN constructing_VBG systems_NNS that_WDT can_MD perform_VB well_RB at_IN the_DT sentence_NN and_CC document_NN level_NN ._.
From_IN there_RB ,_, we_PRP move_VBP to_TO a_DT discussion_NN of_IN feature_NN representation_NN and_CC selection_NN techniques_NNS appropriate_JJ for_IN this_DT problem_NN and_CC how_WRB standard_JJ text_NN classification_NN approaches_NNS can_MD be_VB adapted_VBN to_TO smoothly_RB move_VB from_IN the_DT sentence-level_JJ detection_NN problem_NN to_TO the_DT documentlevel_NN classification_NN problem_NN ._.
We_PRP then_RB conduct_VB an_DT empirical_JJ analysis_NN that_IN helps_VBZ us_PRP determine_VB the_DT effectiveness_NN of_IN our_PRP$ feature_NN extraction_NN procedures_NNS as_RB well_RB as_IN establish_VB baselines_NNS for_IN a_DT number_NN of_IN classification_NN algorithms_NNS on_IN this_DT task_NN ._.
Finally_RB ,_, we_PRP summarize_VBP this_DT paper_NN ''_'' s_NNS contributions_NNS and_CC consider_VB interesting_JJ directions_NNS for_IN future_JJ work_NN ._.
2_LS ._.
RELATED_JJ WORK_VBP Several_JJ other_JJ researchers_NNS have_VBP considered_VBN very_RB similar_JJ text_NN classification_NN tasks_NNS ._.
Cohen_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- describe_VBP an_DT ontology_NN of_IN speech_NN acts_VBZ ,_, such_JJ as_IN Propose_NNP a_DT Meeting_VBG ,_, and_CC attempt_NN to_TO predict_VB when_WRB an_DT e-mail_NN contains_VBZ one_CD of_IN these_DT speech_NN acts_VBZ ._.
We_PRP consider_VBP action-items_NNS to_TO be_VB an_DT important_JJ specific_JJ type_NN of_IN speech_NN act_VBP that_IN falls_VBZ within_IN their_PRP$ more_JJR general_JJ classification_NN ._.
While_IN they_PRP provide_VBP results_NNS for_IN several_JJ classification_NN methods_NNS ,_, their_PRP$ methods_NNS only_RB make_VBP use_NN of_IN human_JJ judgments_NNS at_IN the_DT document-level_NN ._.
In_IN contrast_NN ,_, we_PRP consider_VBP whether_IN accuracy_NN can_MD be_VB increased_VBN by_IN using_VBG finer-grained_JJ human_JJ judgments_NNS that_WDT mark_VBP the_DT specific_JJ sentences_NNS and_CC phrases_NNS of_IN interest_NN ._.
Corston-Oliver_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- consider_VB detecting_VBG items_NNS in_IN e-mail_NN to_TO Put_VB on_RP a_DT To-Do_NN List_NN ._.
This_DT classification_NN task_NN is_VBZ very_RB similar_JJ to_TO ours_JJ except_IN they_PRP do_VBP not_RB consider_VB simple_JJ factual_JJ questions_NNS to_TO belong_VB to_TO this_DT category_NN ._.
We_PRP include_VBP questions_NNS ,_, but_CC note_VBP that_IN not_RB all_DT questions_NNS are_VBP action-items_JJ -_: some_DT are_VBP rhetorical_JJ or_CC simply_RB social_JJ convention_NN ,_, How_WRB are_VBP you_PRP ?_. ._.
From_IN a_DT learning_NN perspective_NN ,_, while_IN they_PRP make_VBP use_NN of_IN judgments_NNS at_IN the_DT sentence-level_NN ,_, they_PRP do_VBP not_RB explicitly_RB compare_VB what_WP if_IN any_DT benefits_NNS finer-grained_JJ judgments_NNS offer_VBP ._.
Additionally_RB ,_, they_PRP do_VBP not_RB study_VB alternative_JJ choices_NNS or_CC approaches_NNS to_TO the_DT classification_NN task_NN ._.
Instead_RB ,_, they_PRP simply_RB apply_VB a_DT standard_JJ SVM_NN at_IN the_DT sentence-level_NN and_CC focus_NN primarily_RB on_IN a_DT linguistic_JJ analysis_NN of_IN how_WRB the_DT sentence_NN can_MD be_VB logically_RB reformulated_VBN before_IN adding_VBG it_PRP to_TO the_DT task_NN list_NN ._.
In_IN this_DT study_NN ,_, we_PRP examine_VBP several_JJ alternative_JJ classification_NN methods_NNS ,_, compare_VBP document-level_JJ and_CC sentence-level_JJ approaches_NNS and_CC analyze_VBP the_DT machine_NN learning_VBG issues_NNS implicit_JJ in_IN these_DT problems_NNS ._.
Interest_NN in_IN a_DT variety_NN of_IN learning_VBG tasks_NNS related_VBN to_TO e-mail_VB has_VBZ been_VBN rapidly_RB growing_VBG in_IN the_DT recent_JJ literature_NN ._.
For_IN example_NN ,_, in_IN a_DT forum_NN dedicated_VBN to_TO e-mail_VB learning_VBG tasks_NNS ,_, Culotta_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- presented_VBD methods_NNS for_IN learning_VBG social_JJ networks_NNS from_IN e-mail_NN ._.
In_IN this_DT work_NN ,_, we_PRP do_VBP not_RB focus_VB on_IN peer_VB relationships_NNS ;_: however_RB ,_, such_JJ methods_NNS could_MD complement_VB those_DT here_RB since_IN peer_VB relationships_NNS often_RB influence_VBP word_NN choice_NN when_WRB requesting_VBG an_DT action_NN ._.
3_LS ._.
PROBLEM_NN DEFINITION_NNP &_CC APPROACH_NNP In_IN contrast_NN to_TO previous_JJ work_NN ,_, we_PRP explicitly_RB focus_VBP on_IN the_DT benefits_NNS that_WDT finer-grained_VBD ,_, more_RBR costly_JJ ,_, sentence-level_JJ human_JJ judgments_NNS offer_VBP over_IN coarse-grained_JJ document-level_JJ judgments_NNS ._.
Additionally_RB ,_, we_PRP consider_VBP multiple_JJ standard_JJ text_NN classification_NN approaches_NNS and_CC analyze_VBP both_CC the_DT quantitative_JJ and_CC qualitative_JJ differences_NNS that_WDT arise_VBP from_IN taking_VBG a_DT document-level_NN vs_CC ._.
a_DT sentence-level_JJ approach_NN to_TO classification_NN ._.
Finally_RB ,_, we_PRP focus_VBP on_IN the_DT representation_NN necessary_JJ to_TO achieve_VB the_DT most_RBS competitive_JJ performance_NN ._.
3_LS ._.
#_# Problem_NNP Definition_NNP In_IN order_NN to_TO provide_VB the_DT most_RBS benefit_VB to_TO the_DT user_NN ,_, a_DT system_NN would_MD not_RB only_RB detect_VB the_DT document_NN ,_, but_CC it_PRP would_MD also_RB indicate_VB the_DT specific_JJ sentences_NNS in_IN the_DT e-mail_NN which_WDT contain_VBP the_DT action-items_NNS ._.
Therefore_RB ,_, there_EX are_VBP three_CD basic_JJ problems_NNS :_: 1_CD ._.
Document_NNP detection_NN :_: Classify_VB a_DT document_NN as_IN to_TO whether_IN or_CC not_RB it_PRP contains_VBZ an_DT action-item_NN ._.
2_LS ._.
Document_NNP ranking_NN :_: Rank_NNP the_DT documents_NNS such_JJ that_IN all_DT documents_NNS containing_VBG action-items_NNS occur_VBP as_RB high_JJ as_IN possible_JJ in_IN the_DT ranking_NN ._.
3_LS ._.
Sentence_NN detection_NN :_: Classify_VB each_DT sentence_NN in_IN a_DT document_NN as_IN to_TO whether_IN or_CC not_RB it_PRP is_VBZ an_DT action-item_NN ._.
As_IN in_IN most_JJS Information_NN Retrieval_NN tasks_NNS ,_, the_DT weight_NN the_DT evaluation_NN metric_JJ should_MD give_VB to_TO precision_NN and_CC recall_NN depends_VBZ on_IN the_DT nature_NN of_IN the_DT application_NN ._.
In_IN situations_NNS where_WRB a_DT user_NN will_MD eventually_RB read_VB all_DT received_VBN messages_NNS ,_, ranking_JJ -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, via_IN precision_NN at_IN recall_NN of_IN 1_CD -RRB-_-RRB- may_MD be_VB most_RBS important_JJ since_IN this_DT will_MD help_VB encourage_VB shorter_JJR delays_NNS in_IN communications_NNS between_IN users_NNS ._.
In_IN contrast_NN ,_, high-precision_JJ detection_NN at_IN low_JJ recall_NN will_MD be_VB of_IN increasing_VBG importance_NN when_WRB the_DT user_NN is_VBZ under_IN severe_JJ time-pressure_NN and_CC therefore_RB will_MD likely_RB not_RB read_VB all_DT mail_NN ._.
This_DT can_MD be_VB the_DT case_NN for_IN crisis_NN managers_NNS during_IN disaster_NN management_NN ._.
Finally_RB ,_, sentence_NN detection_NN plays_VBZ a_DT role_NN in_IN both_CC timepressure_JJ situations_NNS and_CC simply_RB to_TO alleviate_VB the_DT user_NN ''_'' s_NNS required_VBN time_NN to_TO gist_NN the_DT message_NN ._.
3_LS ._.
#_# Approach_NNP As_IN mentioned_VBN above_IN ,_, the_DT labeled_VBN data_NNS can_MD come_VB in_IN one_CD of_IN two_CD forms_NNS :_: a_DT document-labeling_NN provides_VBZ a_DT yes_RB /_: no_DT label_NN for_IN each_DT document_NN as_IN to_TO whether_IN it_PRP contains_VBZ an_DT action-item_NN ;_: a_DT phrase-labeling_NN provides_VBZ only_RB a_DT yes_RB label_NN for_IN the_DT specific_JJ items_NNS of_IN interest_NN ._.
We_PRP term_VBP the_DT human_JJ judgments_NNS a_DT phrase-labeling_JJ since_IN the_DT user_NN ''_'' s_NNS view_NN of_IN the_DT action-item_NN may_MD not_RB correspond_VB with_IN actual_JJ sentence_NN boundaries_NNS or_CC predicted_VBN sentence_NN boundaries_NNS ._.
Obviously_RB ,_, it_PRP is_VBZ straightforward_JJ to_TO generate_VB a_DT document-labeling_JJ consistent_JJ with_IN a_DT phrase-labeling_JJ by_IN labeling_VBG a_DT document_NN yes_RB if_IN and_CC only_RB if_IN it_PRP contains_VBZ at_IN least_JJS one_CD phrase_NN labeled_VBN yes_RB ._.
To_TO train_VB classifiers_NNS for_IN this_DT task_NN ,_, we_PRP can_MD take_VB several_JJ viewpoints_NNS related_VBN to_TO both_CC the_DT basic_JJ problems_NNS we_PRP have_VBP enumerated_VBN and_CC the_DT form_NN of_IN the_DT labeled_VBN data_NNS ._.
The_DT document-level_JJ view_NN treats_VBZ each_DT e-mail_NN as_IN a_DT learning_NN instance_NN with_IN an_DT associated_VBN class-label_NN ._.
Then_RB ,_, the_DT document_NN can_MD be_VB converted_VBN to_TO a_DT feature-value_JJ vector_NN and_CC learning_VBG progresses_VBZ as_IN usual_JJ ._.
Applying_VBG a_DT document-level_JJ classifier_NN to_TO document_VB detection_NN and_CC ranking_NN is_VBZ straightforward_JJ ._.
In_IN order_NN to_TO apply_VB it_PRP to_TO sentence_NN detection_NN ,_, one_CD must_MD make_VB additional_JJ steps_NNS ._.
For_IN example_NN ,_, if_IN the_DT classifier_NN predicts_VBZ a_DT document_NN contains_VBZ an_DT action-item_NN ,_, then_RB areas_NNS of_IN the_DT document_NN that_WDT contain_VBP a_DT high-concentration_NN of_IN words_NNS which_WDT the_DT model_NN weights_NNS heavily_RB in_IN favor_NN of_IN action-items_NNS can_MD be_VB indicated_VBN ._.
The_DT obvious_JJ benefit_NN of_IN the_DT document-level_JJ approach_NN is_VBZ that_IN training_NN set_VBN collection_NN costs_NNS are_VBP lower_JJR since_IN the_DT user_NN only_RB has_VBZ to_TO specify_VB whether_IN or_CC not_RB an_DT e-mail_NN contains_VBZ an_DT action-item_NN and_CC not_RB the_DT specific_JJ sentences_NNS ._.
In_IN the_DT sentence-level_JJ view_NN ,_, each_DT e-mail_NN is_VBZ automatically_RB segmented_JJ into_IN sentences_NNS ,_, and_CC each_DT sentence_NN is_VBZ treated_VBN as_IN a_DT learning_NN instance_NN with_IN an_DT associated_VBN class-label_NN ._.
Since_IN the_DT phrase-labeling_JJ provided_VBN by_IN the_DT user_NN may_MD not_RB coincide_VB with_IN the_DT automatic_JJ segmentation_NN ,_, we_PRP must_MD determine_VB what_WP label_NN to_TO assign_VB a_DT partially_RB overlapping_VBG sentence_NN when_WRB converting_VBG it_PRP to_TO a_DT learning_NN instance_NN ._.
Once_RB trained_VBN ,_, applying_VBG the_DT resulting_VBG classifiers_NNS to_TO sentence_NN detection_NN is_VBZ now_RB straightforward_JJ ,_, but_CC in_IN order_NN to_TO apply_VB the_DT classifiers_NNS to_TO document_VB detection_NN and_CC document_NN ranking_NN ,_, the_DT individual_JJ predictions_NNS over_IN each_DT sentence_NN must_MD be_VB aggregated_VBN in_IN order_NN to_TO make_VB a_DT document-level_JJ prediction_NN ._.
This_DT approach_NN has_VBZ the_DT potential_JJ to_TO benefit_VB from_IN morespecific_JJ labels_NNS that_WDT enable_VBP the_DT learner_NN to_TO focus_VB attention_NN on_IN the_DT key_JJ sentences_NNS instead_RB of_IN having_VBG to_TO learn_VB based_VBN on_IN data_NNS that_IN the_DT majority_NN of_IN the_DT words_NNS in_IN the_DT e-mail_JJ provide_VBP no_DT or_CC little_JJ information_NN about_IN class_NN membership_NN ._.
3_LS ._.
#_# ._.
#_# Features_VBZ Consider_VB some_DT of_IN the_DT phrases_NNS that_WDT might_MD constitute_VB part_NN of_IN an_DT action_NN item_NN :_: would_MD like_VB to_TO know_VB ,_, let_VB me_PRP know_VB ,_, as_RB soon_RB as_IN possible_JJ ,_, have_VBP you_PRP ._.
Each_DT of_IN these_DT phrases_NNS consists_VBZ of_IN common_JJ words_NNS that_WDT occur_VBP in_IN many_JJ e-mails_NNS ._.
However_RB ,_, when_WRB they_PRP occur_VBP in_IN the_DT same_JJ sentence_NN ,_, they_PRP are_VBP far_RB more_RBR indicative_JJ of_IN an_DT action-item_NN ._.
Additionally_RB ,_, order_NN can_MD be_VB important_JJ :_: consider_VB have_VBP you_PRP versus_CC you_PRP have_VBP ._.
Because_IN of_IN this_DT ,_, we_PRP posit_VBP that_IN n-grams_NNS play_VBP a_DT larger_JJR role_NN in_IN this_DT problem_NN than_IN is_VBZ typical_JJ of_IN problems_NNS like_IN topic_NN classification_NN ._.
Therefore_RB ,_, we_PRP consider_VBP all_DT n-grams_NNS up_IN to_TO size_NN #_# ._.
When_WRB using_VBG n-grams_NNS ,_, if_IN we_PRP find_VBP an_DT n-gram_NN of_IN size_NN #_# in_IN a_DT segment_NN of_IN text_NN ,_, we_PRP can_MD represent_VB the_DT text_NN as_RB just_RB one_CD occurrence_NN of_IN the_DT ngram_NN or_CC as_IN one_CD occurrence_NN of_IN the_DT n-gram_NN and_CC an_DT occurrence_NN of_IN each_DT smaller_JJR n-gram_NN contained_VBN by_IN it_PRP ._.
We_PRP choose_VBP the_DT second_NN of_IN these_DT alternatives_NNS since_IN this_DT will_MD allow_VB the_DT algorithm_NN itself_PRP to_TO smoothly_RB back-off_VB in_IN terms_NNS of_IN recall_NN ._.
Methods_NNS such_JJ as_IN nave_NN Bayes_NNS may_MD be_VB hurt_VBN by_IN such_PDT a_DT representation_NN because_IN of_IN double-counting_NN ._.
Since_IN sentence-ending_NN punctuation_NN can_MD provide_VB information_NN ,_, we_PRP retain_VBP the_DT terminating_VBG punctuation_NN token_JJ when_WRB it_PRP is_VBZ identifiable_JJ ._.
Additionally_RB ,_, we_PRP add_VBP a_DT beginning-of-sentence_NN and_CC end-of-sentence_NN token_JJ in_IN order_NN to_TO capture_VB patterns_NNS that_WDT are_VBP often_RB indicators_NNS at_IN the_DT beginning_NN or_CC end_NN of_IN a_DT sentence_NN ._.
Assuming_VBG proper_JJ punctuation_NN ,_, these_DT extra_JJ tokens_NNS are_VBP unnecessary_JJ ,_, but_CC often_RB e-mail_JJ lacks_VBZ proper_JJ punctuation_NN ._.
In_IN addition_NN ,_, for_IN the_DT sentence-level_JJ classifiers_NNS that_WDT use_VBP ngrams_NNS ,_, we_PRP additionally_RB code_VBP for_IN each_DT sentence_NN a_DT binary_JJ encoding_NN of_IN the_DT position_NN of_IN the_DT sentence_NN relative_JJ to_TO the_DT document_NN ._.
This_DT encoding_NN has_VBZ eight_CD associated_VBN features_NNS that_WDT represent_VBP which_WDT octile_VBP -LRB-_-LRB- the_DT first_JJ eighth_JJ ,_, second_JJ eighth_JJ ,_, etc_FW ._. -RRB-_-RRB-
contains_VBZ the_DT sentence_NN ._.
3_LS ._.
#_# ._.
#_# Implementation_NNP Details_NNP In_IN order_NN to_TO compare_VB the_DT document-level_NN to_TO the_DT sentence-level_JJ approach_NN ,_, we_PRP compare_VBP predictions_NNS at_IN the_DT document-level_NN ._.
We_PRP do_VBP not_RB address_VB how_WRB to_TO use_VB a_DT document-level_JJ classifier_NN to_TO make_VB predictions_NNS at_IN the_DT sentence-level_NN ._.
In_IN order_NN to_TO automatically_RB segment_NN the_DT text_NN of_IN the_DT e-mail_NN ,_, we_PRP use_VBP the_DT RASP_NN statistical_JJ parser_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Since_IN the_DT automatically_RB segmented_JJ sentences_NNS may_MD not_RB correspond_VB directly_RB with_IN the_DT phrase-level_JJ boundaries_NNS ,_, we_PRP treat_VBP any_DT sentence_NN that_WDT contains_VBZ at_IN least_JJS ##_CD %_NN of_IN a_DT marked_JJ action-item_NN segment_NN as_IN an_DT action-item_NN ._.
When_WRB evaluating_VBG sentencedetection_NN for_IN the_DT sentence-level_JJ system_NN ,_, we_PRP use_VBP these_DT class_NN labels_VBZ as_IN ground_NN truth_NN ._.
Since_IN we_PRP are_VBP not_RB evaluating_VBG multiple_JJ segmentation_NN approaches_NNS ,_, this_DT does_VBZ not_RB bias_NN any_DT of_IN the_DT methods_NNS ._.
If_IN multiple_JJ segmentation_NN systems_NNS were_VBD under_IN evaluation_NN ,_, one_CD would_MD need_VB to_TO use_VB a_DT metric_JJ that_IN matched_VBN predicted_VBN positive_JJ sentences_NNS to_TO phrases_NNS labeled_VBN positive_JJ ._.
The_DT metric_JJ would_MD need_VB to_TO punish_VB overly_RB long_RB true_JJ predictions_NNS as_RB well_RB as_IN too_RB short_JJ predictions_NNS ._.
Our_PRP$ criteria_NNS for_IN converting_VBG to_TO labeled_VBN instances_NNS implicitly_RB includes_VBZ both_CC criteria_NNS ._.
Since_IN the_DT segmentation_NN is_VBZ fixed_VBN ,_, an_DT overly_RB long_JJ prediction_NN would_MD be_VB predicting_VBG yes_RB for_IN many_JJ no_DT instances_NNS since_IN presumably_RB the_DT extra_JJ length_NN corresponds_VBZ to_TO additional_JJ segmented_JJ sentences_NNS all_DT of_IN which_WDT do_VBP not_RB contain_VB ##_CD %_NN of_IN action-item_NN ._.
Likewise_RB ,_, a_DT too_RB short_JJ prediction_NN must_MD correspond_VB to_TO a_DT small_JJ sentence_NN included_VBD in_IN the_DT action-item_NN but_CC not_RB constituting_VBG all_DT of_IN the_DT action-item_NN ._.
Therefore_RB ,_, in_IN order_NN to_TO consider_VB the_DT prediction_NN to_TO be_VB too_RB short_JJ ,_, there_EX will_MD be_VB an_DT additional_JJ preceding_VBG /_: following_VBG sentence_NN that_WDT is_VBZ an_DT action-item_NN where_WRB we_PRP incorrectly_RB predicted_VBD no_DT ._.
Once_RB a_DT sentence-level_JJ classifier_NN has_VBZ made_VBN a_DT prediction_NN for_IN each_DT sentence_NN ,_, we_PRP must_MD combine_VB these_DT predictions_NNS to_TO make_VB both_CC a_DT document-level_JJ prediction_NN and_CC a_DT document-level_JJ score_NN ._.
We_PRP use_VBP the_DT simple_JJ policy_NN of_IN predicting_VBG positive_JJ when_WRB any_DT of_IN the_DT sentences_NNS is_VBZ predicted_VBN positive_JJ ._.
In_IN order_NN to_TO produce_VB a_DT document_NN score_NN for_IN ranking_JJ ,_, the_DT confidence_NN that_IN the_DT document_NN contains_VBZ an_DT action-item_NN is_VBZ :_: -LRB-_-LRB- d_LS -RRB-_-RRB- =_JJ 1_CD n_NN -LRB-_-LRB- d_NN -RRB-_-RRB- sd_NN |_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ #_# -LRB-_-LRB- s_NNS -RRB-_-RRB- if_IN for_IN any_DT s_NNS d_NN ,_, -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ #_# 1_CD n_NN -LRB-_-LRB- d_NN -RRB-_-RRB- maxsd_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- o_NN ._.
w_NN ._.
where_WRB s_NNS is_VBZ a_DT sentence_NN in_IN document_NN d_NN ,_, is_VBZ the_DT classifier_NN ''_'' s_NNS #_# /_: #_# prediction_NN ,_, is_VBZ the_DT score_NN the_DT classifier_NN assigns_VBZ as_IN its_PRP$ confidence_NN that_IN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ #_# ,_, and_CC n_NN -LRB-_-LRB- d_NN -RRB-_-RRB- is_VBZ the_DT greater_JJR of_IN #_# and_CC the_DT number_NN of_IN -LRB-_-LRB- unigram_NN -RRB-_-RRB- tokens_NNS in_IN the_DT document_NN ._.
In_IN other_JJ words_NNS ,_, when_WRB any_DT sentence_NN is_VBZ predicted_VBN positive_JJ ,_, the_DT document_NN score_NN is_VBZ the_DT length_NN normalized_VBD sum_NN of_IN the_DT sentence_NN scores_NNS above_IN threshold_NN ._.
When_WRB no_DT sentence_NN is_VBZ predicted_VBN positive_JJ ,_, the_DT document_NN score_NN is_VBZ the_DT maximum_JJ sentence_NN score_NN normalized_VBN by_IN length_NN ._.
As_IN in_IN other_JJ text_NN problems_NNS ,_, we_PRP are_VBP more_RBR likely_JJ to_TO emit_VB false_JJ positives_NNS for_IN documents_NNS with_IN more_JJR words_NNS or_CC sentences_NNS ._.
Thus_RB we_PRP include_VBP a_DT length_NN normalization_NN factor_NN ._.
4_LS ._.
EXPERIMENTAL_JJ ANALYSIS_NN 4_CD ._.
#_# The_DT Data_NNS Our_PRP$ corpus_NN consists_VBZ of_IN e-mails_NNS obtained_VBN from_IN volunteers_NNS at_IN an_DT educational_JJ institution_NN and_CC cover_NN subjects_NNS such_JJ as_IN :_: organizing_VBG a_DT research_NN workshop_NN ,_, arranging_VBG for_IN job-candidate_NN interviews_NNS ,_, publishing_VBG proceedings_NNS ,_, and_CC talk_NN announcements_NNS ._.
The_DT messages_NNS were_VBD anonymized_VBN by_IN replacing_VBG the_DT names_NNS of_IN each_DT individual_NN and_CC institution_NN with_IN a_DT pseudonym_NN ._.
#_# After_IN attempting_VBG to_TO identify_VB and_CC eliminate_VB duplicate_VB e-mails_NNS ,_, the_DT corpus_NN contains_VBZ ###_CD e-mail_JJ messages_NNS ._.
After_IN identity_NN anonymization_NN ,_, the_DT corpora_NN has_VBZ three_CD basic_JJ versions_NNS ._.
Quoted_VBN material_NN refers_VBZ to_TO the_DT text_NN of_IN a_DT previous_JJ e-mail_NN that_IN an_DT author_NN often_RB leaves_VBZ in_IN an_DT e-mail_JJ message_NN when_WRB responding_VBG to_TO the_DT e-mail_NN ._.
Quoted_VBN material_NN can_MD act_VB as_IN noise_NN when_WRB learning_VBG since_IN it_PRP may_MD include_VB action-items_NNS from_IN previous_JJ messages_NNS that_WDT are_VBP no_RB longer_RB relevant_JJ ._.
To_TO isolate_VB the_DT effects_NNS of_IN quoted_VBN material_NN ,_, we_PRP have_VBP three_CD versions_NNS of_IN the_DT corpora_NN ._.
The_DT raw_JJ form_NN contains_VBZ the_DT basic_JJ messages_NNS ._.
The_DT auto-stripped_JJ version_NN contains_VBZ the_DT messages_NNS after_IN quoted_VBN material_NN has_VBZ been_VBN automatically_RB removed_VBN ._.
The_DT hand-stripped_JJ version_NN contains_VBZ the_DT messages_NNS after_IN quoted_VBN material_NN has_VBZ been_VBN removed_VBN by_IN a_DT human_JJ ._.
Additionally_RB ,_, the_DT hand-stripped_JJ version_NN has_VBZ had_VBN any_DT xml_NN content_NN and_CC e-mail_JJ signatures_NNS removed_VBD -_: leaving_VBG only_RB the_DT essential_JJ content_NN of_IN the_DT message_NN ._.
The_DT studies_NNS reported_VBN here_RB are_VBP performed_VBN with_IN the_DT hand-stripped_JJ version_NN ._.
This_DT allows_VBZ us_PRP to_TO balance_VB the_DT cognitive_JJ load_NN in_IN terms_NNS of_IN number_NN of_IN tokens_NNS that_WDT must_MD be_VB read_VBN in_IN the_DT user-studies_NN we_PRP report_VBP -_: including_VBG quoted_VBN material_NN would_MD complicate_VB the_DT user_NN studies_NNS since_IN some_DT users_NNS might_MD skip_VB the_DT material_NN while_IN others_NNS read_VBP it_PRP ._.
Additionally_RB ,_, ensuring_VBG all_DT quoted_VBN material_NN is_VBZ removed_VBN 1_CD We_PRP have_VBP an_DT even_RB more_RBR highly_RB anonymized_VBN version_NN of_IN the_DT corpus_NN that_WDT can_MD be_VB made_VBN available_JJ for_IN some_DT outside_JJ experimentation_NN ._.
Please_VB contact_VB the_DT authors_NNS for_IN more_JJR information_NN on_IN obtaining_VBG this_DT data_NNS ._.
prevents_VBZ tainting_VBG the_DT cross-validation_NN since_IN otherwise_RB a_DT test_NN item_NN could_MD occur_VB as_IN quoted_VBN material_NN in_IN a_DT training_NN document_NN ._.
4_LS ._.
#_# ._.
#_# Data_NNS Labeling_VBG Two_CD human_JJ annotators_NNS labeled_VBN each_DT message_NN as_IN to_TO whether_IN or_CC not_RB it_PRP contained_VBD an_DT action-item_NN ._.
In_IN addition_NN ,_, they_PRP identified_VBD each_DT segment_NN of_IN the_DT e-mail_NN which_WDT contained_VBD an_DT action-item_NN ._.
A_DT segment_NN is_VBZ a_DT contiguous_JJ section_NN of_IN text_NN selected_VBN by_IN the_DT human_JJ annotators_NNS and_CC may_MD span_VB several_JJ sentences_NNS or_CC a_DT complete_JJ phrase_NN contained_VBD in_IN a_DT sentence_NN ._.
They_PRP were_VBD instructed_VBN that_IN an_DT action_NN item_NN is_VBZ an_DT explicit_JJ request_NN for_IN information_NN that_WDT requires_VBZ the_DT recipient_JJ ''_'' s_NNS attention_NN or_CC a_DT required_VBN action_NN and_CC told_VBD to_TO highlight_VB the_DT phrases_NNS or_CC sentences_NNS that_WDT make_VBP up_RP the_DT request_NN ._.
Annotator_NNP #_# No_DT Yes_UH Annotator_NNP #_# No_DT ###_NN ##_CD Yes_UH ##_CD ###_CD Table_NNP #_# :_: Agreement_NN of_IN Human_JJ Annotators_NNS at_IN Document_NNP Level_NNP Annotator_NNP One_CD labeled_VBN ###_CD messages_NNS as_IN containing_VBG action_NN items_NNS ._.
Annotator_NNP Two_CD labeled_VBN ###_CD messages_NNS as_IN containing_VBG action_NN items_NNS ._.
The_DT agreement_NN of_IN the_DT human_JJ annotators_NNS is_VBZ shown_VBN in_IN Tables_NNP #_# and_CC 2_CD ._.
The_DT annotators_NNS are_VBP said_VBN to_TO agree_VB at_IN the_DT document-level_NN when_WRB both_CC marked_VBD the_DT same_JJ document_NN as_IN containing_VBG no_DT action-items_NNS or_CC both_DT marked_JJ at_IN least_JJS one_CD action-item_NN regardless_RB of_IN whether_IN the_DT text_NN segments_NNS were_VBD the_DT same_JJ ._.
At_IN the_DT document-level_NN ,_, the_DT annotators_NNS agreed_VBD ##_CD %_NN of_IN the_DT time_NN ._.
The_DT kappa_NN statistic_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- is_VBZ often_RB used_VBN to_TO evaluate_VB inter-annotator_JJ agreement_NN :_: =_JJ A_DT R_NN 1_CD R_NN A_NN is_VBZ the_DT empirical_JJ estimate_NN of_IN the_DT probability_NN of_IN agreement_NN ._.
R_NN is_VBZ the_DT empirical_JJ estimate_NN of_IN the_DT probability_NN of_IN random_JJ agreement_NN given_VBN the_DT empirical_JJ class_NN priors_NNS ._.
A_DT value_NN close_NN to_TO #_# implies_VBZ the_DT annotators_NNS agree_VBP far_RB less_RBR often_RB than_IN would_MD be_VB expected_VBN randomly_RB ,_, while_IN a_DT value_NN close_NN to_TO #_# means_VBZ they_PRP agree_VBP more_RBR often_RB than_IN randomly_RB expected_VBN ._.
At_IN the_DT document-level_NN ,_, the_DT kappa_NN statistic_NN for_IN inter-annotator_JJ agreement_NN is_VBZ #_# ._.
##_NN ._.
This_DT value_NN is_VBZ both_CC strong_JJ enough_RB to_TO expect_VB the_DT problem_NN to_TO be_VB learnable_JJ and_CC is_VBZ comparable_JJ with_IN results_NNS for_IN similar_JJ tasks_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
In_IN order_NN to_TO determine_VB the_DT sentence-level_JJ agreement_NN ,_, we_PRP use_VBP each_DT judgment_NN to_TO create_VB a_DT sentence-corpus_NN with_IN labels_NNS as_IN described_VBN in_IN Section_NN #_# ._.
#_# ._.
#_# ,_, then_RB consider_VB the_DT agreement_NN over_IN these_DT sentences_NNS ._.
This_DT allows_VBZ us_PRP to_TO compare_VB agreement_NN over_IN no_DT judgments_NNS ._.
We_PRP perform_VBP this_DT comparison_NN over_IN the_DT hand-stripped_JJ corpus_NN since_IN that_DT eliminates_VBZ spurious_JJ no_DT judgments_NNS that_WDT would_MD come_VB from_IN including_VBG quoted_VBN material_NN ,_, etc_FW ._.
Both_DT annotators_NNS were_VBD free_JJ to_TO label_VB the_DT subject_NN as_IN an_DT action-item_NN ,_, but_CC since_IN neither_DT did_VBD ,_, we_PRP omit_VBP the_DT subject_JJ line_NN of_IN the_DT message_NN as_RB well_RB ._.
This_DT only_JJ reduces_VBZ the_DT number_NN of_IN no_DT agreements_NNS ._.
This_DT leaves_VBZ ####_CD automatically_RB segmented_JJ sentences_NNS ._.
At_IN the_DT sentence-level_NN ,_, the_DT annotators_NNS agreed_VBD ##_CD %_NN of_IN the_DT time_NN ,_, and_CC the_DT kappa_NN statistic_NN for_IN inter-annotator_JJ agreement_NN is_VBZ #_# ._.
##_NN ._.
In_IN order_NN to_TO produce_VB one_CD single_JJ set_NN of_IN judgments_NNS ,_, the_DT human_JJ annotators_NNS went_VBD through_IN each_DT annotation_NN where_WRB there_EX was_VBD disagreement_NN and_CC came_VBD to_TO a_DT consensus_NN opinion_NN ._.
The_DT annotators_NNS did_VBD not_RB collect_VB statistics_NNS during_IN this_DT process_NN but_CC anecdotally_RB reported_VBD that_IN the_DT majority_NN of_IN disagreements_NNS were_VBD either_DT cases_NNS of_IN clear_JJ annotator_NN oversight_NN or_CC different_JJ interpretations_NNS of_IN conditional_JJ statements_NNS ._.
For_IN example_NN ,_, If_IN you_PRP would_MD like_VB to_TO keep_VB your_PRP$ job_NN ,_, come_VBN to_TO tomorrow_NN ''_'' s_NNS meeting_VBG implies_VBZ a_DT required_JJ action_NN where_WRB If_IN you_PRP would_MD like_VB to_TO join_VB Annotator_NNP #_# No_DT Yes_UH Annotator_NNP #_# No_DT ####_NN ##_CD Yes_UH ##_CD ###_CD Table_NNP #_# :_: Agreement_NN of_IN Human_JJ Annotators_NNS at_IN Sentence_NN Level_NN the_DT football_NN betting_VBG pool_NN ,_, come_VBN to_TO tomorrow_NN ''_'' s_NNS meeting_NN does_VBZ not_RB ._.
The_DT first_JJ would_MD be_VB an_DT action-item_NN in_IN most_JJS contexts_NNS while_IN the_DT second_JJ would_MD not_RB ._.
Of_IN course_NN ,_, many_JJ conditional_JJ statements_NNS are_VBP not_RB so_RB clearly_RB interpretable_JJ ._.
After_IN reconciling_VBG the_DT judgments_NNS there_EX are_VBP ###_CD e-mails_NNS with_IN no_DT action-items_NNS and_CC ###_CD e-mails_NNS containing_VBG actionitems_NNS ._.
Of_IN the_DT ###_CD e-mails_NNS containing_VBG action-items_NNS ,_, ###_CD messages_NNS have_VBP one_CD action-item_NN segment_NN ;_: ##_CD messages_NNS have_VBP two_CD action-item_JJ segments_NNS ;_: ##_CD messages_NNS have_VBP three_CD action-item_JJ segments_NNS ._.
Two_CD messages_NNS have_VBP four_CD action-item_JJ segments_NNS ,_, and_CC one_CD message_NN has_VBZ six_CD action-item_NN segments_NNS ._.
Computing_NNP the_DT sentence-level_JJ agreement_NN using_VBG the_DT reconciled_VBN gold_NN standard_NN judgments_NNS with_IN each_DT of_IN the_DT annotators_NNS ''_'' individual_JJ judgments_NNS gives_VBZ a_DT kappa_NN of_IN #_# ._.
##_NN for_IN Annotator_NNP One_CD and_CC a_DT kappa_NN of_IN #_# ._.
##_NN for_IN Annotator_NNP Two_CD ._.
In_IN terms_NNS of_IN message_NN characteristics_NNS ,_, there_EX were_VBD on_IN average_JJ ###_CD content_NN tokens_NNS in_IN the_DT body_NN after_IN stripping_VBG ._.
For_IN action-item_NN messages_NNS ,_, there_EX were_VBD ###_CD ._.
However_RB ,_, by_IN examining_VBG Figure_NNP #_# we_PRP see_VBP the_DT length_NN distributions_NNS are_VBP nearly_RB identical_JJ ._.
As_IN would_MD be_VB expected_VBN for_IN e-mail_NN ,_, it_PRP is_VBZ a_DT long-tailed_JJ distribution_NN with_IN about_IN half_PDT the_DT messages_NNS having_VBG more_JJR than_IN ##_CD tokens_NNS in_IN the_DT body_NN -LRB-_-LRB- this_DT paragraph_NN has_VBZ 65_CD tokens_NNS -RRB-_-RRB- ._.
4_LS ._.
#_# Classifiers_NNPS For_IN this_DT experiment_NN ,_, we_PRP have_VBP selected_VBN a_DT variety_NN of_IN standard_JJ text_NN classification_NN algorithms_NNS ._.
In_IN selecting_VBG algorithms_NNS ,_, we_PRP have_VBP chosen_VBN algorithms_NNS that_WDT are_VBP not_RB only_RB known_VBN to_TO work_VB well_RB but_CC which_WDT differ_VBP along_IN such_JJ lines_NNS as_IN discriminative_JJ vs_CC ._.
generative_NN and_CC lazy_JJ vs_CC ._.
eager_JJ ._.
We_PRP have_VBP done_VBN this_DT in_IN order_NN to_TO provide_VB both_CC a_DT competitive_JJ and_CC thorough_JJ sampling_NN of_IN learning_VBG methods_NNS for_IN the_DT task_NN at_IN hand_NN ._.
This_DT is_VBZ important_JJ since_IN it_PRP is_VBZ easy_JJ to_TO improve_VB a_DT strawman_NN classifier_NN by_IN introducing_VBG a_DT new_JJ representation_NN ._.
By_IN thoroughly_RB sampling_NN alternative_JJ classifier_NN choices_NNS we_PRP demonstrate_VBP that_IN representation_NN improvements_NNS over_IN bag-of-words_NNS are_VBP not_RB due_JJ to_TO using_VBG the_DT information_NN in_IN the_DT bag-of-words_NNS poorly_RB ._.
4_LS ._.
#_# ._.
#_# kNN_NNP We_PRP employ_VBP a_DT standard_JJ variant_NN of_IN the_DT k-nearest_NN neighbor_NN algorithm_NN used_VBN in_IN text_NN classification_NN ,_, kNN_NN with_IN s-cut_JJ score_NN thresholding_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
We_PRP use_VBP a_DT tfidf-weighting_NN of_IN the_DT terms_NNS with_IN a_DT distanceweighted_JJ vote_NN of_IN the_DT neighbors_NNS to_TO compute_VB the_DT score_NN before_IN thresholding_VBG it_PRP ._.
In_IN order_NN to_TO choose_VB the_DT value_NN of_IN s_NNS for_IN thresholding_NN ,_, we_PRP perform_VBP leave-one-out_JJ cross-validation_NN over_IN the_DT training_NN set_NN ._.
The_DT value_NN of_IN k_NN is_VBZ set_VBN to_TO be_VB #_# -LRB-_-LRB- log2_NN N_NN +_CC #_# -RRB-_-RRB- where_WRB N_NN is_VBZ the_DT number_NN of_IN training_NN points_NNS ._.
This_DT rule_NN for_IN choosing_VBG k_NN is_VBZ theoretically_RB motivated_VBN by_IN results_NNS which_WDT show_VBP such_JJ a_DT rule_NN converges_VBZ to_TO the_DT optimal_JJ classifier_NN as_IN the_DT number_NN of_IN training_NN points_NNS increases_VBZ -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN practice_NN ,_, we_PRP have_VBP also_RB found_VBN it_PRP to_TO be_VB a_DT computational_JJ convenience_NN that_WDT frequently_RB leads_VBZ to_TO comparable_JJ results_NNS with_IN numerically_RB optimizing_VBG k_NN via_IN a_DT cross-validation_NN procedure_NN ._.
4_LS ._.
#_# ._.
#_# Nave_NNP Bayes_NNP We_PRP use_VBP a_DT standard_JJ multinomial_JJ nave_NN Bayes_NNP classifier_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
In_IN using_VBG this_DT classifier_NN ,_, we_PRP smoothed_VBD word_NN and_CC class_NN probabilities_NNS using_VBG a_DT Bayesian_JJ estimate_NN -LRB-_-LRB- with_IN the_DT word_NN prior_RB -RRB-_-RRB- and_CC a_DT Laplace_NNP m-estimate_NN ,_, respectively_RB ._.
0_CD 20_CD 40_CD 60_CD 80_CD 100_CD 120_CD 140_CD 160_CD 0_CD ###_CD ###_CD ###_CD ###_CD ####_CD ####_CD ####_CD NumberofMessages_NNP Number_NNP of_IN Tokens_NNP All_DT Messages_NNS Action-Item_NNP Messages_VBZ 0_CD 0_CD ._.
##_NN 0_CD ._.
##_NN 0_CD ._.
##_NN 0_CD ._.
##_NN 0_CD ._.
#_# 0_CD ._.
##_NN 0_CD ._.
##_NN 0_CD ._.
##_NN 0_CD ._.
##_NN 0_CD ._.
#_# 0_CD ###_CD ###_CD ###_CD ###_CD ####_CD ####_CD ####_CD PercentageofMessages_NNP Number_NNP of_IN Tokens_NNP All_NNP Messages_NNPS Action-Item_NNP Messages_NNPS Figure_NNP #_# :_: The_DT Histogram_NN -LRB-_-LRB- left_NN -RRB-_-RRB- and_CC Distribution_NN -LRB-_-LRB- right_NN -RRB-_-RRB- of_IN Message_NN Length_NN ._.
A_DT bin_NN size_NN of_IN ##_CD words_NNS was_VBD used_VBN ._.
Only_RB tokens_NNS in_IN the_DT body_NN after_IN hand-stripping_JJ were_VBD counted_VBN ._.
After_IN stripping_VBG ,_, the_DT majority_NN of_IN words_NNS left_VBN are_VBP usually_RB actual_JJ message_NN content_NN ._.
Classifiers_NNP Document_NNP Unigram_NNP Document_NNP Ngram_NNP Sentence_NNP Unigram_NNP Sentence_NNP Ngram_NNP F1_NN kNN_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_RB nave_VB Bayes_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN SVM_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_CD Voted_VBD Perceptron_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NNP Accuracy_NNP kNN_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_RB nave_VB Bayes_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN SVM_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_CD Voted_VBD Perceptron_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NNP Table_NNP #_# :_: Average_JJ Document-Detection_NNP Performance_NNP during_IN Cross-Validation_NNP for_IN Each_DT Method_NN and_CC the_DT Sample_NNP Standard_NNP Deviation_NN -LRB-_-LRB- Sn1_NN -RRB-_-RRB- in_IN italics_NNS ._.
The_DT best_JJS performance_NN for_IN each_DT classifier_NN is_VBZ shown_VBN in_IN bold_JJ ._.
4_LS ._.
#_# ._.
#_# SVM_NNP We_PRP have_VBP used_VBN a_DT linear_JJ SVM_NN with_IN a_DT tfidf_NN feature_NN representation_NN and_CC L2-norm_NN as_IN implemented_VBN in_IN the_DT SVMlight_NN package_NN v6_NN ._.
##_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
All_DT default_NN settings_NNS were_VBD used_VBN ._.
4_LS ._.
#_# ._.
#_# Voted_VBD Perceptron_NNP Like_IN the_DT SVM_NNP ,_, the_DT Voted_NNP Perceptron_NNP is_VBZ a_DT kernel-based_JJ learning_NN method_NN ._.
We_PRP use_VBP the_DT same_JJ feature_NN representation_NN and_CC kernel_NN as_IN we_PRP have_VBP for_IN the_DT SVM_NNP ,_, a_DT linear_JJ kernel_NN with_IN tfidf-weighting_NN and_CC an_DT L2-norm_NN ._.
The_DT voted_VBD perceptron_NN is_VBZ an_DT online-learning_JJ method_NN that_WDT keeps_VBZ a_DT history_NN of_IN past_JJ perceptrons_NNS used_VBN ,_, as_RB well_RB as_IN a_DT weight_NN signifying_VBG how_WRB often_RB that_IN perceptron_NN was_VBD correct_JJ ._.
With_IN each_DT new_JJ training_NN example_NN ,_, a_DT correct_JJ classification_NN increases_VBZ the_DT weight_NN on_IN the_DT current_JJ perceptron_NN and_CC an_DT incorrect_JJ classification_NN updates_NNS the_DT perceptron_NN ._.
The_DT output_NN of_IN the_DT classifier_NN uses_VBZ the_DT weights_NNS on_IN the_DT perceptra_NN to_TO make_VB a_DT final_JJ voted_VBD classification_NN ._.
When_WRB used_VBN in_IN an_DT offline-manner_NN ,_, multiple_JJ passes_NNS can_MD be_VB made_VBN through_IN the_DT training_NN data_NNS ._.
Both_CC the_DT voted_VBD perceptron_NN and_CC the_DT SVM_NNP give_VB a_DT solution_NN from_IN the_DT same_JJ hypothesis_NN space_NN -_: in_IN this_DT case_NN ,_, a_DT linear_JJ classifier_NN ._.
Furthermore_RB ,_, it_PRP is_VBZ well-known_JJ that_IN the_DT Voted_NNP Perceptron_NNP increases_VBZ the_DT margin_NN of_IN the_DT solution_NN after_IN each_DT pass_NN through_IN the_DT training_NN data_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Since_IN Cohen_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- obtain_VB worse_JJR results_NNS using_VBG an_DT SVM_NN than_IN a_DT Voted_NNP Perceptron_NNP with_IN one_CD training_NN iteration_NN ,_, they_PRP conclude_VBP that_IN the_DT best_JJS solution_NN for_IN detecting_VBG speech_NN acts_NNS may_MD not_RB lie_VB in_IN an_DT area_NN with_IN a_DT large_JJ margin_NN ._.
Because_IN their_PRP$ tasks_NNS are_VBP highly_RB similar_JJ to_TO ours_JJ ,_, we_PRP employ_VBP both_DT classifiers_NNS to_TO ensure_VB we_PRP are_VBP not_RB overlooking_VBG a_DT competitive_JJ alternative_JJ classifier_NN to_TO the_DT SVM_NNP for_IN the_DT basic_JJ bag-of-words_NNS representation_NN ._.
4_LS ._.
#_# Performance_NNP Measures_NNS To_TO compare_VB the_DT performance_NN of_IN the_DT classification_NN methods_NNS ,_, we_PRP look_VBP at_IN two_CD standard_JJ performance_NN measures_NNS ,_, F1_NN and_CC accuracy_NN ._.
The_DT F1_NN measure_NN -LSB-_-LRB- ##_CD ,_, ##_CD -RSB-_-RRB- is_VBZ the_DT harmonic_JJ mean_NN of_IN precision_NN and_CC recall_NN where_WRB Precision_NN =_JJ Correct_JJ Positives_NNS Predicted_VBD Positives_NNS and_CC Recall_VB =_JJ Correct_NNP Positives_NNPS Actual_NNP Positives_NNPS ._.
4_LS ._.
#_# Experimental_JJ Methodology_NN We_PRP perform_VBP standard_JJ 10-fold_JJ cross-validation_NN on_IN the_DT set_NN of_IN documents_NNS ._.
For_IN the_DT sentence-level_JJ approach_NN ,_, all_DT sentences_NNS in_IN a_DT document_NN are_VBP either_CC entirely_RB in_IN the_DT training_NN set_NN or_CC entirely_RB in_IN the_DT test_NN set_VBN for_IN each_DT fold_NN ._.
For_IN significance_NN tests_NNS ,_, we_PRP use_VBP a_DT two-tailed_JJ t-test_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- to_TO compare_VB the_DT values_NNS obtained_VBN during_IN each_DT cross-validation_NN fold_NN with_IN a_DT p-value_NN of_IN #_# ._.
##_NN ._.
Feature_NN selection_NN was_VBD performed_VBN using_VBG the_DT chi-squared_JJ statistic_NN ._.
Different_JJ levels_NNS of_IN feature_NN selection_NN were_VBD considered_VBN for_IN each_DT classifier_NN ._.
Each_DT of_IN the_DT following_VBG number_NN of_IN features_NNS was_VBD tried_VBN :_: 10_CD ,_, ##_CD ,_, ##_CD ,_, ###_CD ,_, ###_CD ,_, ###_CD ,_, ####_CD ,_, ####_CD ,_, ####_CD ._.
There_EX are_VBP approximately_RB ####_RB unigram_JJ tokens_NNS without_IN feature_NN selection_NN ._.
In_IN order_NN to_TO choose_VB the_DT number_NN of_IN features_NNS to_TO use_VB for_IN each_DT classifier_NN ,_, we_PRP perform_VBP nested_JJ cross-validation_NN and_CC choose_VB the_DT settings_NNS that_WDT yield_VBP the_DT optimal_JJ document-level_NN F1_NN for_IN that_DT classifier_NN ._.
For_IN this_DT study_NN ,_, only_RB the_DT body_NN of_IN each_DT e-mail_JJ message_NN was_VBD used_VBN ._.
Feature_NN selection_NN is_VBZ always_RB applied_VBN to_TO all_DT candidate_NN features_NNS ._.
That_DT is_VBZ ,_, for_IN the_DT n-gram_NN representation_NN ,_, the_DT n-grams_NN and_CC position_NN features_NNS are_VBP also_RB subject_JJ to_TO removal_NN by_IN the_DT feature_NN selection_NN method_NN ._.
4_LS ._.
#_# Results_NNS The_DT results_NNS for_IN document-level_JJ classification_NN are_VBP given_VBN in_IN Table_NNP 3_CD ._.
The_DT primary_JJ hypothesis_NN we_PRP are_VBP concerned_VBN with_IN is_VBZ that_IN n-grams_NNS are_VBP critical_JJ for_IN this_DT task_NN ;_: if_IN this_DT is_VBZ true_JJ ,_, we_PRP expect_VBP to_TO see_VB a_DT significant_JJ gap_NN in_IN performance_NN between_IN the_DT document-level_JJ classifiers_NNS that_WDT use_VBP n-grams_NN -LRB-_-LRB- denoted_VBN Document_NNP Ngram_NNP -RRB-_-RRB- and_CC those_DT using_VBG only_RB unigram_JJ features_NNS -LRB-_-LRB- denoted_VBN Document_NNP Unigram_NNP -RRB-_-RRB- ._.
Examining_VBG Table_NNP #_# ,_, we_PRP observe_VBP that_IN this_DT is_VBZ indeed_RB the_DT case_NN for_IN every_DT classifier_NN except_IN nave_NN Bayes_NNS ._.
This_DT difference_NN in_IN performance_NN produced_VBN by_IN the_DT n-gram_NN representation_NN is_VBZ statistically_RB significant_JJ for_IN each_DT classifier_NN except_IN for_IN nave_NN Bayes_NNS and_CC the_DT accuracy_NN metric_JJ for_IN kNN_NN ._.
Nave_NNP Bayes_NNP poor_JJ performance_NN with_IN the_DT n-gram_NN representation_NN is_VBZ not_RB surprising_JJ since_IN the_DT bag-of-n-grams_NN causes_VBZ excessive_JJ doublecounting_NN as_IN mentioned_VBN in_IN Section_NN #_# ._.
#_# ._.
#_# ;_: however_RB ,_, nave_NN Bayes_NNP is_VBZ not_RB hurt_VBN at_IN the_DT sentence-level_NN because_IN the_DT sparse_JJ examples_NNS provide_VBP few_JJ chances_NNS for_IN agglomerative_JJ effects_NNS of_IN double_JJ counting_NN ._.
In_IN either_DT case_NN ,_, when_WRB a_DT language-modeling_JJ approach_NN is_VBZ desired_VBN ,_, modeling_NN the_DT n-grams_NN directly_RB would_MD be_VB preferable_JJ to_TO nave_VB Bayes_NNP ._.
More_RBR importantly_RB for_IN the_DT n-gram_NN hypothesis_NN ,_, the_DT n-grams_JJ lead_NN to_TO the_DT best_JJS document-level_NN classifier_NN performance_NN as_RB well_RB ._.
As_IN would_MD be_VB expected_VBN ,_, the_DT difference_NN between_IN the_DT sentence-level_JJ n-gram_NN representation_NN and_CC unigram_NN representation_NN is_VBZ small_JJ ._.
This_DT is_VBZ because_IN the_DT window_NN of_IN text_NN is_VBZ so_RB small_JJ that_IN the_DT unigram_JJ representation_NN ,_, when_WRB done_VBN at_IN the_DT sentence-level_NN ,_, implicitly_RB picks_VBZ up_RP on_IN the_DT power_NN of_IN the_DT n-grams_NNS ._.
Further_JJ improvement_NN would_MD signify_VB that_IN the_DT order_NN of_IN the_DT words_NNS matter_VBP even_RB when_WRB only_RB considering_VBG a_DT small_JJ sentence-size_NN window_NN ._.
Therefore_RB ,_, the_DT finer-grained_JJ sentence-level_NN judgments_NNS allows_VBZ a_DT unigram_JJ representation_NN to_TO succeed_VB but_CC only_RB when_WRB performed_VBN in_IN a_DT small_JJ window_NN -_: behaving_VBG as_IN an_DT n-gram_NN representation_NN for_IN all_DT practical_JJ purposes_NNS ._.
Document_NNP Winner_NN Sentence_NN Winner_NN kNN_NN Ngram_NN Ngram_NN nave_NN Bayes_NNP Unigram_NNP Ngram_NNP SVM_NNP Ngram_NNP Ngram_NNP Voted_VBD Perceptron_NNP Ngram_NNP Ngram_NNP Table_NNP #_# :_: Significance_NN results_VBZ for_IN n-grams_NNS versus_CC unigrams_NNS for_IN document_NN detection_NN using_VBG document-level_JJ and_CC sentence-level_JJ classifiers_NNS ._.
When_WRB the_DT F1_NN result_NN is_VBZ statistically_RB significant_JJ ,_, it_PRP is_VBZ shown_VBN in_IN bold_JJ ._.
When_WRB the_DT accuracy_NN result_NN is_VBZ significant_JJ ,_, it_PRP is_VBZ shown_VBN with_IN a_DT ._.
F1_NN Winner_NN Accuracy_NNP Winner_NN kNN_NN Sentence_NN Sentence_NN nave_NN Bayes_NNP Sentence_NNP Sentence_NNP SVM_NNP Sentence_NNP Sentence_NNP Voted_VBD Perceptron_NNP Sentence_NNP Document_NNP Table_NNP #_# :_: Significance_NN results_VBZ for_IN sentence-level_JJ classifiers_NNS vs_CC ._.
document-level_JJ classifiers_NNS for_IN the_DT document_NN detection_NN problem_NN ._.
When_WRB the_DT result_NN is_VBZ statistically_RB significant_JJ ,_, it_PRP is_VBZ shown_VBN in_IN bold_JJ ._.
Further_JJ highlighting_VBG the_DT improvement_NN from_IN finer-grained_JJ judgments_NNS and_CC n-grams_NNS ,_, Figure_NNP #_# graphically_RB depicts_VBZ the_DT edge_NN the_DT SVM_NNP sentence-level_NN classifier_NN has_VBZ over_IN the_DT standard_JJ bag-of-words_NNS approach_VBP with_IN a_DT precision-recall_JJ curve_NN ._.
In_IN the_DT high_JJ precision_NN area_NN of_IN the_DT graph_NN ,_, the_DT consistent_JJ edge_NN of_IN the_DT sentence-level_JJ classifier_NN is_VBZ rather_RB impressive_JJ -_: continuing_VBG at_IN precision_NN #_# out_RP to_TO #_# ._.
#_# recall_NN ._.
This_DT would_MD mean_VB that_IN a_DT tenth_NN of_IN the_DT user_NN ''_'' s_NNS action-items_NNS would_MD be_VB placed_VBN at_IN the_DT top_NN of_IN their_PRP$ action-item_NN sorted_VBD inbox_NN ._.
Additionally_RB ,_, the_DT large_JJ separation_NN at_IN the_DT top_JJ right_NN of_IN the_DT curves_NNS corresponds_VBZ to_TO the_DT area_NN where_WRB the_DT optimal_JJ F1_NN occurs_VBZ for_IN each_DT classifier_NN ,_, agreeing_VBG with_IN the_DT large_JJ improvement_NN from_IN #_# ._.
####_CD to_TO #_# ._.
####_NN in_IN F1_NN score_NN ._.
Considering_VBG the_DT relative_JJ unexplored_JJ nature_NN of_IN classification_NN at_IN the_DT sentence-level_NN ,_, this_DT gives_VBZ great_JJ hope_NN for_IN further_JJ increases_NNS in_IN performance_NN ._.
Accuracy_NN F1_NN Unigram_NNP Ngram_NNP Unigram_NNP Ngram_NNP kNN_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_RB nave_VB Bayes_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN SVM_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_CD Voted_VBD Perceptron_NNP #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NN #_# ._.
####_NNP Table_NNP #_# :_: Performance_NNP of_IN the_DT Sentence-Level_NNP Classifiers_NNP at_IN Sentence_NNP Detection_NN Although_IN Cohen_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- observed_VBN that_IN the_DT Voted_NNP Perceptron_NNP with_IN a_DT single_JJ training_NN iteration_NN outperformed_VBD SVM_NNP in_IN a_DT set_NN of_IN similar_JJ tasks_NNS ,_, we_PRP see_VBP no_DT such_JJ behavior_NN here_RB ._.
This_DT further_JJ strengthens_VBZ the_DT evidence_NN that_IN an_DT alternate_JJ classifier_NN with_IN the_DT bag-of-words_NNS representation_NN could_MD not_RB reach_VB the_DT same_JJ level_NN of_IN performance_NN ._.
The_DT Voted_NNP Perceptron_NNP classifier_NN does_VBZ improve_VB when_WRB the_DT number_NN of_IN training_NN iterations_NNS are_VBP increased_VBN ,_, but_CC it_PRP is_VBZ still_RB lower_JJR than_IN the_DT SVM_NNP classifier_NN ._.
Sentence_NN detection_NN results_NNS are_VBP presented_VBN in_IN Table_NNP #_# ._.
With_IN regard_NN to_TO the_DT sentence_NN detection_NN problem_NN ,_, we_PRP note_VBP that_IN the_DT F1_NN measure_NN gives_VBZ a_DT better_RBR feel_VB for_IN the_DT remaining_VBG room_NN for_IN improvement_NN in_IN this_DT difficult_JJ problem_NN ._.
That_DT is_VBZ ,_, unlike_IN document_NN detection_NN where_WRB actionitem_NN documents_NNS are_VBP fairly_RB common_JJ ,_, action-item_JJ sentences_NNS are_VBP very_RB rare_JJ ._.
Thus_RB ,_, as_IN in_IN other_JJ text_NN problems_NNS ,_, the_DT accuracy_NN numbers_NNS are_VBP deceptively_RB high_JJ sheerly_RB because_IN of_IN the_DT default_NN accuracy_NN attainable_JJ by_IN always_RB predicting_VBG no_DT ._.
Although_IN ,_, the_DT results_NNS here_RB are_VBP significantly_RB above-random_JJ ,_, it_PRP is_VBZ unclear_JJ what_WP level_NN of_IN performance_NN is_VBZ necessary_JJ for_IN sentence_NN detection_NN to_TO be_VB useful_JJ in_IN and_CC of_IN itself_PRP and_CC not_RB simply_RB as_IN a_DT means_NN to_TO document_VB ranking_JJ and_CC classification_NN ._.
Figure_NNP #_# :_: Users_NNS find_VBP action-items_NNS quicker_JJR when_WRB assisted_VBN by_IN a_DT classification_NN system_NN ._.
Finally_RB ,_, when_WRB considering_VBG a_DT new_JJ type_NN of_IN classification_NN task_NN ,_, one_CD of_IN the_DT most_JJS basic_JJ questions_NNS is_VBZ whether_IN an_DT accurate_JJ classifier_NN built_VBN for_IN the_DT task_NN can_MD have_VB an_DT impact_NN on_IN the_DT end-user_NN ._.
In_IN order_NN to_TO demonstrate_VB the_DT impact_NN this_DT task_NN can_MD have_VB on_IN e-mail_JJ users_NNS ,_, we_PRP conducted_VBD a_DT user_NN study_NN using_VBG an_DT earlier_JJR less-accurate_JJ version_NN of_IN the_DT sentence_NN classifier_NN -_: where_WRB instead_RB of_IN using_VBG just_RB a_DT single_JJ sentence_NN ,_, a_DT threesentence_JJ windowed-approach_NN was_VBD used_VBN ._.
There_EX were_VBD three_CD distinct_JJ sets_NNS of_IN e-mail_NN in_IN which_WDT users_NNS had_VBD to_TO find_VB action-items_NNS ._.
These_DT sets_NNS were_VBD either_RB presented_VBN in_IN a_DT random_JJ order_NN -LRB-_-LRB- Unordered_JJ -RRB-_-RRB- ,_, ordered_VBN by_IN the_DT classifier_NN -LRB-_-LRB- Ordered_VBN -RRB-_-RRB- ,_, or_CC ordered_VBN by_IN the_DT classifier_NN and_CC with_IN the_DT 0_CD ._.
#_# 0_CD ._.
#_# 0_CD ._.
#_# 0_CD ._.
#_# 0_CD ._.
#_# 0_CD ._.
#_# 1_CD 0_CD #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# ._.
#_# #_# Precision_NNP Recall_VB Action-Item_NNP Detection_NN SVM_NNP Performance_NNP -LRB-_-LRB- Post_NNP Model_NNP Selection_NN -RRB-_-RRB- Document_NNP Unigram_NNP Sentence_NNP Ngram_NNP Figure_NNP #_# :_: Both_DT n-grams_NN and_CC a_DT small_JJ prediction_NN window_NN lead_NN to_TO consistent_JJ improvements_NNS over_IN the_DT standard_JJ approach_NN ._.
center_NN sentence_NN in_IN the_DT highest_JJS confidence_NN window_NN highlighted_VBD -LRB-_-LRB- Order_NN +_CC help_NN -RRB-_-RRB- ._.
In_IN order_NN to_TO perform_VB fair_JJ comparisons_NNS between_IN conditions_NNS ,_, the_DT overall_JJ number_NN of_IN tokens_NNS in_IN each_DT message_NN set_NN should_MD be_VB approximately_RB equal_JJ ;_: that_DT is_VBZ ,_, the_DT cognitive_JJ reading_NN load_NN should_MD be_VB approximately_RB the_DT same_JJ before_IN the_DT classifier_NN ''_'' s_NNS reordering_NN ._.
Additionally_RB ,_, users_NNS typically_RB show_VBP practice_NN effects_NNS by_IN improving_VBG at_IN the_DT overall_JJ task_NN and_CC thus_RB performing_VBG better_JJR at_IN later_RB message_NN sets_NNS ._.
This_DT is_VBZ typically_RB handled_VBN by_IN varying_VBG the_DT ordering_VBG of_IN the_DT sets_NNS across_IN users_NNS so_IN that_IN the_DT means_NNS are_VBP comparable_JJ ._.
While_IN omitting_VBG further_JJ detail_NN ,_, we_PRP note_VBP the_DT sets_NNS were_VBD balanced_VBN for_IN the_DT total_JJ number_NN of_IN tokens_NNS and_CC a_DT latin_NN square_JJ design_NN was_VBD used_VBN to_TO balance_VB practice_NN effects_NNS ._.
Figure_NNP #_# shows_VBZ that_IN at_IN intervals_NNS of_IN #_# ,_, ##_NN ,_, and_CC ##_NN minutes_NNS ,_, users_NNS consistently_RB found_VBD significantly_RB more_JJR action-items_NNS when_WRB assisted_VBN by_IN the_DT classifier_NN ,_, but_CC were_VBD most_RBS critically_RB aided_VBN in_IN the_DT first_JJ five_CD minutes_NNS ._.
Although_IN ,_, the_DT classifier_NN consistently_RB aids_VBZ the_DT users_NNS ,_, we_PRP did_VBD not_RB gain_VB an_DT additional_JJ end-user_JJ impact_NN by_IN highlighting_VBG ._.
As_IN mentioned_VBN above_IN ,_, this_DT might_MD be_VB a_DT result_NN of_IN the_DT large_JJ room_NN for_IN improvement_NN that_WDT still_RB exists_VBZ for_IN sentence_NN detection_NN ,_, but_CC anecdotal_JJ evidence_NN suggests_VBZ this_DT might_MD also_RB be_VB a_DT result_NN of_IN how_WRB the_DT information_NN is_VBZ presented_VBN to_TO the_DT user_NN rather_RB than_IN the_DT accuracy_NN of_IN sentence_NN detection_NN ._.
For_IN example_NN ,_, highlighting_VBG the_DT wrong_JJ sentence_NN near_IN an_DT actual_JJ action-item_NN hurts_VBZ the_DT user_NN ''_'' s_NNS trust_NN ,_, but_CC if_IN a_DT vague_JJ indicator_NN -LRB-_-LRB- e_LS ._.
g_NN ._.
,_, an_DT arrow_NN -RRB-_-RRB- points_NNS to_TO the_DT approximate_JJ area_NN the_DT user_NN is_VBZ not_RB aware_JJ of_IN the_DT near-miss_NN ._.
Since_IN the_DT user_NN studies_NNS used_VBD a_DT three_CD sentence_NN window_NN ,_, we_PRP believe_VBP this_DT played_VBD a_DT role_NN as_RB well_RB as_IN sentence_NN detection_NN accuracy_NN ._.
4_LS ._.
#_# Discussion_NNP In_IN contrast_NN to_TO problems_NNS where_WRB n-grams_NNS have_VBP yielded_VBN little_JJ difference_NN ,_, we_PRP believe_VBP their_PRP$ power_NN here_RB stems_VBZ from_IN the_DT fact_NN that_IN many_JJ of_IN the_DT meaningful_JJ n-grams_NN for_IN action-items_NNS consist_VBP of_IN common_JJ words_NNS ,_, e_LS ._.
g_NN ._.
,_, let_VB me_PRP know_VB ._.
Therefore_RB ,_, the_DT document-level_JJ unigram_NN approach_NN can_MD not_RB gain_VB much_JJ leverage_NN ,_, even_RB when_WRB modeling_NN their_PRP$ joint_JJ probability_NN correctly_RB ,_, since_IN these_DT words_NNS will_MD often_RB co-occur_VB in_IN the_DT document_NN but_CC not_RB necessarily_RB in_IN a_DT phrase_NN ._.
Additionally_RB ,_, action-item_NN detection_NN is_VBZ distinct_JJ from_IN many_JJ text_NN classification_NN tasks_NNS in_IN that_IN a_DT single_JJ sentence_NN can_MD change_VB the_DT class_NN label_NN of_IN the_DT document_NN ._.
As_IN a_DT result_NN ,_, good_JJ classifiers_NNS can_MD not_RB rely_VB on_IN aggregating_VBG evidence_NN from_IN a_DT large_JJ number_NN of_IN weak_JJ indicators_NNS across_IN the_DT entire_JJ document_NN ._.
Even_RB though_IN we_PRP discarded_VBD the_DT header_NN information_NN ,_, examining_VBG the_DT top-ranked_JJ features_NNS at_IN the_DT document-level_NN reveals_VBZ that_IN many_JJ of_IN the_DT features_NNS are_VBP names_NNS or_CC parts_NNS of_IN e-mail_JJ addresses_NNS that_WDT occurred_VBD in_IN the_DT body_NN and_CC are_VBP highly_RB associated_VBN with_IN e-mails_NNS that_WDT tend_VBP to_TO contain_VB many_JJ or_CC no_DT action-items_NNS ._.
A_DT few_JJ examples_NNS are_VBP terms_NNS such_JJ as_IN org_NN ,_, bob_NN ,_, and_CC gov_NN ._.
We_PRP note_VBP that_IN these_DT features_NNS will_MD be_VB sensitive_JJ to_TO the_DT particular_JJ distribution_NN -LRB-_-LRB- senders_NNS /_: receivers_NNS -RRB-_-RRB- and_CC thus_RB the_DT document-level_JJ approach_NN may_MD produce_VB classifiers_NNS that_WDT transfer_VBP less_JJR readily_RB to_TO alternate_JJ contexts_NNS and_CC users_NNS at_IN different_JJ institutions_NNS ._.
This_DT points_VBZ out_RP that_IN part_NN of_IN the_DT problem_NN of_IN going_VBG beyond_IN bag-of-words_NNS may_MD be_VB the_DT methodology_NN ,_, and_CC investigating_VBG such_JJ properties_NNS as_IN learning_VBG curves_NNS and_CC how_WRB well_RB a_DT model_NN transfers_NNS may_MD highlight_VB differences_NNS in_IN models_NNS which_WDT appear_VBP to_TO have_VB similar_JJ performance_NN when_WRB tested_VBN on_IN the_DT distributions_NNS they_PRP were_VBD trained_VBN on_IN ._.
We_PRP are_VBP currently_RB investigating_VBG whether_IN the_DT sentence-level_JJ classifiers_NNS do_VBP perform_VB better_JJR over_IN different_JJ test_NN corpora_NN without_IN retraining_VBG ._.
5_CD ._.
FUTURE_JJ WORK_VBP While_IN applying_VBG text_NN classifiers_NNS at_IN the_DT document-level_NN is_VBZ fairly_RB well-understood_JJ ,_, there_EX exists_VBZ the_DT potential_NN for_IN significantly_RB increasing_VBG the_DT performance_NN of_IN the_DT sentence-level_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP alternate_JJ ways_NNS of_IN combining_VBG the_DT predictions_NNS over_IN each_DT sentence_NN ,_, weightings_NNS other_JJ than_IN tfidf_NN ,_, which_WDT may_MD not_RB be_VB appropriate_JJ since_IN sentences_NNS are_VBP small_JJ ,_, better_JJR sentence_NN segmentation_NN ,_, and_CC other_JJ types_NNS of_IN phrasal_JJ analysis_NN ._.
Additionally_RB ,_, named_VBN entity_NN tagging_NN ,_, time_NN expressions_NNS ,_, etc_FW ._.
,_, seem_VBP likely_JJ candidates_NNS for_IN features_NNS that_WDT can_MD further_RB improve_VB this_DT task_NN ._.
We_PRP are_VBP currently_RB pursuing_VBG some_DT of_IN these_DT avenues_NNS to_TO see_VB what_WP additional_JJ gains_NNS these_DT offer_NN ._.
Finally_RB ,_, it_PRP would_MD be_VB interesting_JJ to_TO investigate_VB the_DT best_JJS methods_NNS for_IN combining_VBG the_DT document-level_NN and_CC sentence-level_NN classifiers_NNS ._.
Since_IN the_DT simple_JJ bag-of-words_NNS representation_NN at_IN the_DT document-level_NN leads_VBZ to_TO a_DT learned_VBN model_NN that_WDT behaves_VBZ somewhat_RB like_IN a_DT context-specific_JJ prior_JJ dependent_JJ on_IN the_DT sender_NN /_: receiver_NN and_CC general_JJ topic_NN ,_, a_DT first_JJ choice_NN would_MD be_VB to_TO treat_VB it_PRP as_IN such_JJ when_WRB combining_VBG probability_NN estimates_NNS with_IN the_DT sentence-level_JJ classifier_NN ._.
Such_PDT a_DT model_NN might_MD serve_VB as_IN a_DT general_JJ example_NN for_IN other_JJ problems_NNS where_WRB bag-of-words_NNS can_MD establish_VB a_DT baseline_NN model_NN but_CC richer_JJR approaches_NNS are_VBP needed_VBN to_TO achieve_VB performance_NN beyond_IN that_DT baseline_NN ._.
6_CD ._.
SUMMARY_NN AND_CC CONCLUSIONS_NNS The_DT effectiveness_NN of_IN sentence-level_JJ detection_NN argues_VBZ that_IN labeling_VBG at_IN the_DT sentence-level_NN provides_VBZ significant_JJ value_NN ._.
Further_JJ experiments_NNS are_VBP needed_VBN to_TO see_VB how_WRB this_DT interacts_VBZ with_IN the_DT amount_NN of_IN training_NN data_NNS available_JJ ._.
Sentence_NN detection_NN that_WDT is_VBZ then_RB agglomerated_VBN to_TO document-level_JJ detection_NN works_VBZ surprisingly_RB better_JJR given_VBN low_JJ recall_NN than_IN would_MD be_VB expected_VBN with_IN sentence-level_JJ items_NNS ._.
This_DT ,_, in_IN turn_NN ,_, indicates_VBZ that_IN improved_VBN sentence_NN segmentation_NN methods_NNS could_MD yield_VB further_RB improvements_NNS in_IN classification_NN ._.
In_IN this_DT work_NN ,_, we_PRP examined_VBD how_WRB action-items_NNS can_MD be_VB effectively_RB detected_VBN in_IN e-mails_NNS ._.
Our_PRP$ empirical_JJ analysis_NN has_VBZ demonstrated_VBN that_IN n-grams_NNS are_VBP of_IN key_JJ importance_NN to_TO making_VBG the_DT most_JJS of_IN documentlevel_NN judgments_NNS ._.
When_WRB finer-grained_JJ judgments_NNS are_VBP available_JJ ,_, then_RB a_DT standard_JJ bag-of-words_NNS approach_VBP using_VBG a_DT small_JJ -LRB-_-LRB- sentence_NN -RRB-_-RRB- window_NN size_NN and_CC automatic_JJ segmentation_NN techniques_NNS can_MD produce_VB results_NNS almost_RB as_RB good_JJ as_IN the_DT n-gram_NN based_VBN approaches_NNS ._.
Acknowledgments_NNS This_DT material_NN is_VBZ based_VBN upon_IN work_NN supported_VBN by_IN the_DT Defense_NNP Advanced_NNP Research_NNP Projects_NNP Agency_NNP -LRB-_-LRB- DARPA_NNP -RRB-_-RRB- under_IN Contract_NNP No_NNP ._.
NBCHD030010_NN ._.
Any_DT opinions_NNS ,_, findings_NNS and_CC conclusions_NNS or_CC recommendations_NNS expressed_VBN in_IN this_DT material_NN are_VBP those_DT of_IN the_DT author_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- and_CC do_VBP not_RB necessarily_RB reflect_VB the_DT views_NNS of_IN the_DT Defense_NNP Advanced_NNP Research_NNP Projects_NNP Agency_NNP -LRB-_-LRB- DARPA_NNP -RRB-_-RRB- ,_, or_CC the_DT Department_NNP of_IN InteriorNational_NNP Business_NNP Center_NNP -LRB-_-LRB- DOI-NBC_NN -RRB-_-RRB- ._.
We_PRP would_MD like_VB to_TO extend_VB our_PRP$ sincerest_JJS thanks_NNS to_TO Jill_NNP Lehman_NNP whose_WP$ efforts_NNS in_IN data_NNS collection_NN were_VBD essential_JJ in_IN constructing_VBG the_DT corpus_NN ,_, and_CC both_DT Jill_NNP and_CC Aaron_NNP Steinfeld_NNP for_IN their_PRP$ direction_NN of_IN the_DT HCI_NN experiments_NNS ._.
We_PRP would_MD also_RB like_VB to_TO thank_VB Django_NNP Wexler_NNP for_IN constructing_VBG and_CC supporting_VBG the_DT corpus_NN labeling_NN tools_NNS and_CC Curtis_NNP Huttenhower_NNP ''_'' s_VBZ support_NN of_IN the_DT text_NN preprocessing_NN package_NN ._.
Finally_RB ,_, we_PRP gratefully_RB acknowledge_VBP Scott_NNP Fahlman_NNP for_IN his_PRP$ encouragement_NN and_CC useful_JJ discussions_NNS on_IN this_DT topic_NN ._.
7_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- J_NN ._.
Allan_NNP ,_, J_NNP ._.
Carbonell_NNP ,_, G_NNP ._.
Doddington_NNP ,_, J_NNP ._.
Yamron_NNP ,_, and_CC Y_NN ._.
Yang_NNP ._.
Topic_NN detection_NN and_CC tracking_NN pilot_NN study_NN :_: Final_JJ report_NN ._.
In_IN Proceedings_NNP of_IN the_DT DARPA_NNP Broadcast_NNP News_NNP Transcription_NN and_CC Understanding_VBG Workshop_NNP ,_, Washington_NNP ,_, D_NNP ._.
C_NN ._.
,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- C_NN ._.
Apte_NNP ,_, F_NN ._.
Damerau_NNP ,_, and_CC S_NN ._.
M_NN ._.
Weiss_NNP ._.
Automated_VBN learning_NN of_IN decision_NN rules_NNS for_IN text_NN categorization_NN ._.
ACM_NNP Transactions_NNS on_IN Information_NNP Systems_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 233-251_CD ,_, July_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- J_NN ._.
Carletta_NNP ._.
Assessing_VBG agreement_NN on_IN classification_NN tasks_NNS :_: The_DT kappa_NN statistic_NN ._.
Computational_JJ Linguistics_NNS ,_, ##_NN -LRB-_-LRB- #_# -RRB-_-RRB- :_: 249-254_CD ,_, 1996_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- J_NN ._.
Carroll_NNP ._.
High_JJ precision_NN extraction_NN of_IN grammatical_JJ relations_NNS ._.
In_IN Proceedings_NNP of_IN the_DT 19th_JJ International_NNP Conference_NNP on_IN Computational_NNP Linguistics_NNP -LRB-_-LRB- COLING_NNP -RRB-_-RRB- ,_, pages_NNS 134-140_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- W_NN ._.
W_NN ._.
Cohen_NNP ,_, V_NNP ._.
R_NN ._.
Carvalho_NNP ,_, and_CC T_NN ._.
M_NN ._.
Mitchell_NNP ._.
Learning_VBG to_TO classify_VB email_NN into_IN speech_NN acts_VBZ ._.
In_IN EMNLP-2004_NN -LRB-_-LRB- Conference_NN on_IN Empirical_JJ Methods_NNS in_IN Natural_JJ Language_NN Processing_NN -RRB-_-RRB- ,_, pages_NNS 309-316_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
Corston-Oliver_NNP ,_, E_NNP ._.
Ringger_NNP ,_, M_NN ._.
Gamon_NNP ,_, and_CC R_NN ._.
Campbell_NNP ._.
Task-focused_JJ summarization_NN of_IN email_NN ._.
In_IN Text_VB Summarization_NNP Branches_NNPS Out_IN :_: Proceedings_NNP of_IN the_DT ACL-04_NN Workshop_NNP ,_, pages_NNS 43-50_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
Culotta_NNP ,_, R_NN ._.
Bekkerman_NNP ,_, and_CC A_NN ._.
McCallum_NNP ._.
Extracting_VBG social_JJ networks_NNS and_CC contact_NN information_NN from_IN email_NN and_CC the_DT web_NN ._.
In_IN CEAS-2004_NN -LRB-_-LRB- Conference_NN on_IN Email_VB and_CC Anti-Spam_NNP -RRB-_-RRB- ,_, Mountain_NNP View_NNP ,_, CA_NNP ,_, July_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- L_NN ._.
Devroye_NNP ,_, L_NNP ._.
Gyorfi_NNP ,_, and_CC G_NN ._.
Lugosi_NNP ._.
A_DT Probabilistic_NNP Theory_NNP of_IN Pattern_NNP Recognition_NN ._.
Springer-Verlag_NNP ,_, New_NNP York_NNP ,_, NY_NNP ,_, 1996_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
T_NN ._.
Dumais_NNP ,_, J_NNP ._.
Platt_NNP ,_, D_NNP ._.
Heckerman_NNP ,_, and_CC M_NN ._.
Sahami_NNP ._.
Inductive_JJ learning_NN algorithms_NNS and_CC representations_NNS for_IN text_NN categorization_NN ._.
In_IN CIKM_NN ''_'' ##_NN ,_, Proceedings_NNP of_IN the_DT 7th_JJ ACM_NNP Conference_NN on_IN Information_NN and_CC Knowledge_NNP Management_NNP ,_, pages_NNS 148-155_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Freund_NN and_CC R_NN ._.
Schapire_NNP ._.
Large_JJ margin_NN classification_NN using_VBG the_DT perceptron_NN algorithm_NN ._.
Machine_NN Learning_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 277-296_CD ,_, 1999_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- T_NN ._.
Joachims_NNP ._.
Making_VBG large-scale_JJ svm_NN learning_VBG practical_JJ ._.
In_IN B_NN ._.
Scholkopf_NNP ,_, C_NNP ._.
J_NN ._.
Burges_NNP ,_, and_CC A_NN ._.
J_NN ._.
Smola_NNP ,_, editors_NNS ,_, Advances_NNS in_IN Kernel_NNP Methods_NNS -_: Support_NN Vector_NNP Learning_NNP ,_, pages_NNS 41-56_CD ._.
MIT_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- L_NN ._.
S_NN ._.
Larkey_NNP ._.
A_DT patent_NN search_NN and_CC classification_NN system_NN ._.
In_IN Proceedings_NNP of_IN the_DT Fourth_JJ ACM_NNP Conference_NN on_IN Digital_NNP Libraries_NNPS ,_, pages_NNS ###_SYM -_: ###_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
D_NN ._.
Lewis_NNP ._.
An_DT evaluation_NN of_IN phrasal_JJ and_CC clustered_VBN representations_NNS on_IN a_DT text_NN categorization_NN task_NN ._.
In_IN SIGIR_NN ''_'' ##_NN ,_, Proceedings_NNP of_IN the_DT 15th_JJ Annual_JJ International_NNP ACM_NNP Conference_NNP on_IN Research_NNP and_CC Development_NNP in_IN Information_NNP Retrieval_NNP ,_, pages_NNS 37-50_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Liu_NNP ,_, J_NNP ._.
Carbonell_NNP ,_, and_CC R_NN ._.
Jin_NNP ._.
A_DT pairwise_JJ ensemble_NN approach_NN for_IN accurate_JJ genre_NN classification_NN ._.
In_IN Proceedings_NNP of_IN the_DT European_JJ Conference_NN on_IN Machine_NN Learning_NNP -LRB-_-LRB- ECML_NNP -RRB-_-RRB- ,_, 2003_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Liu_NNP ,_, R_NN ._.
Yan_NNP ,_, R_NN ._.
Jin_NNP ,_, and_CC J_NN ._.
Carbonell_NNP ._.
A_DT comparison_NN study_NN of_IN kernels_NNS for_IN multi-label_JJ text_NN classification_NN using_VBG category_NN association_NN ._.
In_IN The_DT Twenty-first_NNP International_NNP Conference_NNP on_IN Machine_NNP Learning_NNP -LRB-_-LRB- ICML_NN -RRB-_-RRB- ,_, ####_NN ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- A_DT ._.
McCallum_NNP and_CC K_NNP ._.
Nigam_NNP ._.
A_DT comparison_NN of_IN event_NN models_NNS for_IN naive_JJ bayes_NNS text_NN classification_NN ._.
In_IN Working_VBG Notes_NNS of_IN AAAI_NNP ''_'' ##_NN -LRB-_-LRB- The_DT 15th_JJ National_NNP Conference_NNP on_IN Artificial_NNP Intelligence_NNP -RRB-_-RRB- ,_, Workshop_NNP on_IN Learning_NNP for_IN Text_VB Categorization_NN ,_, pages_NNS 41-48_CD ,_, ####_CD ._.
TR_NN WS-98-05_NN ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- F_NN ._.
Sebastiani_NNP ._.
Machine_NN learning_NN in_IN automated_VBN text_NN categorization_NN ._.
ACM_NNP Computing_NNP Surveys_NNS ,_, ##_NN -LRB-_-LRB- #_# -RRB-_-RRB- :_: 1-47_CD ,_, March_NNP 2002_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- C_NN ._.
J_NN ._.
van_NN Rijsbergen_NNP ._.
Information_NNP Retrieval_NNP ._.
Butterworths_NNS ,_, London_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Yang_NNP ._.
An_DT evaluation_NN of_IN statistical_JJ approaches_NNS to_TO text_NN categorization_NN ._.
Information_NNP Retrieval_NNP ,_, #_# -LRB-_-LRB- #_# /_: #_# -RRB-_-RRB- :_: 67-88_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Yang_NNP ,_, J_NNP ._.
Carbonell_NNP ,_, R_NN ._.
Brown_NNP ,_, T_NN ._.
Pierce_NNP ,_, B_NN ._.
T_NN ._.
Archibald_NNP ,_, and_CC X_NN ._.
Liu_NNP ._.
Learning_NNP approaches_VBZ to_TO topic_NN detection_NN and_CC tracking_NN ._.
IEEE_NNP EXPERT_NNP ,_, Special_JJ Issue_NN on_IN Applications_NNS of_IN Intelligent_NNP Information_NNP Retrieval_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Yang_NNP and_CC X_NNP ._.
Liu_NNP ._.
A_DT re-examination_NN of_IN text_NN categorization_NN methods_NNS ._.
In_IN SIGIR_NN ''_'' ##_NN ,_, Proceedings_NNP of_IN the_DT 22nd_JJ Annual_JJ International_NNP ACM_NNP Conference_NNP on_IN Research_NNP and_CC Development_NNP in_IN Information_NNP Retrieval_NNP ,_, pages_NNS 42-49_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Y_NN ._.
Yang_NNP ,_, J_NNP ._.
Zhang_NNP ,_, J_NNP ._.
Carbonell_NNP ,_, and_CC C_NN ._.
Jin_NNP ._.
Topic-conditioned_JJ novelty_NN detection_NN ._.
In_IN Proceedings_NNP of_IN the_DT ACM_NNP SIGKDD_NNP International_NNP Conference_NNP on_IN Knowledge_NNP Discovery_NNP and_CC Data_NNP Mining_NNP ,_, July_NNP ####_CD ._.
