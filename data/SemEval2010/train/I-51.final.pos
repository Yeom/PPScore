Learning_NNP and_CC Joint_NNP Deliberation_NNP through_IN Argumentation_NNP in_IN Multi-Agent_NNP Systems_NNPS Santi_NNP Ontan_NNP CCL_NNP ,_, Cognitive_NNP Computing_NNP Lab_NNP Georgia_NNP Institute_NNP of_IN Technology_NNP Atlanta_NNP ,_, GA_NNP ######_CD /_: ####_CD santi_NNS @_IN cc_NN ._.
gatech_NN ._.
edu_NN Enric_NNP Plaza_NNP IIIA_NNP ,_, Artificial_NNP Intelligence_NNP Research_NNP Institute_NNP CSIC_NNP ,_, Spanish_NNP Council_NNP for_IN Scientific_NNP Research_NNP Campus_NNP UAB_NNP ,_, #####_CD Bellaterra_NNP ,_, Catalonia_NNP -LRB-_-LRB- Spain_NNP -RRB-_-RRB- enric_JJ @_SYM iiia_FW ._.
csic_JJ ._.
es_NNS ABSTRACT_NN In_IN this_DT paper_NN we_PRP will_MD present_VB an_DT argumentation_NN framework_NN for_IN learning_VBG agents_NNS -LRB-_-LRB- AMAL_NN -RRB-_-RRB- designed_VBN for_IN two_CD purposes_NNS :_: -LRB-_-LRB- #_# -RRB-_-RRB- for_IN joint_JJ deliberation_NN ,_, and_CC -LRB-_-LRB- #_# -RRB-_-RRB- for_IN learning_VBG from_IN communication_NN ._.
The_DT AMAL_NNP framework_NN is_VBZ completely_RB based_VBN on_IN learning_VBG from_IN examples_NNS :_: the_DT argument_NN preference_NN relation_NN ,_, the_DT argument_NN generation_NN policy_NN ,_, and_CC the_DT counterargument_NN generation_NN policy_NN are_VBP case-based_JJ techniques_NNS ._.
For_IN join_VB deliberation_NN ,_, learning_VBG agents_NNS share_VBP their_PRP$ experience_NN by_IN forming_VBG a_DT committee_NN to_TO decide_VB upon_IN some_DT joint_JJ decision_NN ._.
We_PRP experimentally_RB show_VBP that_IN the_DT argumentation_NN among_IN committees_NNS of_IN agents_NNS improves_VBZ both_CC the_DT individual_JJ and_CC joint_JJ performance_NN ._.
For_IN learning_VBG from_IN communication_NN ,_, an_DT agent_NN engages_VBZ into_IN arguing_VBG with_IN other_JJ agents_NNS in_IN order_NN to_TO contrast_VB its_PRP$ individual_JJ hypotheses_NNS and_CC receive_VB counterexamples_NNS ;_: the_DT argumentation_NN process_NN improves_VBZ their_PRP$ learning_NN scope_NN and_CC individual_JJ performance_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNS I_PRP ._.
#_# ._.
#_# -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Learning_NNP ;_: I_PRP ._.
#_# ._.
##_NN -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Distributed_VBN Artificial_JJ Intelligence-Multiagent_JJ systems_NNS ,_, Intelligent_JJ Agents_NNS 1_CD ._.
INTRODUCTION_NN Argumentation_NN frameworks_NNS for_IN multi-agent_JJ systems_NNS can_MD be_VB used_VBN for_IN different_JJ purposes_NNS like_IN joint_JJ deliberation_NN ,_, persuasion_NN ,_, negotiation_NN ,_, and_CC conflict_NN resolution_NN ._.
In_IN this_DT paper_NN we_PRP will_MD present_VB an_DT argumentation_NN framework_NN for_IN learning_VBG agents_NNS ,_, and_CC show_VBP that_IN it_PRP can_MD be_VB used_VBN for_IN two_CD purposes_NNS :_: -LRB-_-LRB- #_# -RRB-_-RRB- joint_JJ deliberation_NN ,_, and_CC -LRB-_-LRB- #_# -RRB-_-RRB- learning_VBG from_IN communication_NN ._.
Argumentation-based_JJ joint_JJ deliberation_NN involves_VBZ discussion_NN over_IN the_DT outcome_NN of_IN a_DT particular_JJ situation_NN or_CC the_DT appropriate_JJ course_NN of_IN action_NN for_IN a_DT particular_JJ situation_NN ._.
Learning_VBG agents_NNS are_VBP capable_JJ of_IN learning_VBG from_IN experience_NN ,_, in_IN the_DT sense_NN that_WDT past_IN examples_NNS -LRB-_-LRB- situations_NNS and_CC their_PRP$ outcomes_NNS -RRB-_-RRB- are_VBP used_VBN to_TO predict_VB the_DT outcome_NN for_IN the_DT situation_NN at_IN hand_NN ._.
However_RB ,_, since_IN individual_JJ agents_NNS experience_NN may_MD be_VB limited_VBN ,_, individual_JJ knowledge_NN and_CC prediction_NN accuracy_NN is_VBZ also_RB limited_VBN ._.
Thus_RB ,_, learning_VBG agents_NNS that_WDT are_VBP capable_JJ of_IN arguing_VBG their_PRP$ individual_JJ predictions_NNS with_IN other_JJ agents_NNS may_MD reach_VB better_JJR prediction_NN accuracy_NN after_IN such_PDT an_DT argumentation_NN process_NN ._.
Most_JJS existing_VBG argumentation_NN frameworks_NNS for_IN multi-agent_JJ systems_NNS are_VBP based_VBN on_IN deductive_JJ logic_NN or_CC some_DT other_JJ deductive_JJ logic_NN formalism_NN specifically_RB designed_VBN to_TO support_VB argumentation_NN ,_, such_JJ as_IN default_NN logic_NN -LSB-_-LRB- #_# -RSB-_-RRB- -RRB-_-RRB- ._.
Usually_RB ,_, an_DT argument_NN is_VBZ seen_VBN as_IN a_DT logical_JJ statement_NN ,_, while_IN a_DT counterargument_NN is_VBZ an_DT argument_NN offered_VBN in_IN opposition_NN to_TO another_DT argument_NN -LSB-_-LRB- #_# ,_, ##_NN -RSB-_-RRB- ;_: agents_NNS use_VBP a_DT preference_NN relation_NN to_TO resolve_VB conflicting_VBG arguments_NNS ._.
However_RB ,_, logic-based_JJ argumentation_NN frameworks_NNS assume_VBP agents_NNS with_IN preloaded_JJ knowledge_NN and_CC preference_NN relation_NN ._.
In_IN this_DT paper_NN ,_, we_PRP focus_VBP on_IN an_DT Argumentation-based_JJ Multi-Agent_JJ Learning_NNP -LRB-_-LRB- AMAL_NNP -RRB-_-RRB- framework_NN where_WRB both_DT knowledge_NN and_CC preference_NN relation_NN are_VBP learned_VBN from_IN experience_NN ._.
Thus_RB ,_, we_PRP consider_VBP a_DT scenario_NN with_IN agents_NNS that_WDT -LRB-_-LRB- #_# -RRB-_-RRB- work_NN in_IN the_DT same_JJ domain_NN using_VBG a_DT shared_JJ ontology_NN ,_, -LRB-_-LRB- #_# -RRB-_-RRB- are_VBP capable_JJ of_IN learning_VBG from_IN examples_NNS ,_, and_CC -LRB-_-LRB- #_# -RRB-_-RRB- communicate_VBP using_VBG an_DT argumentative_JJ framework_NN ._.
Having_VBG learning_VBG capabilities_NNS allows_VBZ agents_NNS effectively_RB use_VBP a_DT specific_JJ form_NN of_IN counterargument_NN ,_, namely_RB the_DT use_NN of_IN counterexamples_NNS ._.
Counterexamples_NNS offer_VBP the_DT possibility_NN of_IN agents_NNS learning_VBG during_IN the_DT argumentation_NN process_NN ._.
Moreover_RB ,_, learning_VBG agents_NNS allow_VBP techniques_NNS that_WDT use_VBP learnt_VBN experience_NN to_TO generate_VB adequate_JJ arguments_NNS and_CC counterarguments_NNS ._.
Specifically_RB ,_, we_PRP will_MD need_VB to_TO address_VB two_CD issues_NNS :_: -LRB-_-LRB- #_# -RRB-_-RRB- how_WRB to_TO define_VB a_DT technique_NN to_TO generate_VB arguments_NNS and_CC counterarguments_NNS from_IN examples_NNS ,_, and_CC -LRB-_-LRB- #_# -RRB-_-RRB- how_WRB to_TO define_VB a_DT preference_NN relation_NN over_IN two_CD conflicting_VBG arguments_NNS that_WDT have_VBP been_VBN induced_VBN from_IN examples_NNS ._.
This_DT paper_NN presents_VBZ a_DT case-based_JJ approach_NN to_TO address_VB both_CC issues_NNS ._.
The_DT agents_NNS use_VBP case-based_JJ reasoning_NN -LRB-_-LRB- CBR_NN -RRB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- to_TO learn_VB from_IN past_JJ cases_NNS -LRB-_-LRB- where_WRB a_DT case_NN is_VBZ a_DT situation_NN and_CC its_PRP$ outcome_NN -RRB-_-RRB- in_IN order_NN to_TO predict_VB the_DT outcome_NN of_IN a_DT new_JJ situation_NN ._.
We_PRP propose_VBP an_DT argumentation_NN protocol_NN inside_IN the_DT AMAL_NNP framework_NN at_IN supports_NNS agents_NNS in_IN reaching_VBG a_DT joint_JJ prediction_NN over_IN a_DT specific_JJ situation_NN or_CC problem_NN -_: moreover_RB ,_, the_DT reasoning_NN needed_VBN to_TO support_VB the_DT argumentation_NN process_NN will_MD also_RB be_VB based_VBN on_IN cases_NNS ._.
In_IN particular_JJ ,_, we_PRP present_VBP two_CD case-based_JJ measures_NNS ,_, one_CD for_IN generating_VBG the_DT arguments_NNS and_CC counterarguments_NNS adequate_JJ to_TO a_DT particular_JJ situation_NN and_CC another_DT for_IN determining_VBG preference_NN relation_NN among_IN arguments_NNS ._.
Finally_RB ,_, we_PRP evaluate_VBP -LRB-_-LRB- #_# -RRB-_-RRB- if_IN argumentation_NN between_IN learning_VBG agents_NNS can_MD produce_VB a_DT joint_JJ prediction_NN that_WDT improves_VBZ over_IN individual_JJ learning_NN performance_NN and_CC -LRB-_-LRB- #_# -RRB-_-RRB- if_IN learning_VBG from_IN the_DT counterexamples_NNS conveyed_VBN during_IN the_DT argumentation_NN process_NN increases_VBZ the_DT individual_JJ performance_NN with_IN precisely_RB those_DT cases_NNS being_VBG used_VBN while_IN arguing_VBG among_IN them_PRP ._.
The_DT paper_NN is_VBZ structured_VBN as_IN follows_VBZ ._.
Section_NN #_# discusses_VBZ the_DT relation_NN among_IN argumentation_NN ,_, collaboration_NN and_CC learning_NN ._.
Then_RB Section_NN #_# introduces_VBZ our_PRP$ multi-agent_JJ CBR_NN -LRB-_-LRB- MAC_NNP -RRB-_-RRB- framework_NN and_CC the_DT notion_NN of_IN justified_JJ prediction_NN ._.
After_IN that_DT ,_, Section_NNP #_# formally_RB defines_VBZ our_PRP$ argumentation_NN framework_NN ._.
Sections_NNS #_# and_CC #_# present_JJ our_PRP$ case-based_JJ preference_NN relation_NN and_CC argument_NN generation_NN policies_NNS respectively_RB ._.
Later_RB ,_, Section_NN #_# presents_VBZ the_DT argumentation_NN protocol_NN in_IN our_PRP$ AMAL_NNP framework_NN ._.
After_IN that_DT ,_, Section_NNP #_# presents_VBZ an_DT exemplification_NN of_IN the_DT argumentation_NN framework_NN ._.
Finally_RB ,_, Section_NN #_# presents_VBZ an_DT empirical_JJ evaluation_NN of_IN our_PRP$ two_CD main_JJ hypotheses_NNS ._.
The_DT paper_NN closes_VBZ with_IN related_JJ work_NN and_CC conclusions_NNS sections_NNS ._.
2_LS ._.
ARGUMENTATION_NN ,_, COLLABORATION_NNP AND_CC LEARNING_NNP Both_CC learning_VBG and_CC collaboration_NN are_VBP ways_NNS in_IN which_WDT an_DT agent_NN can_MD improve_VB individual_JJ performance_NN ._.
In_IN fact_NN ,_, there_EX is_VBZ a_DT clear_JJ parallelism_NN between_IN learning_NN and_CC collaboration_NN in_IN multi-agent_JJ systems_NNS ,_, since_IN both_DT are_VBP ways_NNS in_IN which_WDT agents_NNS can_MD deal_VB with_IN their_PRP$ shortcomings_NNS ._.
Let_VB us_PRP show_VB which_WDT are_VBP the_DT main_JJ motivations_NNS that_IN an_DT agent_NN can_MD have_VB to_TO learn_VB or_CC to_TO collaborate_VB ._.
Motivations_NNS to_TO learn_VB :_: -_: Increase_NN quality_NN of_IN prediction_NN ,_, -_: Increase_NN efficiency_NN ,_, -_: Increase_NN the_DT range_NN of_IN solvable_JJ problems_NNS ._.
Motivations_NNS to_TO collaborate_VB :_: -_: Increase_NN quality_NN of_IN prediction_NN ,_, -_: Increase_NN efficiency_NN ,_, -_: Increase_NN the_DT range_NN of_IN solvable_JJ problems_NNS ,_, -_: Increase_NN the_DT range_NN of_IN accessible_JJ resources_NNS ._.
Looking_VBG at_IN the_DT above_JJ lists_NNS of_IN motivation_NN ,_, we_PRP can_MD easily_RB see_VB that_IN learning_NN and_CC collaboration_NN are_VBP very_RB related_JJ in_IN multi-agent_JJ systems_NNS ._.
In_IN fact_NN ,_, with_IN the_DT exception_NN of_IN the_DT last_JJ item_NN in_IN the_DT motivations_NNS to_TO collaborate_VB list_NN ,_, they_PRP are_VBP two_CD extremes_NNS of_IN a_DT continuum_NN of_IN strategies_NNS to_TO improve_VB performance_NN ._.
An_DT agent_NN may_MD choose_VB to_TO increase_VB performance_NN by_IN learning_VBG ,_, by_IN collaborating_VBG ,_, or_CC by_IN finding_VBG an_DT intermediate_JJ point_NN that_WDT combines_VBZ learning_NN and_CC collaboration_NN in_IN order_NN to_TO improve_VB performance_NN ._.
In_IN this_DT paper_NN we_PRP will_MD propose_VB AMAL_NNP ,_, an_DT argumentation_NN framework_NN for_IN learning_VBG agents_NNS ,_, and_CC will_MD also_RB also_RB show_VB how_WRB AMAL_NNP can_MD be_VB used_VBN both_DT for_IN learning_VBG from_IN communication_NN and_CC for_IN solving_VBG problems_NNS in_IN a_DT collaborative_JJ way_NN :_: Agents_NNS can_MD solve_VB problems_NNS in_IN a_DT collaborative_JJ way_NN via_IN engaging_VBG an_DT argumentation_NN process_NN about_IN the_DT prediction_NN for_IN the_DT situation_NN at_IN hand_NN ._.
Using_VBG this_DT collaboration_NN ,_, the_DT prediction_NN can_MD be_VB done_VBN in_IN a_DT more_RBR informed_JJ way_NN ,_, since_IN the_DT information_NN known_VBN by_IN several_JJ agents_NNS has_VBZ been_VBN taken_VBN into_IN account_NN ._.
Agents_NNS can_MD also_RB learn_VB from_IN communication_NN with_IN other_JJ agents_NNS by_IN engaging_VBG an_DT argumentation_NN process_NN ._.
Agents_NNS that_WDT engage_VBP in_IN such_JJ argumentation_NN processes_NNS can_MD learn_VB from_IN the_DT arguments_NNS and_CC counterexamples_NNS received_VBN from_IN other_JJ agents_NNS ,_, and_CC use_VB this_DT information_NN for_IN predicting_VBG the_DT outcomes_NNS of_IN future_JJ situations_NNS ._.
In_IN the_DT rest_NN of_IN this_DT paper_NN we_PRP will_MD propose_VB an_DT argumentation_NN framework_NN and_CC show_VB how_WRB it_PRP can_MD be_VB used_VBN both_DT for_IN learning_NN and_CC for_IN solving_VBG problems_NNS in_IN a_DT collaborative_JJ way_NN ._.
3_LS ._.
MULTI-AGENT_JJ CBR_NNP SYSTEMS_NNPS A_DT Multi-Agent_JJ Case_NN Based_VBD Reasoning_NN System_NN -LRB-_-LRB- MAC_NNP -RRB-_-RRB- M_NN =_JJ -LCB-_-LRB- -LRB-_-LRB- A1_NN ,_, C1_NN -RRB-_-RRB- ,_, ..._: ,_, -LRB-_-LRB- An_DT ,_, Cn_NN -RRB-_-RRB- -RCB-_-RRB- is_VBZ a_DT multi-agent_JJ system_NN composed_VBN of_IN A_NN =_JJ -LCB-_-LRB- Ai_NN ,_, ..._: ,_, An_DT -RCB-_-RRB- ,_, a_DT set_NN of_IN CBR_NNP agents_NNS ,_, where_WRB each_DT agent_NN Ai_NNP A_NNP possesses_VBZ an_DT individual_JJ case_NN base_NN Ci_NN ._.
Each_DT individual_JJ agent_NN Ai_NNP in_IN a_DT MAC_NNP is_VBZ completely_RB autonomous_JJ and_CC each_DT agent_NN Ai_NNP has_VBZ access_NN only_RB to_TO its_PRP$ individual_JJ and_CC private_JJ case_NN base_NN Ci_NN ._.
A_DT case_NN base_NN Ci_NN =_JJ -LCB-_-LRB- c1_NN ,_, ..._: ,_, cm_NN -RCB-_-RRB- is_VBZ a_DT collection_NN of_IN cases_NNS ._.
Agents_NNS in_IN a_DT MAC_NNP system_NN are_VBP able_JJ to_TO individually_RB solve_VB problems_NNS ,_, but_CC they_PRP can_MD also_RB collaborate_VB with_IN other_JJ agents_NNS to_TO solve_VB problems_NNS ._.
In_IN this_DT framework_NN ,_, we_PRP will_MD restrict_VB ourselves_PRP to_TO analytical_JJ tasks_NNS ,_, i_FW ._.
e_LS ._.
tasks_NNS like_IN classification_NN ,_, where_WRB the_DT solution_NN of_IN a_DT problem_NN is_VBZ achieved_VBN by_IN selecting_VBG a_DT solution_NN class_NN from_IN an_DT enumerated_JJ set_NN of_IN solution_NN classes_NNS ._.
In_IN the_DT following_VBG we_PRP will_MD note_VB the_DT set_NN of_IN all_PDT the_DT solution_NN classes_NNS by_IN S_NN =_JJ -LCB-_-LRB- S1_NN ,_, ..._: ,_, SK_NNS -RCB-_-RRB- ._.
Therefore_RB ,_, a_DT case_NN c_NN =_JJ P_NN ,_, S_NN is_VBZ a_DT tuple_NN containing_VBG a_DT case_NN description_NN P_NN and_CC a_DT solution_NN class_NN S_NN S_NN ._.
In_IN the_DT following_VBG ,_, we_PRP will_MD use_VB the_DT terms_NNS problem_NN and_CC case_NN description_NN indistinctly_RB ._.
Moreover_RB ,_, we_PRP will_MD use_VB the_DT dot_NN notation_NN to_TO refer_VB to_TO elements_NNS inside_IN a_DT tuple_NN ;_: e_LS ._.
g_NN ._.
,_, to_TO refer_VB to_TO the_DT solution_NN class_NN of_IN a_DT case_NN c_NN ,_, we_PRP will_MD write_VB c_NN ._.
S_NN ._.
Therefore_RB ,_, we_PRP say_VBP a_DT group_NN of_IN agents_NNS perform_VBP joint_JJ deliberation_NN ,_, when_WRB they_PRP collaborate_VBP to_TO find_VB a_DT joint_JJ solution_NN by_IN means_NNS of_IN an_DT argumentation_NN process_NN ._.
However_RB ,_, in_IN order_NN to_TO do_VB so_RB ,_, an_DT agent_NN has_VBZ to_TO be_VB able_JJ to_TO justify_VB its_PRP$ prediction_NN to_TO the_DT other_JJ agents_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
generate_VB an_DT argument_NN for_IN its_PRP$ predicted_VBN solution_NN that_WDT can_MD be_VB examined_VBN and_CC critiqued_VBN by_IN the_DT other_JJ agents_NNS -RRB-_-RRB- ._.
The_DT next_JJ section_NN addresses_NNS this_DT issue_NN ._.
3_LS ._.
#_# Justified_NNP Predictions_NNS Both_CC expert_NN systems_NNS and_CC CBR_NN systems_NNS may_MD have_VB an_DT explanation_NN component_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- in_IN charge_NN of_IN justifying_VBG why_WRB the_DT system_NN has_VBZ provided_VBN a_DT specific_JJ answer_NN to_TO the_DT user_NN ._.
The_DT line_NN of_IN reasoning_NN of_IN the_DT system_NN can_MD then_RB be_VB examined_VBN by_IN a_DT human_JJ expert_NN ,_, thus_RB increasing_VBG the_DT reliability_NN of_IN the_DT system_NN ._.
Most_JJS of_IN the_DT existing_VBG work_NN on_IN explanation_NN generation_NN focuses_VBZ on_IN generating_VBG explanations_NNS to_TO be_VB provided_VBN to_TO the_DT user_NN ._.
However_RB ,_, in_IN our_PRP$ approach_NN we_PRP use_VBP explanations_NNS -LRB-_-LRB- or_CC justifications_NNS -RRB-_-RRB- as_IN a_DT tool_NN for_IN improving_VBG communication_NN and_CC coordination_NN among_IN agents_NNS ._.
We_PRP are_VBP interested_JJ in_IN justifications_NNS since_IN they_PRP can_MD be_VB used_VBN as_IN arguments_NNS ._.
For_IN that_DT purpose_NN ,_, we_PRP will_MD benefit_VB from_IN the_DT ability_NN of_IN some_DT machine_NN learning_VBG methods_NNS to_TO provide_VB justifications_NNS ._.
A_DT justification_NN built_VBN by_IN a_DT CBR_NN method_NN after_IN determining_VBG that_IN the_DT solution_NN of_IN a_DT particular_JJ problem_NN P_NN was_VBD Sk_NNP is_VBZ a_DT description_NN that_WDT contains_VBZ the_DT relevant_JJ information_NN from_IN the_DT problem_NN P_NN that_IN the_DT CBR_NNP method_NN has_VBZ considered_VBN to_TO predict_VB Sk_NN as_IN the_DT solution_NN of_IN P_NN ._.
In_IN particular_JJ ,_, CBR_NNP methods_NNS work_VBP by_IN retrieving_VBG similar_JJ cases_NNS to_TO the_DT problem_NN at_IN hand_NN ,_, and_CC then_RB reusing_VBG their_PRP$ solutions_NNS for_IN the_DT current_JJ problem_NN ,_, expecting_VBG that_IN since_IN the_DT problem_NN and_CC the_DT cases_NNS are_VBP similar_JJ ,_, the_DT solutions_NNS will_MD also_RB be_VB similar_JJ ._.
Thus_RB ,_, if_IN a_DT CBR_NN method_NN has_VBZ retrieved_VBN a_DT set_NN of_IN cases_NNS C1_NN ,_, ..._: ,_, Cn_NN to_TO solve_VB a_DT particular_JJ problem_NN P_NN the_DT justification_NN built_VBN will_MD contain_VB the_DT relevant_JJ information_NN from_IN the_DT problem_NN P_NN that_WDT made_VBD the_DT CBR_NN system_NN retrieve_VBP that_IN particular_JJ set_NN of_IN cases_NNS ,_, i_FW ._.
e_LS ._.
it_PRP will_MD contain_VB the_DT relevant_JJ information_NN that_WDT P_NN and_CC C1_NN ,_, ..._: ,_, Cn_NNP have_VBP in_IN common_JJ ._.
For_IN example_NN ,_, Figure_NNP #_# shows_VBZ a_DT justification_NN build_VB by_IN a_DT CBR_NN system_NN for_IN a_DT toy_NN problem_NN -LRB-_-LRB- in_IN the_DT following_VBG sections_NNS we_PRP will_MD show_VB justifications_NNS for_IN real_JJ problems_NNS -RRB-_-RRB- ._.
In_IN the_DT figure_NN ,_, a_DT problem_NN has_VBZ two_CD attributes_NNS -LRB-_-LRB- Traffic_NN __CD light_NN ,_, and_CC Cars_NNS __VBP passing_VBG -RRB-_-RRB- ,_, the_DT retrieval_NN mechanism_NN of_IN the_DT CBR_NNP system_NN notices_NNS that_WDT by_IN considering_VBG only_RB the_DT attribute_NN Traffic_NN __CD light_NN ,_, it_PRP can_MD retrieve_VB two_CD cases_NNS that_WDT predict_VBP the_DT same_JJ solution_NN :_: wait_VB ._.
Thus_RB ,_, since_IN only_RB this_DT attribute_NN has_VBZ been_VBN used_VBN ,_, it_PRP is_VBZ the_DT only_RB one_CD appearing_VBG in_IN the_DT justification_NN ._.
The_DT values_NNS of_IN the_DT rest_NN of_IN attributes_NNS are_VBP irrelevant_JJ ,_, since_IN whatever_WDT their_PRP$ value_NN the_DT solution_NN class_NN would_MD have_VB been_VBN the_DT same_JJ ._.
976_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- Problem_NNP Traffic_NNP __NNP light_NN :_: red_JJ Cars_NNPS __CD passing_NN :_: no_DT Case_NN #_# Traffic_NNP __CD light_NN :_: red_JJ Cars_NNPS __CD passing_NN :_: no_DT Solution_NN :_: wait_VB Case_NNP #_# Traffic_NNP __CD light_NN :_: red_JJ Cars_NNPS __CD passing_NN :_: yes_UH Solution_NN :_: wait_VB Case_NNP #_# Traffic_NNP __CD light_NN :_: green_JJ Cars_NNS __VBP passing_VBG :_: yes_UH Solution_NN :_: wait_VB Case_NNP #_# Traffic_NNP __CD light_NN :_: green_JJ Cars_NNS __VBP passing_VBG :_: no_DT Solution_NN :_: cross_VB Retrieved_VBN cases_NNS Solution_NN :_: wait_VB Justification_NN Traffic_NN __CD light_NN :_: red_JJ Figure_NN #_# :_: An_DT example_NN of_IN justification_NN generation_NN in_IN a_DT CBR_NN system_NN ._.
Notice_NNP that_IN ,_, since_IN the_DT only_JJ relevant_JJ feature_NN to_TO decide_VB is_VBZ Traffic_NNP __NNP light_NN -LRB-_-LRB- the_DT only_RB one_CD used_VBN to_TO retrieve_VB cases_NNS -RRB-_-RRB- ,_, it_PRP is_VBZ the_DT only_RB one_CD appearing_VBG in_IN the_DT justification_NN ._.
In_IN general_JJ ,_, the_DT meaning_NN of_IN a_DT justification_NN is_VBZ that_IN all_DT -LRB-_-LRB- or_CC most_JJS of_IN -RRB-_-RRB- the_DT cases_NNS in_IN the_DT case_NN base_NN of_IN an_DT agent_NN that_WDT satisfy_VBP the_DT justification_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
all_DT the_DT cases_NNS that_WDT are_VBP subsumed_VBN by_IN the_DT justification_NN -RRB-_-RRB- belong_VBP to_TO the_DT predicted_VBN solution_NN class_NN ._.
In_IN the_DT rest_NN of_IN the_DT paper_NN ,_, we_PRP will_MD use_VB to_TO denote_VB the_DT subsumption_NN relation_NN ._.
In_IN our_PRP$ work_NN ,_, we_PRP use_VBP LID_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, a_DT CBR_NN method_NN capable_JJ of_IN building_VBG symbolic_JJ justifications_NNS such_JJ as_IN the_DT one_CD exemplified_VBN in_IN Figure_NNP #_# ._.
When_WRB an_DT agent_NN provides_VBZ a_DT justification_NN for_IN a_DT prediction_NN ,_, the_DT agent_NN generates_VBZ a_DT justified_JJ prediction_NN :_: DEFINITION_NNP #_# ._.
#_# ._.
A_DT Justified_NNP Prediction_NN is_VBZ a_DT tuple_NN J_NN =_JJ A_NN ,_, P_NN ,_, S_NN ,_, D_NN where_WRB agent_NN A_NN considers_VBZ S_NN the_DT correct_JJ solution_NN for_IN problem_NN P_NN ,_, and_CC that_IN prediction_NN is_VBZ justified_VBN a_DT symbolic_JJ description_NN D_NN such_JJ that_IN J_NN ._.
D_NN J_NN ._.
P_NN ._.
Justifications_NNPS can_MD have_VB many_JJ uses_NNS for_IN CBR_NNP systems_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP are_VBP going_VBG to_TO use_VB justifications_NNS as_IN arguments_NNS ,_, in_IN order_NN to_TO allow_VB learning_VBG agents_NNS to_TO engage_VB in_IN argumentation_NN processes_NNS ._.
4_LS ._.
ARGUMENTS_NNS AND_CC COUNTERARGUMENTS_NNS For_IN our_PRP$ purposes_NNS an_DT argument_NN generated_VBN by_IN an_DT agent_NN A_NN is_VBZ composed_VBN of_IN a_DT statement_NN S_NN and_CC some_DT evidence_NN D_NN supporting_VBG S_NN as_IN correct_JJ ._.
In_IN the_DT remainder_NN of_IN this_DT section_NN we_PRP will_MD see_VB how_WRB this_DT general_JJ definition_NN of_IN argument_NN can_MD be_VB instantiated_VBN in_IN specific_JJ kind_NN of_IN arguments_NNS that_IN the_DT agents_NNS can_MD generate_VB ._.
In_IN the_DT context_NN of_IN MAC_NNP systems_NNS ,_, agents_NNS argue_VBP about_IN predictions_NNS for_IN new_JJ problems_NNS and_CC can_MD provide_VB two_CD kinds_NNS of_IN information_NN :_: a_DT -RRB-_-RRB- specific_JJ cases_NNS P_NN ,_, S_NN ,_, and_CC b_NN -RRB-_-RRB- justified_JJ predictions_NNS :_: A_NN ,_, P_NN ,_, S_NN ,_, D_NN ._.
Using_VBG this_DT information_NN ,_, we_PRP can_MD define_VB three_CD types_NNS of_IN arguments_NNS :_: justified_JJ predictions_NNS ,_, counterarguments_NNS ,_, and_CC counterexamples_NNS ._.
A_DT justified_JJ prediction_NN is_VBZ generated_VBN by_IN an_DT agent_NN Ai_NN to_TO argue_VB that_IN Ai_NNP believes_VBZ that_IN the_DT correct_JJ solution_NN for_IN a_DT given_VBN problem_NN P_NN is_VBZ ._.
S_NN ,_, and_CC the_DT evidence_NN provided_VBN is_VBZ the_DT justification_NN ._.
D_NN ._.
In_IN the_DT example_NN depicted_VBN in_IN Figure_NNP #_# ,_, an_DT agent_NN Ai_NN may_MD generate_VB the_DT argument_NN =_JJ Ai_NN ,_, P_NN ,_, Wait_VB ,_, -LRB-_-LRB- Traffic_NNP __CD light_NN =_JJ red_NN -RRB-_-RRB- ,_, meaning_VBG that_IN the_DT agent_NN Ai_NNP believes_VBZ that_IN the_DT correct_JJ solution_NN for_IN P_NN is_VBZ Wait_VB because_IN the_DT attribute_NN Traffic_NN __CD light_JJ equals_VBZ red_NN ._.
A_DT counterargument_NN is_VBZ an_DT argument_NN offered_VBN in_IN opposition_NN to_TO another_DT argument_NN ._.
In_IN our_PRP$ framework_NN ,_, a_DT counterargument_NN consists_VBZ of_IN a_DT justified_JJ prediction_NN Aj_NNP ,_, P_NN ,_, S_NN ,_, D_NN generated_VBN by_IN an_DT agent_NN Aj_NN with_IN the_DT intention_NN to_TO rebut_VB an_DT argument_NN generated_VBN by_IN another_DT agent_NN Ai_NNP ,_, that_IN endorses_VBZ a_DT solution_NN class_NN S_NN different_JJ from_IN that_DT of_IN ._.
S_NN for_IN the_DT problem_NN at_IN hand_NN and_CC justifies_VBZ this_DT with_IN a_DT justification_NN D_NN ._.
In_IN the_DT example_NN in_IN Figure_NNP #_# ,_, if_IN an_DT agent_NN generates_VBZ the_DT argument_NN =_JJ Ai_NN ,_, P_NN ,_, Walk_VB ,_, -LRB-_-LRB- Cars_NNS __VBP passing_NN =_JJ no_DT -RRB-_-RRB- ,_, an_DT agent_NN that_WDT thinks_VBZ that_IN the_DT correct_JJ solution_NN is_VBZ Wait_VB might_MD answer_VB with_IN the_DT counterargument_NN =_JJ Aj_NN ,_, P_NN ,_, Wait_VB ,_, -LRB-_-LRB- Cars_NNS __VBP passing_NN =_JJ no_DT Traffic_NN __NN light_NN =_JJ red_NN -RRB-_-RRB- ,_, meaning_VBG that_IN ,_, although_IN there_EX are_VBP no_DT cars_NNS passing_VBG ,_, the_DT traffic_NN light_NN is_VBZ red_JJ ,_, and_CC the_DT street_NN can_MD not_RB be_VB crossed_VBN ._.
A_DT counterexample_NN c_NN is_VBZ a_DT case_NN that_WDT contradicts_VBZ an_DT argument_NN ._.
Thus_RB a_DT counterexample_NN is_VBZ also_RB a_DT counterargument_NN ,_, one_CD that_WDT states_VBZ that_IN a_DT specific_JJ argument_NN is_VBZ not_RB always_RB true_JJ ,_, and_CC the_DT evidence_NN provided_VBN is_VBZ the_DT case_NN c_NN ._.
Specifically_RB ,_, for_IN a_DT case_NN c_NN to_TO be_VB a_DT counterexample_NN of_IN an_DT argument_NN ,_, the_DT following_JJ conditions_NNS have_VBP to_TO be_VB met_VBN :_: ._.
D_NN c_NN and_CC ._.
S_NN =_JJ c_NN ._.
S_NN ,_, i_FW ._.
e_LS ._.
the_DT case_NN must_MD satisfy_VB the_DT justification_NN ._.
D_NN and_CC the_DT solution_NN of_IN c_NN must_MD be_VB different_JJ than_IN the_DT predicted_VBN by_IN ._.
By_IN exchanging_VBG arguments_NNS and_CC counterarguments_NNS -LRB-_-LRB- including_VBG counterexamples_NNS -RRB-_-RRB- ,_, agents_NNS can_MD argue_VB about_IN the_DT correct_JJ solution_NN of_IN a_DT given_VBN problem_NN ,_, i_FW ._.
e_LS ._.
they_PRP can_MD engage_VB a_DT joint_JJ deliberation_NN process_NN ._.
However_RB ,_, in_IN order_NN to_TO do_VB so_RB ,_, they_PRP need_VBP a_DT specific_JJ interaction_NN protocol_NN ,_, a_DT preference_NN relation_NN between_IN contradicting_VBG arguments_NNS ,_, and_CC a_DT decision_NN policy_NN to_TO generate_VB counterarguments_NNS -LRB-_-LRB- including_VBG counterexamples_NNS -RRB-_-RRB- ._.
In_IN the_DT following_VBG sections_NNS we_PRP will_MD present_VB these_DT elements_NNS ._.
5_CD ._.
PREFERENCE_NN RELATION_NN A_NN specific_JJ argument_NN provided_VBN by_IN an_DT agent_NN might_MD not_RB be_VB consistent_JJ with_IN the_DT information_NN known_VBN to_TO other_JJ agents_NNS -LRB-_-LRB- or_CC even_RB to_TO some_DT of_IN the_DT information_NN known_VBN by_IN the_DT agent_NN that_WDT has_VBZ generated_VBN the_DT justification_NN due_JJ to_TO noise_NN in_IN training_NN data_NNS -RRB-_-RRB- ._.
For_IN that_DT reason_NN ,_, we_PRP are_VBP going_VBG to_TO define_VB a_DT preference_NN relation_NN over_IN contradicting_VBG justified_JJ predictions_NNS based_VBN on_IN cases_NNS ._.
Basically_RB ,_, we_PRP will_MD define_VB a_DT confidence_NN measure_NN for_IN each_DT justified_JJ prediction_NN -LRB-_-LRB- that_WDT takes_VBZ into_IN account_NN the_DT cases_NNS owned_VBN by_IN each_DT agent_NN -RRB-_-RRB- ,_, and_CC the_DT justified_JJ prediction_NN with_IN the_DT highest_JJS confidence_NN will_MD be_VB the_DT preferred_JJ one_CD ._.
The_DT idea_NN behind_IN case-based_JJ confidence_NN is_VBZ to_TO count_VB how_WRB many_JJ of_IN the_DT cases_NNS in_IN an_DT individual_JJ case_NN base_NN endorse_VB a_DT justified_JJ prediction_NN ,_, and_CC how_WRB many_JJ of_IN them_PRP are_VBP counterexamples_NNS of_IN it_PRP ._.
The_DT more_JJR the_DT endorsing_VBG cases_NNS ,_, the_DT higher_JJR the_DT confidence_NN ;_: and_CC the_DT more_JJR the_DT counterexamples_NNS ,_, the_DT lower_JJR the_DT confidence_NN ._.
Specifically_RB ,_, to_TO assess_VB the_DT confidence_NN of_IN a_DT justified_JJ prediction_NN ,_, an_DT agent_NN obtains_VBZ the_DT set_NN of_IN cases_NNS in_IN its_PRP$ individual_JJ case_NN base_NN that_WDT are_VBP subsumed_VBN by_IN ._.
D_NN ._.
With_IN them_PRP ,_, an_DT agent_NN Ai_NNP obtains_VBZ the_DT Y_NN -LRB-_-LRB- aye_NN -RRB-_-RRB- and_CC N_NN -LRB-_-LRB- nay_NN -RRB-_-RRB- values_NNS :_: Y_NN Ai_NN =_JJ |_CD -LCB-_-LRB- c_NN Ci_NN |_NN ._.
D_NN c_NN ._.
P_NN ._.
S_NN =_JJ c_NN ._.
S_NN -RCB-_-RRB- |_NN is_VBZ the_DT number_NN of_IN cases_NNS in_IN the_DT agent_NN ''_'' s_NNS case_NN base_NN subsumed_VBN by_IN the_DT justification_NN ._.
D_NN that_WDT belong_VBP to_TO the_DT solution_NN class_NN ._.
S_NN ,_, NAi_NN =_JJ |_CD -LCB-_-LRB- c_NN Ci_NN |_NN ._.
D_NN c_NN ._.
P_NN ._.
S_NN =_JJ c_NN ._.
S_NN -RCB-_-RRB- |_NN is_VBZ the_DT number_NN of_IN cases_NNS in_IN the_DT agent_NN ''_'' s_NNS case_NN base_NN subsumed_VBN by_IN justification_NN ._.
D_NN that_WDT do_VBP not_RB belong_VB to_TO that_DT solution_NN class_NN ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD +_CC +_CC +_CC +_CC +_CC +_CC -_: -_: +_CC Figure_NNP #_# :_: Confidence_NN of_IN arguments_NNS is_VBZ evaluated_VBN by_IN contrasting_VBG them_PRP against_IN the_DT case_NN bases_NNS of_IN the_DT agents_NNS ._.
An_DT agent_NN estimates_VBZ the_DT confidence_NN of_IN an_DT argument_NN as_IN :_: CAi_NNS -LRB-_-LRB- -RRB-_-RRB- =_JJ Y_NN Ai_NN 1_CD +_CC Y_NN Ai_NN +_CC NAi_NNP i_LS ._.
e_LS ._.
the_DT confidence_NN on_IN a_DT justified_JJ prediction_NN is_VBZ the_DT number_NN of_IN endorsing_VBG cases_NNS divided_VBN by_IN the_DT number_NN of_IN endorsing_VBG cases_NNS plus_CC counterexamples_NNS ._.
Notice_NNP that_IN we_PRP add_VBP #_# to_TO the_DT denominator_NN ,_, this_DT is_VBZ to_TO avoid_VB giving_VBG excessively_RB high_JJ confidences_NNS to_TO justified_JJ predictions_NNS whose_WP$ confidence_NN has_VBZ been_VBN computed_VBN using_VBG a_DT small_JJ number_NN of_IN cases_NNS ._.
Notice_NNP that_IN this_DT correction_NN follows_VBZ the_DT same_JJ idea_NN than_IN the_DT Laplace_NNP correction_NN to_TO estimate_VB probabilities_NNS ._.
Figure_NNP #_# illustrates_VBZ the_DT individual_JJ evaluation_NN of_IN the_DT confidence_NN of_IN an_DT argument_NN ,_, in_IN particular_JJ ,_, three_CD endorsing_VBG cases_NNS and_CC one_CD counterexample_NN are_VBP found_VBN in_IN the_DT case_NN base_NN of_IN agents_NNS Ai_NNP ,_, giving_VBG an_DT estimated_VBN confidence_NN of_IN #_# ._.
#_# Moreover_RB ,_, we_PRP can_MD also_RB define_VB the_DT joint_JJ confidence_NN of_IN an_DT argument_NN as_IN the_DT confidence_NN computed_VBN using_VBG the_DT cases_NNS present_JJ in_IN the_DT case_NN bases_NNS of_IN all_PDT the_DT agents_NNS in_IN the_DT group_NN :_: C_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ i_FW Y_NN Ai_NN 1_CD +_CC i_FW Y_NN Ai_NN +_CC NAi_NNP Notice_NNP that_IN ,_, to_TO collaboratively_RB compute_VB the_DT joint_JJ confidence_NN ,_, the_DT agents_NNS only_RB have_VBP to_TO make_VB public_JJ the_DT aye_NN and_CC nay_NN values_NNS locally_RB computed_VBD for_IN a_DT given_VBN argument_NN ._.
In_IN our_PRP$ framework_NN ,_, agents_NNS use_VBP this_DT joint_JJ confidence_NN as_IN the_DT preference_NN relation_NN :_: a_DT justified_JJ prediction_NN is_VBZ preferred_VBN over_IN another_DT one_CD if_IN C_NN -LRB-_-LRB- -RRB-_-RRB- C_NN -LRB-_-LRB- -RRB-_-RRB- ._.
6_CD ._.
GENERATION_NN OF_IN ARGUMENTS_NNS In_IN our_PRP$ framework_NN ,_, arguments_NNS are_VBP generated_VBN by_IN the_DT agents_NNS from_IN cases_NNS ,_, using_VBG learning_VBG methods_NNS ._.
Any_DT learning_NN method_NN able_JJ to_TO provide_VB a_DT justified_JJ prediction_NN can_MD be_VB used_VBN to_TO generate_VB arguments_NNS ._.
For_IN instance_NN ,_, decision_NN trees_NNS and_CC LID_NN -LSB-_-LRB- #_# -RSB-_-RRB- are_VBP suitable_JJ learning_NN methods_NNS ._.
Specifically_RB ,_, in_IN the_DT experiments_NNS reported_VBN in_IN this_DT paper_NN agents_NNS use_VBP LID_NN ._.
Thus_RB ,_, when_WRB an_DT agent_NN wants_VBZ to_TO generate_VB an_DT argument_NN endorsing_VBG that_IN a_DT specific_JJ solution_NN class_NN is_VBZ the_DT correct_JJ solution_NN for_IN a_DT problem_NN P_NN ,_, it_PRP generates_VBZ a_DT justified_JJ prediction_NN as_IN explained_VBN in_IN Section_NN #_# ._.
#_# ._.
For_IN instance_NN ,_, Figure_NNP #_# shows_VBZ a_DT real_JJ justification_NN generated_VBN by_IN LID_NN after_IN solving_VBG a_DT problem_NN P_NN in_IN the_DT domain_NN of_IN marine_JJ sponges_NNS identification_NN ._.
In_IN particular_JJ ,_, Figure_NNP #_# shows_VBZ how_WRB when_WRB an_DT agent_NN receives_VBZ a_DT new_JJ problem_NN to_TO solve_VB -LRB-_-LRB- in_IN this_DT case_NN ,_, a_DT new_JJ sponge_NN to_TO determine_VB its_PRP$ order_NN -RRB-_-RRB- ,_, the_DT agent_NN uses_VBZ LID_NN to_TO generate_VB an_DT argument_NN -LRB-_-LRB- consisting_VBG on_IN a_DT justified_JJ prediction_NN -RRB-_-RRB- using_VBG the_DT cases_NNS in_IN the_DT case_NN base_NN of_IN the_DT agent_NN ._.
The_DT justification_NN shown_VBN in_IN Figure_NNP #_# can_MD be_VB interpreted_VBN saying_VBG that_IN the_DT predicted_VBN solution_NN is_VBZ hadromerida_NN because_IN the_DT smooth_JJ form_NN of_IN the_DT megascleres_NNS of_IN the_DT spiculate_JJ skeleton_NN of_IN the_DT sponge_NN is_VBZ of_IN type_NN tylostyle_NN ,_, the_DT spikulate_JJ skeleton_NN of_IN the_DT sponge_NN has_VBZ no_DT uniform_JJ length_NN ,_, and_CC there_EX is_VBZ no_DT gemmules_NNS in_IN the_DT external_JJ features_NNS of_IN the_DT sponge_NN ._.
Thus_RB ,_, the_DT argument_NN generated_VBN will_MD be_VB =_JJ A1_NN ,_, P_NN ,_, hadromerida_NN ,_, D1_NN ._.
6_CD ._.
#_# Generation_NNP of_IN Counterarguments_NNP As_IN previously_RB stated_VBN ,_, agents_NNS may_MD try_VB to_TO rebut_VB arguments_NNS by_IN generating_VBG counterargument_NN or_CC by_IN finding_VBG counterexamples_NNS ._.
Let_VB us_PRP explain_VB how_WRB they_PRP can_MD be_VB generated_VBN ._.
An_DT agent_NN Ai_NN wants_VBZ to_TO generate_VB a_DT counterargument_NN to_TO rebut_VB an_DT argument_NN when_WRB is_VBZ in_IN contradiction_NN with_IN the_DT local_JJ case_NN base_NN of_IN Ai_NNP ._.
Moreover_RB ,_, while_IN generating_VBG such_JJ counterargument_NN ,_, Ai_NNP expects_VBZ that_DT is_VBZ preferred_VBN over_RP ._.
For_IN that_DT purpose_NN ,_, we_PRP will_MD present_VB a_DT specific_JJ policy_NN to_TO generate_VB counterarguments_NNS based_VBN on_IN the_DT specificity_NN criterion_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
The_DT specificity_NN criterion_NN is_VBZ widely_RB used_VBN in_IN deductive_JJ frameworks_NNS for_IN argumentation_NN ,_, and_CC states_VBZ that_IN between_IN two_CD conflicting_VBG arguments_NNS ,_, the_DT most_RBS specific_JJ should_MD be_VB preferred_VBN since_IN it_PRP is_VBZ ,_, in_IN principle_NN ,_, more_JJR informed_VBN ._.
Thus_RB ,_, counterarguments_NNS generated_VBD based_VBN on_IN the_DT specificity_NN criterion_NN are_VBP expected_VBN to_TO be_VB preferable_JJ -LRB-_-LRB- since_IN they_PRP are_VBP more_RBR informed_VBN -RRB-_-RRB- to_TO the_DT arguments_NNS they_PRP try_VBP to_TO rebut_VB ._.
However_RB ,_, there_EX is_VBZ no_DT guarantee_NN that_IN such_JJ counterarguments_NNS will_MD always_RB win_VB ,_, since_IN ,_, as_IN we_PRP have_VBP stated_VBN in_IN Section_NN #_# ,_, agents_NNS in_IN our_PRP$ framework_NN use_VB a_DT preference_NN relation_NN based_VBN on_IN joint_JJ confidence_NN ._.
Moreover_RB ,_, one_CD may_MD think_VB that_IN it_PRP would_MD be_VB better_RBR that_IN the_DT agents_NNS generate_VBP counterarguments_NNS based_VBN on_IN the_DT joint_JJ confidence_NN preference_NN relation_NN ;_: however_RB it_PRP is_VBZ not_RB obvious_JJ how_WRB to_TO generate_VB counterarguments_NNS based_VBN on_IN joint_JJ confidence_NN in_IN an_DT efficient_JJ way_NN ,_, since_IN collaboration_NN is_VBZ required_VBN in_IN order_NN to_TO evaluate_VB joint_JJ confidence_NN ._.
Thus_RB ,_, the_DT agent_NN generating_VBG the_DT counterargument_NN should_MD constantly_RB communicate_VB with_IN the_DT other_JJ agents_NNS at_IN each_DT step_NN of_IN the_DT induction_NN algorithm_NN used_VBN to_TO generate_VB counterarguments_NNS -LRB-_-LRB- presently_RB one_CD of_IN our_PRP$ future_JJ research_NN lines_NNS -RRB-_-RRB- ._.
Thus_RB ,_, in_IN our_PRP$ framework_NN ,_, when_WRB an_DT agent_NN wants_VBZ to_TO generate_VB a_DT counterargument_NN to_TO an_DT argument_NN ,_, has_VBZ to_TO be_VB more_RBR specific_JJ than_IN -LRB-_-LRB- i_LS ._.
e_LS ._. ._.
D_NN <_JJR ._.
D_NN -RRB-_-RRB- ._.
The_DT generation_NN of_IN counterarguments_NNS using_VBG the_DT specificity_NN criterion_NN imposes_VBZ some_DT restrictions_NNS over_IN the_DT learning_NN method_NN ,_, although_IN LID_NN or_CC ID3_NN can_MD be_VB easily_RB adapted_VBN for_IN this_DT task_NN ._.
For_IN instance_NN ,_, LID_NN is_VBZ an_DT algorithm_NN that_WDT generates_VBZ a_DT description_NN starting_VBG from_IN scratch_NN and_CC heuristically_RB adding_VBG features_NNS to_TO that_DT term_NN ._.
Thus_RB ,_, at_IN every_DT step_NN ,_, the_DT description_NN is_VBZ made_VBN more_RBR specific_JJ than_IN in_IN the_DT previous_JJ step_NN ,_, and_CC the_DT number_NN of_IN cases_NNS that_WDT are_VBP subsumed_VBN by_IN that_DT description_NN is_VBZ reduced_VBN ._.
When_WRB the_DT description_NN covers_VBZ only_RB -LRB-_-LRB- or_CC almost_RB only_RB -RRB-_-RRB- cases_NNS of_IN a_DT single_JJ solution_NN class_NN LID_NN terminates_VBZ and_CC predicts_VBZ that_IN solution_NN class_NN ._.
To_TO generate_VB a_DT counterargument_NN to_TO an_DT argument_NN LID_NN just_RB has_VBZ to_TO use_VB as_IN starting_VBG point_NN the_DT description_NN ._.
D_NN instead_RB of_IN starting_VBG from_IN scratch_NN ._.
In_IN this_DT way_NN ,_, the_DT justification_NN provided_VBN by_IN LID_NN will_MD always_RB be_VB subsumed_VBN by_IN ._.
D_NN ,_, and_CC thus_RB the_DT resulting_VBG counterargument_NN will_MD be_VB more_RBR specific_JJ than_IN ._.
However_RB ,_, notice_NN that_WDT LID_NN may_MD sometimes_RB not_RB be_VB able_JJ to_TO generate_VB counterarguments_NNS ,_, since_IN LID_NN may_MD not_RB be_VB able_JJ to_TO specialize_VB the_DT description_NN ._.
D_NNP any_DT further_RBR ,_, or_CC because_IN the_DT agent_NN Ai_NNP has_VBZ no_DT case_NN inCi_NN that_WDT is_VBZ subsumed_VBN by_IN ._.
D_NN ._.
Figure_NNP #_# shows_VBZ how_WRB an_DT agent_NN A2_NN that_WDT disagreed_VBD with_IN the_DT argument_NN shown_VBN in_IN Figure_NNP #_# ,_, generates_VBZ a_DT counterargument_NN using_VBG LID_NN ._.
Moreover_RB ,_, Figure_NNP #_# shows_VBZ the_DT generation_NN of_IN a_DT counterargument_NN #_# 2_CD for_IN the_DT argument_NN #_# 1_CD -LRB-_-LRB- in_IN Figure_NNP #_# -RRB-_-RRB- that_WDT is_VBZ a_DT specialization_NN of_IN #_# 1_CD ._.
978_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- Solution_NN :_: hadromerida_NN Justification_NN :_: D1_NN Sponge_NNP Spikulate_NNP skeleton_NNP External_NNP features_VBZ External_JJ features_NNS Gemmules_NNP :_: no_DT Spikulate_NNP Skeleton_NNP Megascleres_NNP Uniform_NNP length_NN :_: no_DT Megascleres_NNPS Smooth_JJ form_NN :_: tylostyle_NN Case_NN Base_NN of_IN A1_NN LID_NNP New_NNP sponge_VBP P_NN Figure_NN #_# :_: Example_NN of_IN a_DT real_JJ justification_NN generated_VBN by_IN LID_NN in_IN the_DT marine_JJ sponges_NNS data_NNS set_NN ._.
Specifically_RB ,_, in_IN our_PRP$ experiments_NNS ,_, when_WRB an_DT agent_NN Ai_NN wants_VBZ to_TO rebut_VB an_DT argument_NN ,_, uses_VBZ the_DT following_VBG policy_NN :_: 1_CD ._.
Agent_NNP Ai_NNP uses_VBZ LID_NN to_TO try_VB to_TO find_VB a_DT counterargument_NN more_RBR specific_JJ than_IN ;_: if_IN found_VBN ,_, is_VBZ sent_VBN to_TO the_DT other_JJ agent_NN as_IN a_DT counterargument_NN of_IN ._.
2_LS ._.
If_IN not_RB found_VBN ,_, then_RB Ai_NNP searches_NNS for_IN a_DT counterexample_NN c_NN Ci_NN of_IN ._.
If_IN a_DT case_NN c_NN is_VBZ found_VBN ,_, then_RB c_NN is_VBZ sent_VBN to_TO the_DT other_JJ agent_NN as_IN a_DT counterexample_NN of_IN ._.
3_LS ._.
If_IN no_DT counterexamples_NNS are_VBP found_VBN ,_, then_RB Ai_NNP can_MD not_RB rebut_VB the_DT argument_NN ._.
7_CD ._.
ARGUMENTATION-BASED_NN MULTI-AGENT_JJ LEARNING_NN The_DT interaction_NN protocol_NN of_IN AMAL_NNP allows_VBZ a_DT group_NN of_IN agents_NNS A1_NN ,_, ..._: ,_, An_DT to_TO deliberate_VB about_IN the_DT correct_JJ solution_NN of_IN a_DT problem_NN P_NN by_IN means_NNS of_IN an_DT argumentation_NN process_NN ._.
If_IN the_DT argumentation_NN process_NN arrives_VBZ to_TO a_DT consensual_JJ solution_NN ,_, the_DT joint_JJ deliberation_NN ends_VBZ ;_: otherwise_RB a_DT weighted_JJ vote_NN is_VBZ used_VBN to_TO determine_VB the_DT joint_JJ solution_NN ._.
Moreover_RB ,_, AMAL_NNP also_RB allows_VBZ the_DT agents_NNS to_TO learn_VB from_IN the_DT counterexamples_NNS received_VBN from_IN other_JJ agents_NNS ._.
The_DT AMAL_NNP protocol_NN consists_VBZ on_IN a_DT series_NN of_IN rounds_NNS ._.
In_IN the_DT initial_JJ round_NN ,_, each_DT agent_NN states_NNS which_WDT is_VBZ its_PRP$ individual_JJ prediction_NN for_IN P_NN ._.
Then_RB ,_, at_IN each_DT round_NN an_DT agent_NN can_MD try_VB to_TO rebut_VB the_DT prediction_NN made_VBN by_IN any_DT of_IN the_DT other_JJ agents_NNS ._.
The_DT protocol_NN uses_VBZ a_DT token_JJ passing_VBG mechanism_NN so_IN that_IN agents_NNS -LRB-_-LRB- one_CD at_IN a_DT time_NN -RRB-_-RRB- can_MD send_VB counterarguments_NNS or_CC counterexamples_NNS if_IN they_PRP disagree_VBP with_IN the_DT prediction_NN made_VBN by_IN any_DT other_JJ agent_NN ._.
Specifically_RB ,_, each_DT agent_NN is_VBZ allowed_VBN to_TO send_VB one_CD counterargument_NN or_CC counterexample_NN each_DT time_NN he_PRP gets_VBZ the_DT token_JJ -LRB-_-LRB- notice_NN that_IN this_DT restriction_NN is_VBZ just_RB to_TO simplify_VB the_DT protocol_NN ,_, and_CC that_IN it_PRP does_VBZ not_RB restrict_VB the_DT number_NN of_IN counterargument_NN an_DT agent_NN can_MD sent_VBN ,_, since_IN they_PRP can_MD be_VB delayed_VBN for_IN subsequent_JJ rounds_NNS -RRB-_-RRB- ._.
When_WRB an_DT agent_NN receives_VBZ a_DT counterargument_NN or_CC counterexample_NN ,_, it_PRP informs_VBZ the_DT other_JJ agents_NNS if_IN it_PRP accepts_VBZ the_DT counterargument_NN -LRB-_-LRB- and_CC changes_VBZ its_PRP$ prediction_NN -RRB-_-RRB- or_CC not_RB ._.
Moreover_RB ,_, agents_NNS have_VBP also_RB the_DT opportunity_NN to_TO answer_VB to_TO counterarguments_NNS when_WRB they_PRP receive_VBP the_DT token_JJ ,_, by_IN trying_VBG to_TO generate_VB a_DT counterargument_NN to_TO the_DT counterargument_NN ._.
When_WRB all_PDT the_DT agents_NNS have_VBP had_VBN the_DT token_JJ once_RB ,_, the_DT token_JJ returns_NNS to_TO the_DT first_JJ agent_NN ,_, and_CC so_RB on_IN ._.
If_IN at_IN any_DT time_NN in_IN the_DT protocol_NN ,_, all_PDT the_DT agents_NNS agree_VBP or_CC during_IN the_DT last_JJ n_NN rounds_NNS no_DT agent_NN has_VBZ generated_VBN any_DT counterargument_NN ,_, the_DT protocol_NN ends_VBZ ._.
Moreover_RB ,_, if_IN at_IN the_DT end_NN of_IN the_DT argumentation_NN the_DT agents_NNS have_VBP not_RB reached_VBN an_DT agreement_NN ,_, then_RB a_DT voting_NN mechanism_NN that_WDT uses_VBZ the_DT confidence_NN of_IN each_DT prediction_NN as_IN weights_NNS is_VBZ used_VBN to_TO decide_VB the_DT final_JJ solution_NN -LRB-_-LRB- Thus_RB ,_, AMAL_NN follows_VBZ the_DT same_JJ mechanism_NN as_IN human_JJ committees_NNS ,_, first_RB each_DT individual_JJ member_NN of_IN a_DT committee_NN exposes_VBZ his_PRP$ arguments_NNS and_CC discuses_NNS those_DT of_IN the_DT other_JJ members_NNS -LRB-_-LRB- joint_JJ deliberation_NN -RRB-_-RRB- ,_, and_CC if_IN no_DT consensus_NN is_VBZ reached_VBN ,_, then_RB a_DT voting_NN mechanism_NN is_VBZ required_VBN -RRB-_-RRB- ._.
At_IN each_DT iteration_NN ,_, agents_NNS can_MD use_VB the_DT following_VBG performatives_NNS :_: assert_VB -LRB-_-LRB- -RRB-_-RRB- :_: the_DT justified_JJ prediction_NN held_VBN during_IN the_DT next_JJ round_NN will_MD be_VB ._.
An_DT agent_NN can_MD only_RB hold_VB a_DT single_JJ prediction_NN at_IN each_DT round_NN ,_, thus_RB is_VBZ multiple_JJ asserts_VBZ are_VBP send_VB ,_, only_RB the_DT last_JJ one_CD is_VBZ considered_VBN as_IN the_DT currently_RB held_VBN prediction_NN ._.
rebut_VB -LRB-_-LRB- ,_, -RRB-_-RRB- :_: the_DT agent_NN has_VBZ found_VBN a_DT counterargument_NN to_TO the_DT prediction_NN ._.
We_PRP will_MD define_VB Ht_NN =_JJ t_NN 1_CD ,_, ..._: ,_, t_NN n_NN as_IN the_DT predictions_NNS that_IN each_DT of_IN the_DT n_NN agents_NNS hold_VBP at_IN a_DT round_JJ t_NN ._.
Moreover_RB ,_, we_PRP will_MD also_RB define_VB contradict_VB -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- =_JJ -LCB-_-LRB- Ht_JJ |_NN ._.
S_NN =_JJ t_NN i_FW ._.
S_NN -RCB-_-RRB- as_IN the_DT set_NN of_IN contradicting_VBG arguments_NNS for_IN an_DT agent_NN Ai_NNP in_IN a_DT round_JJ t_NN ,_, i_FW ._.
e_LS ._.
the_DT set_NN of_IN arguments_NNS at_IN round_JJ t_NN that_WDT support_VBP a_DT different_JJ solution_NN class_NN than_IN t_NN i_FW ._.
The_DT protocol_NN is_VBZ initiated_VBN because_IN one_CD of_IN the_DT agents_NNS receives_VBZ a_DT problem_NN P_NN to_TO be_VB solved_VBN ._.
After_IN that_DT ,_, the_DT agent_NN informs_VBZ all_PDT the_DT other_JJ agents_NNS about_IN the_DT problem_NN P_NN to_TO solve_VB ,_, and_CC the_DT protocol_NN starts_VBZ :_: 1_CD ._.
At_IN round_NN t_NN =_JJ #_# ,_, each_DT one_CD of_IN the_DT agents_NNS individually_RB solves_VBZ P_NN ,_, and_CC builds_VBZ a_DT justified_JJ prediction_NN using_VBG its_PRP$ own_JJ CBR_NN method_NN ._.
Then_RB ,_, each_DT agent_NN Ai_NNP sends_VBZ the_DT performative_JJ assert_VBP -LRB-_-LRB- #_# i_LS -RRB-_-RRB- to_TO the_DT other_JJ agents_NNS ._.
Thus_RB ,_, the_DT agents_NNS know_VBP H0_NN =_JJ #_# i_FW ,_, ..._: ,_, #_# n_NN ._.
Once_RB all_PDT the_DT predictions_NNS have_VBP been_VBN sent_VBN the_DT token_JJ is_VBZ given_VBN to_TO the_DT first_JJ agent_NN A1_NN ._.
2_LS ._.
At_IN each_DT round_NN t_NN -LRB-_-LRB- other_JJ than_IN #_# -RRB-_-RRB- ,_, the_DT agents_NNS check_VBP whether_IN their_PRP$ arguments_NNS in_IN Ht_NN agree_VBP ._.
If_IN they_PRP do_VBP ,_, the_DT protocol_NN moves_VBZ to_TO step_VB 5_CD ._.
Moreover_RB ,_, if_IN during_IN the_DT last_JJ n_NN rounds_NNS no_DT agent_NN has_VBZ sent_VBN any_DT counterexample_NN or_CC counterargument_NN ,_, the_DT protocol_NN also_RB moves_VBZ to_TO step_VB #_# ._.
Otherwise_RB ,_, the_DT agent_NN Ai_NN owner_NN of_IN the_DT token_JJ tries_VBZ to_TO generate_VB a_DT counterargument_NN for_IN each_DT of_IN the_DT opposing_VBG arguments_NNS in_IN contradict_VBP -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- Ht_NN ._.
Then_RB ,_, the_DT counterargument_NN t_NN i_FW against_IN the_DT prediction_NN t_NN j_NN with_IN the_DT lowest_JJS confidence_NN C_NN -LRB-_-LRB- t_NN j_NN -RRB-_-RRB- is_VBZ selected_VBN -LRB-_-LRB- since_IN t_NN j_NN is_VBZ the_DT prediction_NN more_RBR likely_JJ to_TO be_VB successfully_RB rebutted_VBN -RRB-_-RRB- ._.
If_IN t_NN i_FW is_VBZ a_DT counterargument_NN ,_, then_RB ,_, Ai_NNP locally_RB compares_VBZ t_NN i_FW with_IN t_NN i_FW by_IN assessing_VBG their_PRP$ confidence_NN against_IN its_PRP$ individual_JJ case_NN base_NN Ci_NN -LRB-_-LRB- notice_NN that_WDT Ai_NNP is_VBZ comparing_VBG its_PRP$ previous_JJ argument_NN with_IN the_DT counterargument_NN that_WDT Ai_NNP itself_PRP has_VBZ just_RB generated_VBN and_CC that_DT is_VBZ about_IN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD Sponge_NNP Spikulate_NNP skeleton_NNP External_NNP features_VBZ External_JJ features_NNS Gemmules_NNP :_: no_DT Growing_VBG :_: Spikulate_NNP Skeleton_NNP Megascleres_NNP Uniform_NNP length_NN :_: no_DT Megascleres_NNPS Smooth_JJ form_NN :_: tylostyle_NN Growing_VBG Grow_NNP :_: massive_JJ Case_NN Base_NN of_IN A2_NN LID_NN Solution_NN :_: astrophorida_NN Justification_NN :_: D2_NN Figure_NN #_# :_: Generation_NN of_IN a_DT counterargument_NN using_VBG LID_NN in_IN the_DT sponges_NNS data_NNS set_NN ._.
to_TO send_VB to_TO Aj_NN -RRB-_-RRB- ._.
If_IN CAi_NN -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- >_JJR CAi_NN -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- ,_, then_RB Ai_NNP considers_VBZ that_IN t_NN i_FW is_VBZ stronger_JJR than_IN its_PRP$ previous_JJ argument_NN ,_, changes_VBZ its_PRP$ argument_NN to_TO t_NN i_FW by_IN sending_VBG assert_VB -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- to_TO the_DT rest_NN of_IN the_DT agents_NNS -LRB-_-LRB- the_DT intuition_NN behind_IN this_DT is_VBZ that_IN since_IN a_DT counterargument_NN is_VBZ also_RB an_DT argument_NN ,_, Ai_NN checks_NNS if_IN the_DT newly_RB counterargument_NN is_VBZ a_DT better_JJR argument_NN than_IN the_DT one_CD he_PRP was_VBD previously_RB holding_VBG -RRB-_-RRB- and_CC rebut_VB -LRB-_-LRB- t_NN i_FW ,_, t_NN j_NN -RRB-_-RRB- to_TO Aj_NNP ._.
Otherwise_RB -LRB-_-LRB- i_FW ._.
e_LS ._.
CAi_NN -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- CAi_NN -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- -RRB-_-RRB- ,_, Ai_NNP will_MD send_VB only_RB rebut_VB -LRB-_-LRB- t_NN i_FW ,_, t_NN j_NN -RRB-_-RRB- to_TO Aj_NNP ._.
In_IN any_DT of_IN the_DT two_CD situations_NNS the_DT protocol_NN moves_VBZ to_TO step_VB #_# ._.
If_IN t_NN i_FW is_VBZ a_DT counterexample_NN c_NN ,_, then_RB Ai_NNP sends_VBZ rebut_VB -LRB-_-LRB- c_NN ,_, t_NN j_NN -RRB-_-RRB- to_TO Aj_NNP ._.
The_DT protocol_NN moves_VBZ to_TO step_VB #_# ._.
If_IN Ai_NNP can_MD not_RB generate_VB any_DT counterargument_NN or_CC counterexample_NN ,_, the_DT token_JJ is_VBZ sent_VBN to_TO the_DT next_JJ agent_NN ,_, a_DT new_JJ round_NN t_NN +_CC #_# starts_VBZ ,_, and_CC the_DT protocol_NN moves_VBZ to_TO state_NN #_# ._.
3_LS ._.
The_DT agent_NN Aj_NN that_WDT has_VBZ received_VBN the_DT counterargument_NN t_NN i_FW ,_, locally_RB compares_VBZ it_PRP against_IN its_PRP$ own_JJ argument_NN ,_, t_NN j_NN ,_, by_IN locally_RB assessing_VBG their_PRP$ confidence_NN ._.
If_IN CAj_NN -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- >_JJR CAj_NN -LRB-_-LRB- t_NN j_NN -RRB-_-RRB- ,_, then_RB Aj_NNP will_MD accept_VB the_DT counterargument_NN as_IN stronger_JJR than_IN its_PRP$ own_JJ argument_NN ,_, and_CC it_PRP will_MD send_VB assert_VB -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- to_TO the_DT other_JJ agents_NNS ._.
Otherwise_RB -LRB-_-LRB- i_FW ._.
e_LS ._.
CAj_NN -LRB-_-LRB- t_NN i_LS -RRB-_-RRB- CAj_NN -LRB-_-LRB- t_NN j_NN -RRB-_-RRB- -RRB-_-RRB- ,_, Aj_NNP will_MD not_RB accept_VB the_DT counterargument_NN ,_, and_CC will_MD inform_VB the_DT other_JJ agents_NNS accordingly_RB ._.
Any_DT of_IN the_DT two_CD situations_NNS start_VBP a_DT new_JJ round_NN t_NN +_CC #_# ,_, Ai_NNP sends_VBZ the_DT token_JJ to_TO the_DT next_JJ agent_NN ,_, and_CC the_DT protocol_NN moves_VBZ back_RB to_TO state_NN #_# ._.
4_LS ._.
The_DT agent_NN Aj_NN that_WDT has_VBZ received_VBN the_DT counterexample_NN c_NN retains_VBZ it_PRP into_IN its_PRP$ case_NN base_NN and_CC generates_VBZ a_DT new_JJ argument_NN t_NN +_CC #_# j_NN that_WDT takes_VBZ into_IN account_NN c_NN ,_, and_CC informs_VBZ the_DT rest_NN of_IN the_DT agents_NNS by_IN sending_VBG assert_VB -LRB-_-LRB- t_NN +_CC #_# j_NN -RRB-_-RRB- to_TO all_DT of_IN them_PRP ._.
Then_RB ,_, Ai_NNP sends_VBZ the_DT token_JJ to_TO the_DT next_JJ agent_NN ,_, a_DT new_JJ round_NN t_NN +_CC #_# starts_VBZ ,_, and_CC the_DT protocol_NN moves_VBZ back_RB to_TO step_VB #_# ._.
5_CD ._.
The_DT protocol_NN ends_VBZ yielding_VBG a_DT joint_JJ prediction_NN ,_, as_IN follows_VBZ :_: if_IN the_DT arguments_NNS in_IN Ht_NN agree_VBP then_RB their_PRP$ prediction_NN is_VBZ the_DT joint_JJ prediction_NN ,_, otherwise_RB a_DT voting_NN mechanism_NN is_VBZ used_VBN to_TO decide_VB the_DT joint_JJ prediction_NN ._.
The_DT voting_NN mechanism_NN uses_VBZ the_DT joint_JJ confidence_NN measure_NN as_IN the_DT voting_NN weights_NNS ,_, as_IN follows_VBZ :_: S_NN =_JJ arg_NN max_NN SkS_NNP iHt_NNP |_VBD i_LS ._.
S_NN =_JJ Sk_NN C_NN -LRB-_-LRB- i_LS -RRB-_-RRB- Moreover_RB ,_, in_IN order_NN to_TO avoid_VB infinite_JJ iterations_NNS ,_, if_IN an_DT agent_NN sends_VBZ twice_RB the_DT same_JJ argument_NN or_CC counterargument_NN to_TO the_DT same_JJ agent_NN ,_, the_DT message_NN is_VBZ not_RB considered_VBN ._.
8_CD ._.
EXEMPLIFICATION_NNP Let_VB us_PRP consider_VB a_DT system_NN composed_VBN of_IN three_CD agents_NNS A1_NN ,_, A2_NN and_CC A3_NN ._.
One_CD of_IN the_DT agents_NNS ,_, A1_NN receives_VBZ a_DT problem_NN P_NN to_TO solve_VB ,_, and_CC decides_VBZ to_TO use_VB AMAL_NNP to_TO solve_VB it_PRP ._.
For_IN that_DT reason_NN ,_, invites_VBZ A2_NN and_CC A3_NN to_TO take_VB part_NN in_IN the_DT argumentation_NN process_NN ._.
They_PRP accept_VBP the_DT invitation_NN ,_, and_CC the_DT argumentation_NN protocol_NN starts_VBZ ._.
Initially_RB ,_, each_DT agent_NN generates_VBZ its_PRP$ individual_JJ prediction_NN for_IN P_NN ,_, and_CC broadcasts_VBZ it_PRP to_TO the_DT other_JJ agents_NNS ._.
Thus_RB ,_, all_DT of_IN them_PRP can_MD compute_VB H0_NN =_JJ #_# 1_CD ,_, #_# 2_CD ,_, #_# 3_CD ._.
In_IN particular_JJ ,_, in_IN this_DT example_NN :_: #_# 1_CD =_JJ A1_NN ,_, P_NN ,_, hadromerida_NN ,_, D1_NN #_# 2_CD =_JJ A2_NN ,_, P_NN ,_, astrophorida_NN ,_, D2_NN #_# 3_CD =_JJ A3_NN ,_, P_NN ,_, axinellida_NN ,_, D3_NN A1_NN starts_VBZ owning_VBG the_DT token_JJ and_CC tries_VBZ to_TO generate_VB counterarguments_NNS for_IN #_# 2_CD and_CC #_# 3_CD ,_, but_CC does_VBZ not_RB succeed_VB ,_, however_RB it_PRP has_VBZ one_CD counterexample_NN c13_NN for_IN #_# 3_CD ._.
Thus_RB ,_, A1_NN sends_VBZ the_DT the_DT message_NN rebut_VB -LRB-_-LRB- c13_NN ,_, #_# 3_LS -RRB-_-RRB- to_TO A3_NN ._.
A3_NN incorporates_VBZ c13_NN into_IN its_PRP$ case_NN base_NN and_CC tries_VBZ to_TO solve_VB the_DT problem_NN P_NN again_RB ,_, now_RB taking_VBG c13_NN into_IN consideration_NN ._.
A3_NN comes_VBZ up_RP with_IN the_DT justified_JJ prediction_NN #_# 3_CD =_JJ A3_NN ,_, P_NN ,_, hadromerida_NN ,_, D4_NN ,_, and_CC broadcasts_VBZ it_PRP to_TO the_DT rest_NN of_IN the_DT agents_NNS with_IN the_DT message_NN assert_VB -LRB-_-LRB- #_# 3_LS -RRB-_-RRB- ._.
Thus_RB ,_, all_DT of_IN them_PRP know_VBP the_DT new_JJ H1_NN =_JJ #_# 1_CD ,_, #_# 2_CD ,_, #_# 3_CD ._.
Round_NNP #_# starts_VBZ and_CC A2_NN gets_VBZ the_DT token_JJ ._.
A2_NN tries_VBZ to_TO generate_VB counterarguments_NNS for_IN #_# 1_CD and_CC #_# 3_CD and_CC only_RB succeeds_VBZ to_TO generate_VB a_DT counterargument_NN #_# 2_CD =_JJ A2_NN ,_, P_NN ,_, astrophorida_NN ,_, D5_NN against_IN #_# 3_CD ._.
The_DT counterargument_NN is_VBZ sent_VBN to_TO A3_NN with_IN the_DT message_NN rebut_VB -LRB-_-LRB- #_# 2_CD ,_, #_# 3_LS -RRB-_-RRB- ._.
Agent_NNP A3_NNP receives_VBZ the_DT counterargument_NN and_CC assesses_VBZ its_PRP$ local_JJ confidence_NN ._.
The_DT result_NN is_VBZ that_IN the_DT individual_JJ confidence_NN of_IN the_DT counterargument_NN #_# 2_CD is_VBZ lower_JJR than_IN the_DT local_JJ confidence_NN of_IN #_# 3_CD ._.
Therefore_RB ,_, A3_NN does_VBZ not_RB accept_VB the_DT counterargument_NN ,_, and_CC thus_RB H2_NN =_JJ #_# 1_CD ,_, #_# 2_CD ,_, #_# 3_CD ._.
Round_NNP #_# starts_VBZ and_CC A3_NN gets_VBZ the_DT token_JJ ._.
A3_NN generates_VBZ a_DT counterargument_NN #_# 3_CD =_JJ A3_NN ,_, P_NN ,_, hadromerida_NN ,_, D6_NN for_IN #_# 2_CD and_CC sends_VBZ it_PRP to_TO A2_NN with_IN the_DT message_NN rebut_VB -LRB-_-LRB- #_# 3_CD ,_, #_# 2_LS -RRB-_-RRB- ._.
Agent_NNP A2_NN receives_VBZ the_DT counterargument_NN and_CC assesses_VBZ its_PRP$ local_JJ confidence_NN ._.
The_DT result_NN is_VBZ that_IN the_DT local_JJ confidence_NN of_IN the_DT counterargument_NN #_# 3_CD is_VBZ higher_JJR than_IN the_DT local_JJ confidence_NN of_IN #_# 2_CD ._.
Therefore_RB ,_, A2_NN accepts_VBZ the_DT counterargument_NN and_CC informs_VBZ the_DT rest_NN of_IN the_DT agents_NNS with_IN the_DT message_NN assert_VB -LRB-_-LRB- #_# 3_LS -RRB-_-RRB- ._.
After_IN that_DT ,_, H3_NN =_JJ #_# 1_CD ,_, #_# 3_CD ,_, #_# 3_CD ._.
At_IN Round_NNP #_# ,_, since_IN all_PDT the_DT agents_NNS agree_VBP -LRB-_-LRB- all_PDT the_DT justified_JJ predictions_NNS in_IN H3_NN predict_VBP hadromerida_NN as_IN the_DT solution_NN class_NN -RRB-_-RRB- The_DT protocol_NN ends_VBZ ,_, and_CC A1_NN -LRB-_-LRB- the_DT agent_NN that_WDT received_VBD the_DT problem_NN -RRB-_-RRB- considers_VBZ hadromerida_NN as_IN the_DT joint_JJ solution_NN for_IN the_DT problem_NN P_NN ._.
9_CD ._.
EXPERIMENTAL_JJ EVALUATION_NN 980_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- SPONGE_NNP 75_CD 77_CD 79_CD 81_CD 83_CD 85_CD 87_CD 89_CD 91_CD 2_CD #_# #_# #_# AMAL_NNP Voting_NNP Individual_NNP SOYBEAN_NNP 55_CD 60_CD 65_CD 70_CD 75_CD 80_CD 85_CD 90_CD 2_CD #_# #_# #_# AMAL_NNP Voting_NNP Individual_NNP Figure_NNP #_# :_: Individual_NNP and_CC joint_JJ accuracy_NN for_IN #_# to_TO #_# agents_NNS ._.
In_IN this_DT section_NN we_PRP empirically_RB evaluate_VBP the_DT AMAL_NN argumentation_NN framework_NN ._.
We_PRP have_VBP made_VBN experiments_NNS in_IN two_CD different_JJ data_NNS sets_NNS :_: soybean_NN -LRB-_-LRB- from_IN the_DT UCI_NNP machine_NN learning_VBG repository_NN -RRB-_-RRB- and_CC sponge_NN -LRB-_-LRB- a_DT relational_JJ data_NN set_NN -RRB-_-RRB- ._.
The_DT soybean_NN data_NN set_NN has_VBZ ###_CD examples_NNS and_CC ##_NN solution_NN classes_NNS ,_, while_IN the_DT sponge_NN data_NN set_NN has_VBZ ###_CD examples_NNS and_CC #_# solution_NN classes_NNS ._.
In_IN an_DT experimental_JJ run_NN ,_, the_DT data_NNS set_NN is_VBZ divided_VBN in_IN #_# sets_NNS :_: the_DT training_NN set_NN and_CC the_DT test_NN set_NN ._.
The_DT training_NN set_VBN examples_NNS are_VBP distributed_VBN among_IN #_# different_JJ agents_NNS without_IN replication_NN ,_, i_FW ._.
e_LS ._.
there_EX is_VBZ no_DT example_NN shared_VBN by_IN two_CD agents_NNS ._.
In_IN the_DT testing_NN stage_NN ,_, problems_NNS in_IN the_DT test_NN set_NN arrive_VBP randomly_RB to_TO one_CD of_IN the_DT agents_NNS ,_, and_CC their_PRP$ goal_NN is_VBZ to_TO predict_VB the_DT correct_JJ solution_NN ._.
The_DT experiments_NNS are_VBP designed_VBN to_TO test_VB two_CD hypotheses_NNS :_: -LRB-_-LRB- H1_NN -RRB-_-RRB- that_WDT argumentation_NN is_VBZ a_DT useful_JJ framework_NN for_IN joint_JJ deliberation_NN and_CC can_MD improve_VB over_IN other_JJ typical_JJ methods_NNS such_JJ as_IN voting_NN ;_: and_CC -LRB-_-LRB- H2_NN -RRB-_-RRB- that_IN learning_VBG from_IN communication_NN improves_VBZ the_DT individual_JJ performance_NN of_IN a_DT learning_NN agent_NN participating_VBG in_IN an_DT argumentation_NN process_NN ._.
Moreover_RB ,_, we_PRP also_RB expect_VBP that_IN the_DT improvement_NN achieved_VBN from_IN argumentation_NN will_MD increase_VB as_IN the_DT number_NN of_IN agents_NNS participating_VBG in_IN the_DT argumentation_NN increases_NNS -LRB-_-LRB- since_IN more_JJR information_NN will_MD be_VB taken_VBN into_IN account_NN -RRB-_-RRB- ._.
Concerning_VBG H1_NN -LRB-_-LRB- argumentation_NN is_VBZ a_DT useful_JJ framework_NN for_IN joint_JJ deliberation_NN -RRB-_-RRB- ,_, we_PRP ran_VBD #_# experiments_NNS ,_, using_VBG #_# ,_, #_# ,_, #_# ,_, and_CC #_# agents_NNS respectively_RB -LRB-_-LRB- in_IN all_DT experiments_NNS each_DT agent_NN has_VBZ a_DT ##_CD %_NN of_IN the_DT training_NN data_NNS ,_, since_IN the_DT training_NN is_VBZ always_RB distributed_VBN among_IN #_# agents_NNS -RRB-_-RRB- ._.
Figure_NNP #_# shows_VBZ the_DT result_NN of_IN those_DT experiments_NNS in_IN the_DT sponge_NN and_CC soybean_NN data_NNS sets_NNS ._.
Classification_NN accuracy_NN is_VBZ plotted_VBN in_IN the_DT vertical_JJ axis_NN ,_, and_CC in_IN the_DT horizontal_JJ axis_NN the_DT number_NN of_IN agents_NNS that_WDT took_VBD part_NN in_IN the_DT argumentation_NN processes_NNS is_VBZ shown_VBN ._.
For_IN each_DT number_NN of_IN agents_NNS ,_, three_CD bars_NNS are_VBP shown_VBN :_: individual_JJ ,_, Voting_NNP ,_, and_CC AMAL_NNP ._.
The_DT individual_JJ bar_NN shows_VBZ the_DT average_JJ accuracy_NN of_IN individual_JJ agents_NNS predictions_NNS ;_: the_DT voting_NN bar_NN shows_VBZ the_DT average_JJ accuracy_NN of_IN the_DT joint_JJ prediction_NN achieved_VBN by_IN voting_VBG but_CC without_IN any_DT argumentation_NN ;_: and_CC finally_RB the_DT AMAL_NNP bar_NN shows_VBZ the_DT average_JJ accuracy_NN of_IN the_DT joint_JJ prediction_NN using_VBG argumentation_NN ._.
The_DT results_NNS shown_VBN are_VBP the_DT average_NN of_IN 5_CD 10-fold_RB cross_VB validation_NN runs_NNS ._.
Figure_NNP #_# shows_VBZ that_IN collaboration_NN -LRB-_-LRB- voting_NN and_CC AMAL_NN -RRB-_-RRB- outperforms_VBZ individual_JJ problem_NN solving_VBG ._.
Moreover_RB ,_, as_IN we_PRP expected_VBD ,_, the_DT accuracy_NN improves_VBZ as_IN more_JJR agents_NNS collaborate_VBP ,_, since_IN more_JJR information_NN is_VBZ taken_VBN into_IN account_NN ._.
We_PRP can_MD also_RB see_VB that_IN AMAL_NNP always_RB outperforms_VBZ standard_JJ voting_NN ,_, proving_VBG that_IN joint_JJ decisions_NNS are_VBP based_VBN on_IN better_JJR information_NN as_IN provided_VBN by_IN the_DT argumentation_NN process_NN ._.
For_IN instance_NN ,_, the_DT joint_JJ accuracy_NN for_IN #_# agents_NNS in_IN the_DT sponge_NN data_NN set_NN is_VBZ of_IN ##_NN ._.
##_CD %_NN for_IN AMAL_NNP and_CC ##_NNP ._.
##_CD %_NN for_IN voting_NN -LRB-_-LRB- while_IN individual_JJ accuracy_NN is_VBZ just_RB ##_RB ._.
##_CD %_NN -RRB-_-RRB- ._.
Moreover_RB ,_, the_DT improvement_NN achieved_VBN by_IN AMAL_NN over_IN Voting_NNP is_VBZ even_RB larger_JJR in_IN the_DT soybean_NN data_NN set_NN ._.
The_DT reason_NN is_VBZ that_IN the_DT soybean_NN data_NN set_NN is_VBZ more_RBR difficult_JJ -LRB-_-LRB- in_IN the_DT sense_NN that_WDT agents_NNS need_VBP more_JJR data_NNS to_TO produce_VB good_JJ predictions_NNS -RRB-_-RRB- ._.
These_DT experimental_JJ results_NNS show_VBP that_IN AMAL_NNP effectively_RB exploits_VBZ the_DT opportunity_NN for_IN improvement_NN :_: the_DT accuracy_NN is_VBZ higher_JJR only_RB because_IN more_JJR agents_NNS have_VBP changed_VBN their_PRP$ opinion_NN during_IN argumentation_NN -LRB-_-LRB- otherwise_RB they_PRP would_MD achieve_VB the_DT same_JJ result_NN as_IN Voting_NNP -RRB-_-RRB- ._.
Concerning_VBG H2_NN -LRB-_-LRB- learning_NN from_IN communication_NN in_IN argumentation_NN processes_NNS improves_VBZ individual_JJ prediction_NN -RRB-_-RRB- ,_, we_PRP ran_VBD the_DT following_VBG experiment_NN :_: initially_RB ,_, we_PRP distributed_VBD a_DT ##_CD %_NN of_IN the_DT training_NN set_NN among_IN the_DT five_CD agents_NNS ;_: after_IN that_DT ,_, the_DT rest_NN of_IN the_DT cases_NNS in_IN the_DT training_NN set_NN is_VBZ sent_VBN to_TO the_DT agents_NNS one_CD by_IN one_CD ;_: when_WRB an_DT agent_NN receives_VBZ a_DT new_JJ training_NN case_NN ,_, it_PRP has_VBZ several_JJ options_NNS :_: the_DT agent_NN can_MD discard_VB it_PRP ,_, the_DT agent_NN can_MD retain_VB it_PRP ,_, or_CC the_DT agent_NN can_MD use_VB it_PRP for_IN engaging_VBG an_DT argumentation_NN process_NN ._.
Figure_NNP #_# shows_VBZ the_DT result_NN of_IN that_DT experiment_NN for_IN the_DT two_CD data_NNS sets_NNS ._.
Figure_NNP #_# contains_VBZ three_CD plots_NNS ,_, where_WRB NL_NNP -LRB-_-LRB- not_RB learning_VBG -RRB-_-RRB- shows_VBZ accuracy_NN of_IN an_DT agent_NN with_IN no_DT learning_NN at_IN all_DT ;_: L_NN -LRB-_-LRB- learning_NN -RRB-_-RRB- ,_, shows_VBZ the_DT evolution_NN of_IN the_DT individual_JJ classification_NN accuracy_NN when_WRB agents_NNS learn_VBP by_IN retaining_VBG the_DT training_NN cases_NNS they_PRP individually_RB receive_VBP -LRB-_-LRB- notice_NN that_WDT when_WRB all_PDT the_DT training_NN cases_NNS have_VBP been_VBN retained_VBN at_IN 100_CD %_NN ,_, the_DT accuracy_NN should_MD be_VB equal_JJ to_TO that_DT of_IN Figure_NNP #_# for_IN individual_JJ agents_NNS -RRB-_-RRB- ;_: and_CC finally_RB LFC_NN -LRB-_-LRB- learning_NN from_IN communication_NN -RRB-_-RRB- shows_VBZ the_DT evolution_NN of_IN the_DT individual_JJ classification_NN accuracy_NN of_IN learning_VBG agents_NNS that_WDT also_RB learn_VBP by_IN retaining_VBG those_DT counterexamples_NNS received_VBN during_IN argumentation_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
they_PRP learn_VBP both_DT from_IN training_NN examples_NNS and_CC counterexamples_NNS -RRB-_-RRB- ._.
Figure_NNP #_# shows_VBZ that_IN if_IN an_DT agent_NN Ai_NNP learns_VBZ also_RB from_IN communication_NN ,_, Ai_NNP can_MD significantly_RB improve_VB its_PRP$ individual_JJ performance_NN with_IN just_RB a_DT small_JJ number_NN of_IN additional_JJ cases_NNS -LRB-_-LRB- those_DT selected_VBN as_IN relevant_JJ counterexamples_NNS for_IN Ai_NNP during_IN argumentation_NN -RRB-_-RRB- ._.
For_IN instance_NN ,_, in_IN the_DT soybean_NN data_NN set_NN ,_, individual_JJ agents_NNS have_VBP achieved_VBN an_DT accuracy_NN of_IN ##_NN ._.
##_CD %_NN when_WRB they_PRP also_RB learn_VBP from_IN communication_NN versus_CC an_DT accuracy_NN of_IN ##_NN ._.
##_CD %_NN when_WRB they_PRP only_RB learn_VBP from_IN their_PRP$ individual_JJ experience_NN ._.
The_DT number_NN of_IN cases_NNS learnt_VBN from_IN communication_NN depends_VBZ on_IN the_DT properties_NNS of_IN the_DT data_NNS set_NN :_: in_IN the_DT sponges_NNS data_NNS set_VBN ,_, agents_NNS have_VBP retained_VBN only_RB very_RB few_JJ additional_JJ cases_NNS ,_, and_CC significantly_RB improved_VBD individual_JJ accuracy_NN ;_: namely_RB they_PRP retain_VBP ##_NN ._.
##_NN cases_NNS in_IN average_NN -LRB-_-LRB- compared_VBN to_TO the_DT ##_NN ._.
#_# cases_NNS retained_VBN if_IN they_PRP do_VBP not_RB learn_VB from_IN communication_NN -RRB-_-RRB- ._.
In_IN the_DT soybean_NN data_NNS set_VBD more_RBR counterexamples_NNS are_VBP learnt_VBN to_TO significantly_RB improve_VB individual_JJ accuracy_NN ,_, namely_RB they_PRP retain_VBP ##_NN ._.
##_NN cases_NNS in_IN average_NN -LRB-_-LRB- compared_VBN to_TO ##_VB ._.
##_NN cases_NNS retained_VBN if_IN they_PRP do_VBP not_RB learn_VB from_IN communication_NN -RRB-_-RRB- ._.
Finally_RB ,_, the_DT fact_NN that_IN both_CC data_NNS sets_NNS show_VBP a_DT significant_JJ improvement_NN points_VBZ out_RP the_DT adaptive_JJ nature_NN of_IN the_DT argumentation-based_JJ approach_NN to_TO learning_VBG from_IN communication_NN :_: the_DT useful_JJ cases_NNS are_VBP selected_VBN as_IN counterexamples_NNS -LRB-_-LRB- and_CC no_DT more_JJR than_IN those_DT needed_VBN -RRB-_-RRB- ,_, and_CC they_PRP have_VBP the_DT intended_JJ effect_NN ._.
10_CD ._.
RELATED_JJ WORK_VBP Concerning_VBG CBR_NNP in_IN a_DT multi-agent_JJ setting_NN ,_, the_DT first_JJ research_NN was_VBD on_IN negotiated_VBN case_NN retrieval_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- among_IN groups_NNS of_IN agents_NNS ._.
Our_PRP$ work_NN on_IN multi-agent_JJ case-based_JJ learning_NN started_VBD in_IN ####_CD -LSB-_-LRB- #_# -RSB-_-RRB- ;_: later_RB Mc_NN Ginty_NNP and_CC Smyth_NNP -LSB-_-LRB- #_# -RSB-_-RRB- presented_VBD a_DT multi-agent_JJ collaborative_JJ CBR_NNP approach_NN -LRB-_-LRB- CCBR_NN -RRB-_-RRB- for_IN planning_NN ._.
Finally_RB ,_, another_DT interesting_JJ approach_NN is_VBZ multi-case-base_JJ reasoning_NN -LRB-_-LRB- MCBR_NN -RRB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- ,_, that_WDT deals_VBZ with_IN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD SPONGE_NNP 60_CD 65_CD 70_CD 75_CD 80_CD 85_CD 25_CD %_NN ##_CD %_NN ##_CD %_NN ##_CD %_NN ##_CD %_NN ###_CD %_NN LFC_NN L_NN NL_NNP SOYBEAN_NNP 20_CD 30_CD 40_CD 50_CD 60_CD 70_CD 80_CD 90_CD 25_CD %_NN ##_CD %_NN ##_CD %_NN ##_CD %_NN ##_CD %_NN ###_CD %_NN LFC_NN L_NN NL_NNP Figure_NNP #_# :_: Learning_NNP from_IN communication_NN resulting_VBG from_IN argumentation_NN in_IN a_DT system_NN composed_VBN of_IN #_# agents_NNS ._.
distributed_VBN systems_NNS where_WRB there_EX are_VBP several_JJ case_NN bases_NNS available_JJ for_IN the_DT same_JJ task_NN and_CC addresses_NNS the_DT problems_NNS of_IN cross-case_NN base_NN adaptation_NN ._.
The_DT main_JJ difference_NN is_VBZ that_IN our_PRP$ MAC_NNP approach_NN is_VBZ a_DT way_NN to_TO distribute_VB the_DT Reuse_NN process_NN of_IN CBR_NNP -LRB-_-LRB- using_VBG a_DT voting_NN system_NN -RRB-_-RRB- while_IN Retrieve_NN is_VBZ performed_VBN individually_RB by_IN each_DT agent_NN ;_: the_DT other_JJ multiagent_JJ CBR_NN approaches_NNS ,_, however_RB ,_, focus_NN on_IN distributing_VBG the_DT Retrieve_NNP process_NN ._.
Research_NNP on_IN MAS_NNP argumentation_NN focus_NN on_IN several_JJ issues_NNS like_IN a_DT -RRB-_-RRB- logics_NNS ,_, protocols_NNS and_CC languages_NNS that_WDT support_VBP argumentation_NN ,_, b_NN -RRB-_-RRB- argument_NN selection_NN and_CC c_NN -RRB-_-RRB- argument_NN interpretation_NN ._.
Approaches_NNS for_IN logic_NN and_CC languages_NNS that_WDT support_VBP argumentation_NN include_VBP defeasible_JJ logic_NN -LSB-_-LRB- #_# -RSB-_-RRB- and_CC BDI_NNP models_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Although_IN argument_NN selection_NN is_VBZ a_DT key_JJ aspect_NN of_IN automated_VBN argumentation_NN ,_, most_JJS research_NN has_VBZ been_VBN focused_VBN on_IN preference_NN relations_NNS among_IN arguments_NNS ._.
In_IN our_PRP$ framework_NN we_PRP have_VBP addressed_VBN both_DT argument_NN selection_NN and_CC preference_NN relations_NNS using_VBG a_DT case-based_JJ approach_NN ._.
11_CD ._.
CONCLUSIONS_NNS AND_CC FUTURE_NNS WORK_VBP In_IN this_DT paper_NN we_PRP have_VBP presented_VBN an_DT argumentation-based_JJ framework_NN for_IN multi-agent_JJ learning_NN ._.
Specifically_RB ,_, we_PRP have_VBP presented_VBN AMAL_NNP ,_, a_DT framework_NN that_WDT allows_VBZ a_DT group_NN of_IN learning_VBG agents_NNS to_TO argue_VB about_IN the_DT solution_NN of_IN a_DT given_VBN problem_NN and_CC we_PRP have_VBP shown_VBN how_WRB the_DT learning_NN capabilities_NNS can_MD be_VB used_VBN to_TO generate_VB arguments_NNS and_CC counterarguments_NNS ._.
The_DT experimental_JJ evaluation_NN shows_VBZ that_IN the_DT increased_VBN amount_NN of_IN information_NN provided_VBN to_TO the_DT agents_NNS by_IN the_DT argumentation_NN process_NN increases_VBZ their_PRP$ predictive_JJ accuracy_NN ,_, and_CC specially_RB when_WRB an_DT adequate_JJ number_NN of_IN agents_NNS take_VBP part_NN in_IN the_DT argumentation_NN ._.
The_DT main_JJ contributions_NNS of_IN this_DT work_NN are_VBP :_: a_DT -RRB-_-RRB- an_DT argumentation_NN framework_NN for_IN learning_VBG agents_NNS ;_: b_LS -RRB-_-RRB- a_DT case-based_JJ preference_NN relation_NN over_IN arguments_NNS ,_, based_VBN on_IN computing_VBG an_DT overall_JJ confidence_NN estimation_NN of_IN arguments_NNS ;_: c_NN -RRB-_-RRB- a_DT case-based_JJ policy_NN to_TO generate_VB counterarguments_NNS and_CC select_JJ counterexamples_NNS ;_: and_CC d_NN -RRB-_-RRB- an_DT argumentation-based_JJ approach_NN for_IN learning_VBG from_IN communication_NN ._.
Finally_RB ,_, in_IN the_DT experiments_NNS presented_VBN here_RB a_DT learning_VBG agent_NN would_MD retain_VB all_DT counterexamples_NNS submitted_VBN by_IN the_DT other_JJ agent_NN ;_: however_RB ,_, this_DT is_VBZ a_DT very_RB simple_JJ case_NN retention_NN policy_NN ,_, and_CC we_PRP will_MD like_VB to_TO experiment_NN with_IN more_JJR informed_JJ policies_NNS -_: with_IN the_DT goal_NN that_WDT individual_JJ learning_VBG agents_NNS could_MD significantly_RB improve_VB using_VBG only_RB a_DT small_JJ set_NN of_IN cases_NNS proposed_VBN by_IN other_JJ agents_NNS ._.
Finally_RB ,_, our_PRP$ approach_NN is_VBZ focused_VBN on_IN lazy_JJ learning_NN ,_, and_CC future_JJ works_NNS aims_VBZ at_IN incorporating_VBG eager_JJ inductive_JJ learning_NN inside_IN the_DT argumentative_JJ framework_NN for_IN learning_VBG from_IN communication_NN ._.
12_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- Agnar_NNP Aamodt_NNP and_CC Enric_NNP Plaza_NNP ._.
Case-based_JJ reasoning_NN :_: Foundational_JJ issues_NNS ,_, methodological_JJ variations_NNS ,_, and_CC system_NN approaches_NNS ._.
Artificial_JJ Intelligence_NNP Communications_NNPS ,_, 7_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 39-59_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- E_NN ._.
Armengol_NNP and_CC E_NNP ._.
Plaza_NNP ._.
Lazy_JJ induction_NN of_IN descriptions_NNS for_IN relational_JJ case-based_JJ learning_NN ._.
In_IN ECML_NN ''_'' ####_CD ,_, pages_NNS 13-24_CD ,_, 2001_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Gerhard_NNP Brewka_NNP ._.
Dynamic_NNP argument_NN systems_NNS :_: A_DT formal_JJ model_NN of_IN argumentation_NN processes_NNS based_VBN on_IN situation_NN calculus_NN ._.
Journal_NNP of_IN Logic_NNP and_CC Computation_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 257-282_CD ,_, 2001_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Carlos_NNP I_PRP ._.
Chesevar_NNP and_CC Guillermo_NNP R_NNP ._.
Simari_NNP ._.
Formalizing_VBG Defeasible_JJ Argumentation_NN using_VBG Labelled_NNP Deductive_NNP Systems_NNPS ._.
Journal_NNP of_IN Computer_NNP Science_NNP &_CC Technology_NNP ,_, 1_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 18-33_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Leake_NNP and_CC R_NN ._.
Sooriamurthi_NNP ._.
Automatically_RB selecting_VBG strategies_NNS for_IN multi-case-base_JJ reasoning_NN ._.
In_IN S_NN ._.
Craw_NN and_CC A_NN ._.
Preece_NNP ,_, editors_NNS ,_, ECCBR_NN ''_'' ####_CD ,_, pages_NNS 204-219_CD ,_, Berlin_NNP ,_, 2002_CD ._.
Springer_NNP Verlag_NNP ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Francisco_NNP J_NNP ._.
Martn_NNP ,_, Enric_NNP Plaza_NNP ,_, and_CC Josep-Lluis_NNP Arcos_NNP ._.
Knowledge_NN and_CC experience_NN reuse_NN through_IN communications_NNS among_IN competent_JJ -LRB-_-LRB- peer_VBP -RRB-_-RRB- agents_NNS ._.
International_NNP Journal_NNP of_IN Software_NNP Engineering_NNP and_CC Knowledge_NNP Engineering_NNP ,_, 9_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 319-341_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Lorraine_NNP McGinty_NNP and_CC Barry_NNP Smyth_NNP ._.
Collaborative_JJ case-based_JJ reasoning_NN :_: Applications_NNS in_IN personalized_JJ route_NN planning_NN ._.
In_IN I_PRP ._.
Watson_NNP and_CC Q_NNP ._.
Yang_NNP ,_, editors_NNS ,_, ICCBR_NN ,_, number_NN 2080_CD in_IN LNAI_NNP ,_, pages_NNS 362-376_CD ._.
Springer-Verlag_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Santi_NNP Ontan_NNP and_CC Enric_NNP Plaza_NNP ._.
Justification-based_JJ multiagent_JJ learning_NN ._.
In_IN ICML_NN ''_'' ####_CD ,_, pages_NNS 576-583_CD ._.
Morgan_NNP Kaufmann_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Enric_NNP Plaza_NNP ,_, Eva_NNP Armengol_NNP ,_, and_CC Santiago_NNP Ontan_NNP ._.
The_DT explanatory_JJ power_NN of_IN symbolic_JJ similarity_NN in_IN case-based_JJ reasoning_NN ._.
Artificial_JJ Intelligence_NNP Review_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 145-161_CD ,_, 2005_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- David_NNP Poole_NNP ._.
On_IN the_DT comparison_NN of_IN theories_NNS :_: Preferring_VBG the_DT most_RBS specific_JJ explanation_NN ._.
In_IN IJCAI-85_NN ,_, pages_NNS 144-147_CD ,_, 1985_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- M_NN V_NN Nagendra_NNP Prassad_NNP ,_, Victor_NNP R_NNP Lesser_NNP ,_, and_CC Susan_NNP Lander_NNP ._.
Retrieval_NN and_CC reasoning_NN in_IN distributed_VBN case_NN bases_NNS ._.
Technical_NNP report_NN ,_, UMass_NNP Computer_NNP Science_NNP Department_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- K_NN ._.
Sycara_NNP S_NN ._.
Kraus_NNP and_CC A_NNP ._.
Evenchik_NNP ._.
Reaching_VBG agreements_NNS through_IN argumentation_NN :_: a_DT logical_JJ model_NN and_CC implementation_NN ._.
Artificial_JJ Intelligence_NNP Journal_NNP ,_, ###_CD :_: 1-69_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- N_NN ._.
R_NN ._.
Jennings_NNP S_NN ._.
Parsons_NNP ,_, C_NNP ._.
Sierra_NNP ._.
Agents_NNS that_WDT reason_NN and_CC negotiate_VB by_IN arguing_VBG ._.
Journal_NNP of_IN Logic_NNP and_CC Computation_NNP ,_, 8_CD :_: 261-292_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- Bruce_NNP A_NNP ._.
Wooley_NNP ._.
Explanation_NN component_NN of_IN software_NN systems_NNS ._.
ACM_NNP CrossRoads_NNPS ,_, #_# ._.
#_# ,_, ####_CD ._.
982_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB-
