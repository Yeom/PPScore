Approximate_JJ and_CC Online_NNP Multi-Issue_NNP Negotiation_NNP Shaheen_NNP S_NN ._.
Fatima_NNP Department_NNP of_IN Computer_NNP Science_NNP University_NNP of_IN Liverpool_NNP Liverpool_NNP L69_NN 3BX_NN ,_, UK_NNP ._.
shaheen_NN @_SYM csc_NN ._.
liv_NN ._.
ac_NN ._.
uk_NN Michael_NNP Wooldridge_NNP Department_NNP of_IN Computer_NNP Science_NNP University_NNP of_IN Liverpool_NNP Liverpool_NNP L69_NN 3BX_NN ,_, UK_NNP ._.
mjw_NN @_SYM csc_NN ._.
liv_NN ._.
ac_NN ._.
uk_NN Nicholas_NNS R_NN ._.
Jennings_NNP School_NNP of_IN Electronics_NNP and_CC Computer_NNP Science_NNP University_NNP of_IN Southampton_NNP Southampton_NNP SO17_NN 1BJ_NN ,_, UK_NNP ._.
nrj_NN @_SYM ecs_NNS ._.
soton_NN ._.
ac_NN ._.
uk_NN ABSTRACT_NN This_DT paper_NN analyzes_VBZ bilateral_JJ multi-issue_JJ negotiation_NN between_IN selfinterested_JJ autonomous_JJ agents_NNS ._.
The_DT agents_NNS have_VBP time_NN constraints_NNS in_IN the_DT form_NN of_IN both_CC deadlines_NNS and_CC discount_NN factors_NNS ._.
There_EX are_VBP m_NN >_JJR #_# issues_NNS for_IN negotiation_NN where_WRB each_DT issue_NN is_VBZ viewed_VBN as_IN a_DT pie_NN of_IN size_NN one_CD ._.
The_DT issues_NNS are_VBP indivisible_JJ -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, individual_JJ issues_NNS can_MD not_RB be_VB split_VBN between_IN the_DT parties_NNS ;_: each_DT issue_NN must_MD be_VB allocated_VBN in_IN its_PRP$ entirety_NN to_TO either_CC agent_NN -RRB-_-RRB- ._.
Here_RB different_JJ agents_NNS value_NN different_JJ issues_NNS differently_RB ._.
Thus_RB ,_, the_DT problem_NN is_VBZ for_IN the_DT agents_NNS to_TO decide_VB how_WRB to_TO allocate_VB the_DT issues_NNS between_IN themselves_PRP so_RB as_IN to_TO maximize_VB their_PRP$ individual_JJ utilities_NNS ._.
For_IN such_JJ negotiations_NNS ,_, we_PRP first_RB obtain_VB the_DT equilibrium_NN strategies_NNS for_IN the_DT case_NN where_WRB the_DT issues_NNS for_IN negotiation_NN are_VBP known_VBN a_DT priori_FW to_TO the_DT parties_NNS ._.
Then_RB ,_, we_PRP analyse_VBP their_PRP$ time_NN complexity_NN and_CC show_VBP that_IN finding_VBG the_DT equilibrium_NN offers_VBZ is_VBZ an_DT NP-hard_JJ problem_NN ,_, even_RB in_IN a_DT complete_JJ information_NN setting_NN ._.
In_IN order_NN to_TO overcome_VB this_DT computational_JJ complexity_NN ,_, we_PRP then_RB present_JJ negotiation_NN strategies_NNS that_WDT are_VBP approximately_RB optimal_JJ but_CC computationally_RB efficient_JJ ,_, and_CC show_VBP that_IN they_PRP form_VBP an_DT equilibrium_NN ._.
We_PRP also_RB analyze_VBP the_DT relative_JJ error_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT difference_NN between_IN the_DT true_JJ optimum_NN and_CC the_DT approximate_JJ -RRB-_-RRB- ._.
The_DT time_NN complexity_NN of_IN the_DT approximate_JJ equilibrium_NN strategies_NNS is_VBZ O_NN -LRB-_-LRB- nm_NN /_: #_# -RRB-_-RRB- where_WRB n_NN is_VBZ the_DT negotiation_NN deadline_NN and_CC the_DT relative_JJ error_NN ._.
Finally_RB ,_, we_PRP extend_VBP the_DT analysis_NN to_TO online_JJ negotiation_NN where_WRB different_JJ issues_NNS become_VBP available_JJ at_IN different_JJ time_NN points_NNS and_CC the_DT agents_NNS are_VBP uncertain_JJ about_IN their_PRP$ valuations_NNS for_IN these_DT issues_NNS ._.
Specifically_RB ,_, we_PRP show_VBP that_IN an_DT approximate_JJ equilibrium_NN exists_VBZ for_IN online_JJ negotiation_NN and_CC show_VBP that_IN the_DT expected_VBN difference_NN between_IN the_DT optimum_NN and_CC the_DT approximate_JJ is_VBZ O_NN -LRB-_-LRB- m_NN -RRB-_-RRB- ._.
These_DT approximate_JJ strategies_NNS also_RB have_VBP polynomial_JJ time_NN complexity_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNS I_PRP ._.
#_# ._.
##_NN -LSB-_-LRB- Distributed_VBN Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Multiagent_NNP Systems_NNP General_NNP Terms_NNS Algorithms_NNS ,_, Design_NN ,_, Theory_NNP 1_CD ._.
INTRODUCTION_NN Negotiation_NN is_VBZ a_DT key_JJ form_NN of_IN interaction_NN in_IN multiagent_JJ systems_NNS ._.
It_PRP is_VBZ a_DT process_NN in_IN which_WDT disputing_JJ agents_NNS decide_VBP how_WRB to_TO divide_VB the_DT gains_NNS from_IN cooperation_NN ._.
Since_IN this_DT decision_NN is_VBZ made_VBN jointly_RB by_IN the_DT agents_NNS themselves_PRP -LSB-_-LRB- ##_CD ,_, ##_CD ,_, ##_CD ,_, ##_CD -RSB-_-RRB- ,_, each_DT party_NN can_MD only_RB obtain_VB what_WP the_DT other_JJ is_VBZ prepared_VBN to_TO allow_VB them_PRP ._.
Now_RB ,_, the_DT simplest_JJS form_NN of_IN negotiation_NN involves_VBZ two_CD agents_NNS and_CC a_DT single_JJ issue_NN ._.
For_IN example_NN ,_, consider_VB a_DT scenario_NN in_IN which_WDT a_DT buyer_NN and_CC a_DT seller_NN negotiate_VB on_IN the_DT price_NN of_IN a_DT good_JJ ._.
To_TO begin_VB ,_, the_DT two_CD agents_NNS are_VBP likely_JJ to_TO differ_VB on_IN the_DT price_NN at_IN which_WDT they_PRP believe_VBP the_DT trade_NN should_MD take_VB place_NN ,_, but_CC through_IN a_DT process_NN of_IN joint_JJ decision-making_NN they_PRP either_RB arrive_VBP at_IN a_DT price_NN that_WDT is_VBZ mutually_RB acceptable_JJ or_CC they_PRP fail_VBP to_TO reach_VB an_DT agreement_NN ._.
Since_IN agents_NNS are_VBP likely_JJ to_TO begin_VB with_IN different_JJ prices_NNS ,_, one_CD or_CC both_DT of_IN them_PRP must_MD move_VB toward_IN the_DT other_JJ ,_, through_IN a_DT series_NN of_IN offers_NNS and_CC counter_JJ offers_NNS ,_, in_IN order_NN to_TO obtain_VB a_DT mutually_RB acceptable_JJ outcome_NN ._.
However_RB ,_, before_IN the_DT agents_NNS can_MD actually_RB perform_VB such_JJ negotiations_NNS ,_, they_PRP must_MD decide_VB the_DT rules_NNS for_IN making_VBG offers_NNS and_CC counter_JJ offers_NNS ._.
That_DT is_VBZ ,_, they_PRP must_MD set_VB the_DT negotiation_NN protocol_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
On_IN the_DT basis_NN of_IN this_DT protocol_NN ,_, each_DT agent_NN chooses_VBZ its_PRP$ strategy_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, what_WDT offers_VBZ it_PRP should_MD make_VB during_IN the_DT course_NN of_IN negotiation_NN -RRB-_-RRB- ._.
Given_VBN this_DT context_NN ,_, this_DT work_NN focuses_VBZ on_IN competitive_JJ scenarios_NNS with_IN self-interested_JJ agents_NNS ._.
For_IN such_JJ cases_NNS ,_, each_DT participant_NN defines_VBZ its_PRP$ strategy_NN so_RB as_IN to_TO maximise_VB its_PRP$ individual_JJ utility_NN ._.
However_RB ,_, in_IN most_JJS bilateral_JJ negotiations_NNS ,_, the_DT parties_NNS involved_VBN need_VBP to_TO settle_VB more_JJR than_IN one_CD issue_NN ._.
For_IN this_DT case_NN ,_, the_DT issues_NNS may_MD be_VB divisible_JJ or_CC indivisible_JJ -LSB-_-LRB- #_# -RSB-_-RRB- ._.
For_IN the_DT former_JJ ,_, the_DT problem_NN for_IN the_DT agents_NNS is_VBZ to_TO decide_VB how_WRB to_TO split_VB each_DT issue_NN between_IN themselves_PRP -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
For_IN the_DT latter_JJ ,_, the_DT individual_JJ issues_NNS can_MD not_RB be_VB divided_VBN ._.
An_DT issue_NN ,_, in_IN its_PRP$ entirety_NN ,_, must_MD be_VB allocated_VBN to_TO either_DT of_IN the_DT two_CD agents_NNS ._.
Since_IN the_DT agents_NNS value_NN different_JJ issues_NNS differently_RB ,_, they_PRP must_MD come_VB to_TO terms_NNS about_IN who_WP will_MD take_VB which_WDT issue_NN ._.
To_TO date_NN ,_, most_JJS of_IN the_DT existing_VBG work_NN on_IN multi-issue_JJ negotiation_NN has_VBZ focussed_VBN on_IN the_DT former_JJ case_NN -LSB-_-LRB- #_# ,_, #_# ,_, #_# ,_, ##_NN ,_, ##_NN ,_, #_# -RSB-_-RRB- ._.
However_RB ,_, in_IN many_JJ real-world_JJ settings_NNS ,_, the_DT issues_NNS are_VBP indivisible_JJ ._.
Hence_RB ,_, our_PRP$ focus_NN here_RB is_VBZ on_IN negotiation_NN for_IN indivisible_JJ issues_NNS ._.
Such_JJ negotiations_NNS are_VBP very_RB common_JJ in_IN multiagent_JJ systems_NNS ._.
For_IN example_NN ,_, consider_VB the_DT case_NN of_IN task_NN allocation_NN between_IN two_CD agents_NNS ._.
There_EX is_VBZ a_DT set_NN of_IN tasks_NNS to_TO be_VB carried_VBN out_RP and_CC different_JJ agents_NNS have_VBP different_JJ preferences_NNS for_IN the_DT tasks_NNS ._.
The_DT tasks_NNS can_MD not_RB be_VB partitioned_VBN ;_: a_DT task_NN must_MD be_VB carried_VBN out_RP by_IN one_CD agent_NN ._.
The_DT problem_NN then_RB is_VBZ for_IN the_DT agents_NNS to_TO negotiate_VB about_IN who_WP will_MD carry_VB out_RP which_WDT task_NN ._.
A_DT key_JJ problem_NN in_IN the_DT study_NN of_IN multi-issue_JJ negotiation_NN is_VBZ to_TO determine_VB the_DT equilibrium_NN strategies_NNS ._.
An_DT equally_RB important_JJ problem_NN ,_, especially_RB in_IN the_DT context_NN of_IN software_NN agents_NNS ,_, is_VBZ to_TO find_VB the_DT time_NN complexity_NN of_IN computing_VBG the_DT equilibrium_NN offers_VBZ ._.
However_RB ,_, such_JJ computational_JJ issues_NNS have_VBP so_RB far_RB received_VBN little_JJ attention_NN ._.
As_IN we_PRP will_MD show_VB ,_, this_DT is_VBZ mainly_RB due_JJ to_TO the_DT fact_NN that_IN existing_VBG work_NN -LRB-_-LRB- describe_VBP in_IN Section_NN #_# -RRB-_-RRB- has_VBZ mostly_RB focused_VBN on_IN negotiation_NN for_IN divisible_JJ issues_NNS 951_CD 978-81-904262-7-5_CD -LRB-_-LRB- RPS_NN -RRB-_-RRB- c_NN ####_CD IFAAMAS_NNP and_CC finding_VBG the_DT equilibrium_NN for_IN this_DT case_NN is_VBZ computationally_RB easier_JJR than_IN that_DT for_IN the_DT case_NN of_IN indivisible_JJ issues_NNS ._.
Our_PRP$ primary_JJ objective_NN is_VBZ ,_, therefore_RB ,_, to_TO answer_VB the_DT computational_JJ questions_NNS for_IN the_DT latter_JJ case_NN for_IN the_DT types_NNS of_IN situations_NNS that_WDT are_VBP commonly_RB faced_VBN by_IN agents_NNS in_IN real-world_JJ contexts_NNS ._.
Thus_RB ,_, we_PRP consider_VBP negotiations_NNS in_IN which_WDT there_EX is_VBZ incomplete_JJ information_NN and_CC time_NN constraints_NNS ._.
Incompleteness_NN of_IN information_NN on_IN the_DT part_NN of_IN negotiators_NNS is_VBZ a_DT common_JJ feature_NN of_IN most_JJS practical_JJ negotiations_NNS ._.
Also_RB ,_, agents_NNS typically_RB have_VBP time_NN constraints_NNS in_IN the_DT form_NN of_IN both_CC deadlines_NNS and_CC discount_NN factors_NNS ._.
Deadlines_NNS are_VBP an_DT essential_JJ element_NN since_IN negotiation_NN can_MD not_RB go_VB on_IN indefinitely_RB ,_, rather_RB it_PRP must_MD end_VB within_IN a_DT reasonable_JJ time_NN limit_NN ._.
Likewise_RB ,_, discount_NN factors_NNS are_VBP essential_JJ since_IN the_DT goods_NNS may_MD be_VB perishable_JJ or_CC their_PRP$ value_NN may_MD decline_VB due_JJ to_TO inflation_NN ._.
Moreover_RB ,_, the_DT strategic_JJ behaviour_NN of_IN agents_NNS with_IN deadlines_NNS and_CC discount_NN factors_NNS differs_VBZ from_IN those_DT without_IN -LRB-_-LRB- see_VB -LSB-_-LRB- ##_CD -RSB-_-RRB- for_IN single_JJ issue_NN bargaining_NN without_IN deadlines_NNS and_CC -LSB-_-LRB- ##_CD ,_, 13_CD -RSB-_-RRB- for_IN bargaining_NN with_IN deadlines_NNS and_CC discount_NN factors_NNS in_IN the_DT context_NN of_IN divisible_JJ issues_NNS -RRB-_-RRB- ._.
Given_VBN this_DT ,_, we_PRP consider_VBP indivisible_JJ issues_NNS and_CC first_RB analyze_VB the_DT strategic_JJ behaviour_NN of_IN agents_NNS to_TO obtain_VB the_DT equilibrium_NN strategies_NNS for_IN the_DT case_NN where_WRB all_PDT the_DT issues_NNS for_IN negotiation_NN are_VBP known_VBN a_DT priori_FW to_TO both_DT agents_NNS ._.
For_IN this_DT case_NN ,_, we_PRP show_VBP that_IN the_DT problem_NN of_IN finding_VBG the_DT equilibrium_NN offers_VBZ is_VBZ NP-hard_NN ,_, even_RB in_IN a_DT complete_JJ information_NN setting_NN ._.
Then_RB ,_, in_IN order_NN to_TO overcome_VB the_DT problem_NN of_IN time_NN complexity_NN ,_, we_PRP present_VBP strategies_NNS that_WDT are_VBP approximately_RB optimal_JJ but_CC computationally_RB efficient_JJ ,_, and_CC show_VBP that_IN they_PRP form_VBP an_DT equilibrium_NN ._.
We_PRP also_RB analyze_VBP the_DT relative_JJ error_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT difference_NN between_IN the_DT true_JJ optimum_NN and_CC the_DT approximate_JJ -RRB-_-RRB- ._.
The_DT time_NN complexity_NN of_IN the_DT approximate_JJ equilibrium_NN strategies_NNS is_VBZ O_NN -LRB-_-LRB- nm_NN /_: #_# -RRB-_-RRB- where_WRB n_NN is_VBZ the_DT negotiation_NN deadline_NN and_CC the_DT relative_JJ error_NN ._.
Finally_RB ,_, we_PRP extend_VBP the_DT analysis_NN to_TO online_JJ negotiation_NN where_WRB different_JJ issues_NNS become_VBP available_JJ at_IN different_JJ time_NN points_NNS and_CC the_DT agents_NNS are_VBP uncertain_JJ about_IN their_PRP$ valuations_NNS for_IN these_DT issues_NNS ._.
Specifically_RB ,_, we_PRP show_VBP that_IN an_DT approximate_JJ equilibrium_NN exists_VBZ for_IN online_JJ negotiation_NN and_CC show_VBP that_IN the_DT expected_VBN difference_NN between_IN the_DT optimum_NN and_CC the_DT approximate_JJ is_VBZ O_NN -LRB-_-LRB- m_NN -RRB-_-RRB- ._.
These_DT approximate_JJ strategies_NNS also_RB have_VBP polynomial_JJ time_NN complexity_NN ._.
In_IN so_RB doing_VBG ,_, our_PRP$ contribution_NN lies_VBZ in_IN analyzing_VBG the_DT computational_JJ complexity_NN of_IN the_DT above_JJ multi-issue_JJ negotiation_NN problem_NN ,_, and_CC finding_VBG the_DT approximate_JJ and_CC online_JJ equilibria_NNS ._.
No_DT previous_JJ work_NN has_VBZ determined_VBN these_DT equilibria_NNS ._.
Since_IN software_NN agents_NNS have_VBP limited_VBN computational_JJ resources_NNS ,_, our_PRP$ results_NNS are_VBP especially_RB relevant_JJ to_TO such_JJ resource_NN bounded_VBD agents_NNS ._.
The_DT remainder_NN of_IN the_DT paper_NN is_VBZ organised_VBN as_IN follows_VBZ ._.
We_PRP begin_VBP by_IN giving_VBG a_DT brief_JJ overview_NN of_IN single-issue_JJ negotiation_NN in_IN Section_NN #_# ._.
In_IN Section_NN #_# ,_, we_PRP obtain_VBP the_DT equilibrium_NN for_IN multi-issue_JJ negotiation_NN and_CC show_VBP that_IN finding_VBG equilibrium_NN offers_NNS is_VBZ an_DT NP-hard_JJ problem_NN ._.
We_PRP then_RB present_VBP an_DT approximate_JJ equilibrium_NN and_CC evaluate_VB its_PRP$ approximation_NN error_NN ._.
Section_NN #_# analyzes_VBZ online_JJ multi-issue_JJ negotiation_NN ._.
Section_NN #_# discusses_VBZ the_DT related_JJ literature_NN and_CC Section_NN #_# concludes_VBZ ._.
2_LS ._.
SINGLE-ISSUE_JJ NEGOTIATION_NN We_PRP adopt_VBP the_DT single_JJ issue_NN model_NN of_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- because_IN this_DT is_VBZ a_DT model_NN where_WRB ,_, during_IN negotiation_NN ,_, the_DT parties_NNS are_VBP allowed_VBN to_TO make_VB offers_NNS from_IN a_DT set_NN of_IN discrete_JJ offers_NNS ._.
Since_IN our_PRP$ focus_NN is_VBZ on_IN indivisible_JJ issues_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, parties_NNS are_VBP allowed_VBN to_TO make_VB one_CD of_IN two_CD possible_JJ offers_NNS :_: zero_CD or_CC one_CD -RRB-_-RRB- ,_, our_PRP$ scenario_NN fits_VBZ in_RP well_RB with_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ._.
Hence_RB we_PRP use_VBP this_DT basic_JJ single_JJ issue_NN model_NN and_CC extend_VB it_PRP to_TO multiple_JJ issues_NNS ._.
Before_IN doing_VBG so_RB ,_, we_PRP give_VBP an_DT overview_NN of_IN this_DT model_NN and_CC its_PRP$ equilibrium_NN strategies_NNS ._.
There_EX are_VBP two_CD strategic_JJ agents_NNS :_: a_DT and_CC b_NN ._.
Each_DT agent_NN has_VBZ time_NN constraints_NNS in_IN the_DT form_NN of_IN deadlines_NNS and_CC discount_NN factors_NNS ._.
The_DT two_CD agents_NNS negotiate_VBP over_IN a_DT single_JJ indivisible_JJ issue_NN -LRB-_-LRB- i_LS -RRB-_-RRB- ._.
This_DT issue_NN is_VBZ a_DT pie_NN ''_'' of_IN size_NN #_# and_CC the_DT agents_NNS want_VBP to_TO determine_VB who_WP gets_VBZ the_DT pie_NN ._.
There_EX is_VBZ a_DT deadline_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, a_DT number_NN of_IN rounds_NNS by_IN which_WDT negotiation_NN must_MD end_VB -RRB-_-RRB- ._.
Let_VB n_NN N_NN +_CC denote_VB this_DT deadline_NN ._.
The_DT agents_NNS use_VBP an_DT alternating_VBG offers_NNS protocol_NN -LRB-_-LRB- as_IN the_DT one_CD of_IN Rubinstein_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- -RRB-_-RRB- ,_, which_WDT proceeds_VBZ through_IN a_DT series_NN of_IN time_NN periods_NNS ._.
One_CD of_IN the_DT agents_NNS ,_, say_VBP a_DT ,_, starts_VBZ negotiation_NN in_IN the_DT first_JJ time_NN period_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, t_NN =_JJ #_# -RRB-_-RRB- by_IN making_VBG an_DT offer_NN -LRB-_-LRB- xi_NN =_JJ #_# or_CC #_# -RRB-_-RRB- to_TO b_NN ._.
Agent_NNP b_NN can_MD either_RB accept_VB or_CC reject_VB the_DT offer_NN ._.
If_IN it_PRP accepts_VBZ ,_, negotiation_NN ends_VBZ in_IN an_DT agreement_NN with_IN a_DT getting_VBG xi_NN and_CC b_NN getting_VBG yi_NN =_JJ #_# xi_FW ._.
Otherwise_RB ,_, negotiation_NN proceeds_NNS to_TO the_DT next_JJ time_NN period_NN ,_, in_IN which_WDT agent_NN b_NN makes_VBZ a_DT counter-offer_NN ._.
This_DT process_NN of_IN making_VBG offers_NNS continues_VBZ until_IN one_CD of_IN the_DT agents_NNS either_CC accepts_VBZ an_DT offer_NN or_CC quits_VBZ negotiation_NN -LRB-_-LRB- resulting_VBG in_IN a_DT conflict_NN -RRB-_-RRB- ._.
Thus_RB ,_, there_EX are_VBP three_CD possible_JJ actions_NNS an_DT agent_NN can_MD take_VB during_IN any_DT time_NN period_NN :_: accept_VB the_DT last_JJ offer_NN ,_, make_VBP a_DT new_JJ counter-offer_NN ,_, or_CC quit_VB the_DT negotiation_NN ._.
An_DT essential_JJ feature_NN of_IN negotiations_NNS involving_VBG alternating_VBG offers_NNS is_VBZ that_IN the_DT agents_NNS ''_'' utilities_NNS decrease_VBP with_IN time_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Specifically_RB ,_, the_DT decrease_NN occurs_VBZ at_IN each_DT step_NN of_IN offer_NN and_CC counteroffer_NN ._.
This_DT decrease_NN is_VBZ represented_VBN with_IN a_DT discount_NN factor_NN denoted_VBN #_# <_JJR i_FW #_# for_IN both1_NN agents_NNS ._.
Let_VB -LSB-_-LRB- xt_NN i_FW ,_, yt_NN i_FW -RSB-_-RRB- denote_VBP the_DT offer_NN made_VBN at_IN time_NN period_NN t_NN where_WRB xt_NN i_FW and_CC yt_FW i_FW denote_VBP the_DT share_NN for_IN agent_NN a_DT and_CC b_NN respectively_RB ._.
Then_RB ,_, for_IN a_DT given_VBN pie_NN ,_, the_DT set_NN of_IN possible_JJ offers_NNS is_VBZ :_: -LCB-_-LRB- -LSB-_-LRB- xt_NN i_FW ,_, yt_NN i_FW -RSB-_-RRB- :_: xt_NN i_FW =_JJ #_# or_CC #_# ,_, yt_NN i_FW =_JJ #_# or_CC #_# ,_, and_CC xt_NN i_FW +_CC yt_FW i_FW =_JJ #_# -RCB-_-RRB- At_IN time_NN t_NN ,_, if_IN a_DT and_CC b_NN receive_VBP a_DT share_NN of_IN xt_NN i_FW and_CC yt_FW i_FW respectively_RB ,_, then_RB their_PRP$ utilities_NNS are_VBP :_: ua_NN i_FW -LRB-_-LRB- xt_NN i_FW ,_, t_NN -RRB-_-RRB- =_JJ j_NN xt_NN i_FW t1_FW if_IN t_NN n_NN 0_CD otherwise_RB ub_NN i_FW -LRB-_-LRB- yt_NN i_FW ,_, t_NN -RRB-_-RRB- =_JJ j_NN yt_NN i_FW t1_FW if_IN t_NN n_NN 0_CD otherwise_RB The_DT conflict_NN utility_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT utility_NN received_VBN in_IN the_DT event_NN that_IN no_DT deal_NN is_VBZ struck_VBN -RRB-_-RRB- is_VBZ zero_CD for_IN both_DT agents_NNS ._.
For_IN the_DT above_JJ setting_NN ,_, the_DT agents_NNS reason_NN as_IN follows_VBZ in_IN order_NN to_TO determine_VB what_WP to_TO offer_VB at_IN t_NN =_JJ #_# ._.
We_PRP let_VBD A_NN -LRB-_-LRB- #_# -RRB-_-RRB- -LRB-_-LRB- B_NN -LRB-_-LRB- #_# -RRB-_-RRB- -RRB-_-RRB- denote_VBP a_DT ''_'' s_NNS -LRB-_-LRB- b_NN ''_'' s_NNS -RRB-_-RRB- equilibrium_NN offer_NN for_IN the_DT first_JJ time_NN period_NN ._.
Let_VB agent_NN a_DT denote_VBP the_DT first_JJ mover_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, at_IN t_NN =_JJ #_# ,_, a_DT proposes_VBZ to_TO b_NN who_WP should_MD get_VB the_DT pie_NN -RRB-_-RRB- ._.
To_TO begin_VB ,_, consider_VB the_DT case_NN where_WRB the_DT deadline_NN for_IN both_DT agents_NNS is_VBZ n_NN =_JJ #_# ._.
If_IN b_NN accepts_VBZ ,_, the_DT division_NN occurs_VBZ as_IN agreed_VBN ;_: if_IN not_RB ,_, neither_CC agent_NN gets_VBZ anything_NN -LRB-_-LRB- since_IN n_NN =_JJ #_# is_VBZ the_DT deadline_NN -RRB-_-RRB- ._.
Here_RB ,_, a_DT is_VBZ in_IN a_DT powerful_JJ position_NN and_CC is_VBZ able_JJ to_TO propose_VB to_TO keep_VB ###_CD percent_NN of_IN the_DT pie_NN and_CC give_VB nothing_NN to_TO b_NN #_# ._.
Since_IN the_DT deadline_NN is_VBZ n_NN =_JJ #_# ,_, b_NN accepts_VBZ this_DT offer_NN and_CC agreement_NN takes_VBZ place_NN in_IN the_DT first_JJ time_NN period_NN ._.
Now_RB ,_, consider_VB the_DT case_NN where_WRB the_DT deadline_NN is_VBZ n_NN =_JJ #_# ._.
In_IN order_NN to_TO decide_VB what_WP to_TO offer_VB in_IN the_DT first_JJ round_NN ,_, a_DT looks_VBZ ahead_RB to_TO t_NN =_JJ #_# and_CC reasons_NNS backwards_RB ._.
Agent_NNP a_DT reasons_NNS that_IN if_IN negotiation_NN proceeds_NNS to_TO the_DT second_JJ round_NN ,_, b_NN will_MD take_VB ###_CD percent_NN of_IN the_DT pie_NN by_IN offering_VBG -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- and_CC leave_VB nothing_NN for_IN a_DT ._.
Thus_RB ,_, in_IN the_DT first_JJ time_NN period_NN ,_, if_IN a_DT offers_NNS b_NN anything_NN less_JJR than_IN the_DT whole_JJ pie_NN ,_, b_NN will_MD reject_VB the_DT offer_NN ._.
Hence_RB ,_, during_IN the_DT first_JJ time_NN period_NN ,_, agent_NN a_DT offers_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
Agent_NNP b_NN accepts_VBZ this_DT and_CC an_DT agreement_NN occurs_VBZ in_IN the_DT first_JJ time_NN period_NN ._.
In_IN general_JJ ,_, if_IN the_DT deadline_NN is_VBZ n_NN ,_, negotiation_NN proceeds_NNS as_IN follows_VBZ ._.
As_IN before_RB ,_, agent_NN a_DT decides_VBZ what_WP to_TO offer_VB in_IN the_DT first_JJ round_NN by_IN looking_VBG ahead_RB as_IN far_RB as_IN t_NN =_JJ n_NN and_CC then_RB reasoning_NN backwards_RB ._.
Agent_NNP a_DT ''_'' s_NNS 1_CD Having_VBG a_DT different_JJ discount_NN factor_NN for_IN different_JJ agents_NNS only_RB makes_VBZ the_DT presentation_NN more_RBR involved_VBN without_IN leading_VBG to_TO any_DT changes_NNS in_IN the_DT analysis_NN of_IN the_DT strategic_JJ behaviour_NN of_IN the_DT agents_NNS or_CC the_DT time_NN complexity_NN of_IN finding_VBG the_DT equilibrium_NN offers_VBZ ._.
Hence_RB we_PRP have_VBP a_DT single_JJ discount_NN factor_NN for_IN both_DT agents_NNS ._.
2_CD It_PRP is_VBZ possible_JJ that_IN b_NN may_MD reject_VB such_PDT a_DT proposal_NN ._.
However_RB ,_, irrespective_RB of_IN whether_IN b_NN accepts_VBZ or_CC rejects_VBZ the_DT proposal_NN ,_, it_PRP gets_VBZ zero_CD utility_NN -LRB-_-LRB- because_IN the_DT deadline_NN is_VBZ n_NN =_JJ #_# -RRB-_-RRB- ._.
Thus_RB ,_, we_PRP assume_VBP that_IN b_NN accepts_VBZ a_DT ''_'' s_VBZ offer_NN ._.
952_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- offer_NN for_IN t_NN =_JJ #_# depends_VBZ on_IN who_WP the_DT offering_NN agent_NN is_VBZ for_IN the_DT last_JJ time_NN period_NN ._.
This_DT ,_, in_IN turn_NN ,_, depends_VBZ on_IN whether_IN n_NN is_VBZ odd_JJ or_CC even_RB ._.
Since_IN a_DT makes_VBZ an_DT offer_NN at_IN t_NN =_JJ #_# and_CC the_DT agents_NNS use_VBP the_DT alternating_VBG offers_NNS protocol_NN ,_, the_DT offering_NN agent_NN for_IN the_DT last_JJ time_NN period_NN is_VBZ b_NN if_IN n_NN is_VBZ even_RB and_CC it_PRP is_VBZ a_DT if_IN n_NN is_VBZ odd_JJ ._.
Thus_RB ,_, depending_VBG on_IN whether_IN n_NN is_VBZ odd_JJ or_CC even_RB ,_, a_DT makes_VBZ the_DT following_VBG offer_NN at_IN t_NN =_JJ #_# :_: A_DT -LRB-_-LRB- #_# -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN ODD_NN n_NN ACCEPT_VBP IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- #_# -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN EVEN_NN n_NN ACCEPT_VBP IF_IN a_DT ''_'' s_VBZ TURN_NNP Agent_NNP b_NN accepts_VBZ this_DT offer_NN and_CC negotiation_NN ends_VBZ in_IN the_DT first_JJ time_NN period_NN ._.
Note_VB that_IN the_DT equilibrium_NN outcome_NN depends_VBZ on_IN who_WP makes_VBZ the_DT first_JJ move_NN ._.
Since_IN we_PRP have_VBP two_CD agents_NNS and_CC either_DT of_IN them_PRP could_MD move_VB first_RB ,_, we_PRP get_VBP two_CD possible_JJ equilibrium_NN outcomes_NNS ._.
On_IN the_DT basis_NN of_IN the_DT above_JJ equilibrium_NN for_IN single-issue_JJ negotiation_NN with_IN complete_JJ information_NN ,_, we_PRP first_RB obtain_VB the_DT equilibrium_NN for_IN multiple_JJ issues_NNS and_CC then_RB show_VBP that_IN computing_VBG these_DT offers_NNS is_VBZ a_DT hard_JJ problem_NN ._.
We_PRP then_RB present_VBP a_DT time_NN efficient_JJ approximate_JJ equilibrium_NN ._.
3_LS ._.
MULTI-ISSUE_NNP NEGOTIATION_NNP We_PRP first_RB analyse_VBP the_DT complete_JJ information_NN setting_NN ._.
This_DT section_NN forms_VBZ the_DT base_NN which_WDT we_PRP extend_VBP to_TO the_DT case_NN of_IN information_NN uncertainty_NN in_IN Section_NN #_# ._.
Here_RB a_DT and_CC b_NN negotiate_VBP over_IN m_NN >_JJR #_# indivisible_JJ issues_NNS ._.
These_DT issues_NNS are_VBP m_NN distinct_JJ pies_NNS and_CC the_DT agents_NNS want_VBP to_TO determine_VB how_WRB to_TO distribute_VB the_DT pies_NNS between_IN themselves_PRP ._.
Let_VB S_NN =_JJ -LCB-_-LRB- #_# ,_, #_# ,_, ..._: ,_, m_NN -RCB-_-RRB- denote_VBP the_DT set_NN of_IN m_NN pies_NNS ._.
As_IN before_RB ,_, each_DT pie_NN is_VBZ of_IN size_NN #_# ._.
Let_VB the_DT discount_NN factor_NN for_IN issue_NN c_NN ,_, where_WRB #_# c_NN m_NN ,_, be_VB #_# <_JJR c_NN #_# ._.
For_IN each_DT issue_NN ,_, let_VB n_NN denote_VBP each_DT agent_NN ''_'' s_NNS deadline_NN ._.
In_IN the_DT offer_NN for_IN time_NN period_NN t_NN -LRB-_-LRB- where_WRB #_# t_NN n_NN -RRB-_-RRB- ,_, agent_NN a_DT ''_'' s_NNS -LRB-_-LRB- b_NN ''_'' s_NNS -RRB-_-RRB- share_NN for_IN each_DT of_IN the_DT m_NN issues_NNS is_VBZ now_RB represented_VBN as_IN an_DT m_NN element_NN vector_NN xt_NN Bm_NN -LRB-_-LRB- yt_NN Bm_NN -RRB-_-RRB- where_WRB B_NN denotes_VBZ the_DT set_NN -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ._.
Thus_RB ,_, if_IN agent_NN a_DT ''_'' s_VBZ share_NN for_IN issue_NN c_NN at_IN time_NN t_NN is_VBZ xt_NN c_NN ,_, then_RB agent_NN b_NN ''_'' s_NNS share_NN is_VBZ yt_NN c_NN =_JJ -LRB-_-LRB- 1xt_JJ c_NN -RRB-_-RRB- ._.
The_DT shares_NNS for_IN a_DT and_CC b_NN are_VBP together_RB represented_VBN as_IN the_DT package_NN -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ._.
As_IN is_VBZ traditional_JJ in_IN multi-issue_JJ utility_NN theory_NN ,_, we_PRP define_VBP an_DT agent_NN ''_'' s_NNS cumulative_JJ utility_NN using_VBG the_DT standard_JJ additive_JJ form_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
The_DT functions_NNS Ua_NNP :_: Bm_NN Bm_NN N_NN +_CC R_NN and_CC Ub_NNP :_: Bm_NN Bm_NN N_NN +_CC R_NN give_VBP the_DT cumulative_JJ utilities_NNS for_IN a_DT and_CC b_NN respectively_RB at_IN time_NN t_NN ._.
These_DT are_VBP defined_VBN as_IN follows_VBZ :_: Ua_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- =_JJ -LRB-_-LRB- m_NN c_NN =_JJ 1ka_FW c_NN ua_NN c_NN -LRB-_-LRB- xt_NN c_NN ,_, t_NN -RRB-_-RRB- if_IN t_NN n_NN 0_CD otherwise_RB -LRB-_-LRB- #_# -RRB-_-RRB- Ub_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- =_JJ -LRB-_-LRB- m_NN c_NN =_JJ 1kb_JJ cub_NN c_NN -LRB-_-LRB- yt_NN c_NN ,_, t_NN -RRB-_-RRB- if_IN t_NN n_NN 0_CD otherwise_RB -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB ka_FW Nm_FW +_CC denotes_VBZ an_DT m_NN element_NN vector_NN of_IN constants_NNS for_IN agent_NN a_DT and_CC kb_NN Nm_NN +_CC that_IN for_IN b_NN ._.
Here_RB N_NN +_CC denotes_VBZ the_DT set_NN of_IN positive_JJ integers_NNS ._.
These_DT vectors_NNS indicate_VBP how_WRB the_DT agents_NNS value_NN different_JJ issues_NNS ._.
For_IN example_NN ,_, if_IN ka_FW c_NN >_JJR ka_FW c_NN +_CC #_# ,_, then_RB agent_NN a_DT values_NNS issue_NN c_NN more_JJR than_IN issue_NN c_NN +_CC #_# ._.
Likewise_RB for_IN agent_NN b_NN ._.
In_IN other_JJ words_NNS ,_, the_DT m_NN issues_NNS are_VBP perfect_JJ substitutes_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, all_DT that_DT matters_NNS to_TO an_DT agent_NN is_VBZ its_PRP$ total_JJ utility_NN for_IN all_PDT the_DT m_NN issues_NNS and_CC not_RB that_DT for_IN any_DT subset_NN of_IN them_PRP -RRB-_-RRB- ._.
In_IN all_PDT the_DT settings_NNS we_PRP study_NN ,_, the_DT issues_NNS will_MD be_VB perfect_JJ substitutes_NNS ._.
To_TO begin_VB each_DT agent_NN has_VBZ complete_JJ information_NN about_IN all_DT negotiation_NN parameters_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, n_NN ,_, m_NN ,_, ka_FW c_NN ,_, kb_NN c_NN ,_, and_CC c_NN for_IN #_# c_NN m_NN -RRB-_-RRB- ._.
Now_RB ,_, multi-issue_JJ negotiation_NN can_MD be_VB done_VBN using_VBG different_JJ procedures_NNS ._.
Broadly_RB speaking_VBG ,_, there_EX are_VBP three_CD key_JJ procedures_NNS for_IN negotiating_VBG multiple_JJ issues_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- :_: 1_CD ._.
the_DT package_NN deal_NN procedure_NN where_WRB all_PDT the_DT issues_NNS are_VBP settled_VBN together_RB as_IN a_DT bundle_NN ,_, 2_CD ._.
the_DT sequential_JJ procedure_NN where_WRB the_DT issues_NNS are_VBP discussed_VBN one_CD after_IN another_DT ,_, and_CC 3_CD ._.
the_DT simultaneous_JJ procedure_NN where_WRB the_DT issues_NNS are_VBP discussed_VBN in_IN parallel_NN ._.
Between_IN these_DT three_CD procedures_NNS ,_, the_DT package_NN deal_NN is_VBZ known_VBN to_TO generate_VB Pareto_NNP optimal_JJ outcomes_NNS -LSB-_-LRB- ##_NNS ,_, #_# -RSB-_-RRB- ._.
Hence_RB we_PRP adopt_VB it_PRP here_RB ._.
We_PRP first_RB give_VB a_DT brief_JJ description_NN of_IN the_DT procedure_NN and_CC then_RB determine_VB the_DT equilibrium_NN strategies_NNS for_IN it_PRP ._.
3_LS ._.
#_# The_DT package_NN deal_NN procedure_NN In_IN this_DT procedure_NN ,_, the_DT agents_NNS use_VBP the_DT same_JJ protocol_NN as_IN for_IN singleissue_JJ negotiation_NN -LRB-_-LRB- described_VBN in_IN Section_NN #_# -RRB-_-RRB- ._.
However_RB ,_, an_DT offer_NN for_IN the_DT package_NN deal_NN includes_VBZ a_DT proposal_NN for_IN each_DT issue_NN under_IN negotiation_NN ._.
Thus_RB ,_, for_IN m_NN issues_NNS ,_, an_DT offer_NN includes_VBZ m_NN divisions_NNS ,_, one_CD for_IN each_DT issue_NN ._.
Agents_NNS are_VBP allowed_VBN to_TO either_CC accept_VB a_DT complete_JJ offer_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, all_DT m_NN issues_NNS -RRB-_-RRB- or_CC reject_VBP a_DT complete_JJ offer_NN ._.
An_DT agreement_NN can_MD therefore_RB take_VB place_NN either_RB on_IN all_DT m_NN issues_NNS or_CC on_IN none_NN of_IN them_PRP ._.
As_IN per_IN the_DT single-issue_JJ negotiation_NN ,_, an_DT agent_NN decides_VBZ what_WP to_TO offer_VB by_IN looking_VBG ahead_RB and_CC reasoning_NN backwards_RB ._.
However_RB ,_, since_IN an_DT offer_NN for_IN the_DT package_NN deal_NN includes_VBZ a_DT share_NN for_IN all_PDT the_DT m_NN issues_NNS ,_, the_DT agents_NNS can_MD now_RB make_VB tradeoffs_NNS across_IN the_DT issues_NNS in_IN order_NN to_TO maximise_VB their_PRP$ cumulative_JJ utilities_NNS ._.
For_IN #_# c_NN m_NN ,_, the_DT equilibrium_NN offer_NN for_IN issue_NN c_NN at_IN time_NN t_NN is_VBZ denoted_VBN as_IN -LSB-_-LRB- at_IN c_NN ,_, bt_NN c_NN -RSB-_-RRB- where_WRB at_IN c_NN and_CC bt_NN c_NN denote_VBP the_DT shares_NNS for_IN agent_NN a_DT and_CC b_NN respectively_RB ._.
We_PRP denote_VBP the_DT equilibrium_NN package_NN at_IN time_NN t_NN as_IN -LSB-_-LRB- at_IN ,_, bt_NN -RSB-_-RRB- where_WRB at_IN Bm_NN -LRB-_-LRB- bt_NN Bm_NN -RRB-_-RRB- is_VBZ an_DT m_NN element_NN vector_NN that_WDT denotes_VBZ a_DT ''_'' s_NNS -LRB-_-LRB- b_NN ''_'' s_NNS -RRB-_-RRB- share_NN for_IN each_DT of_IN the_DT m_NN issues_NNS ._.
Also_RB ,_, for_IN 1_CD c_NN m_NN ,_, c_NN is_VBZ the_DT discount_NN factor_NN for_IN issue_NN c_NN ._.
The_DT symbols_NNS #_# and_CC #_# denote_VBP m_NN element_NN vectors_NNS of_IN zeroes_NNS and_CC ones_NNS respectively_RB ._.
Note_VB that_DT for_IN #_# t_NN n_NN ,_, at_IN c_NN +_CC bt_NN c_NN =_JJ #_# -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT sum_NN of_IN the_DT agents_NNS ''_'' shares_NNS -LRB-_-LRB- at_IN time_NN t_NN -RRB-_-RRB- for_IN each_DT pie_NN is_VBZ one_CD -RRB-_-RRB- ._.
Finally_RB ,_, for_IN time_NN period_NN t_NN -LRB-_-LRB- for_IN 1_CD t_NN n_NN -RRB-_-RRB- we_PRP let_VBD A_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -LRB-_-LRB- respectively_RB B_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- denote_VBP the_DT equilibrium_NN strategy_NN for_IN agent_NN a_DT -LRB-_-LRB- respectively_RB b_LS -RRB-_-RRB- ._.
3_LS ._.
#_# Equilibrium_NN strategies_NNS As_IN mentioned_VBN in_IN Section_NN #_# ,_, the_DT package_NN deal_NN allows_VBZ agents_NNS to_TO make_VB tradeoffs_NNS ._.
We_PRP let_VBD TRADEOFFA_NN -LRB-_-LRB- TRADEOFFB_NN -RRB-_-RRB- denote_VBP agent_NN a_DT ''_'' s_NNS -LRB-_-LRB- b_NN ''_'' s_NNS -RRB-_-RRB- function_NN for_IN making_VBG tradeoffs_NNS ._.
We_PRP let_VBD P_NN denote_VBP a_DT set_NN of_IN parameters_NNS to_TO the_DT procedure_NN TRADEOFFA_NN -LRB-_-LRB- TRADEOFFB_NN -RRB-_-RRB- where_WRB P_NN =_JJ -LCB-_-LRB- ka_FW ,_, kb_NN ,_, ,_, m_NN -RCB-_-RRB- ._.
Given_VBN this_DT ,_, the_DT following_VBG theorem_NN characterises_VBZ the_DT equilibrium_NN for_IN the_DT package_NN deal_NN procedure_NN ._.
THEOREM_NNP #_# ._.
For_IN the_DT package_NN deal_NN procedure_NN ,_, the_DT following_VBG strategies_NNS form_VBP a_DT Nash_NNP equilibrium_NN ._.
The_DT equilibrium_NN strategy_NN for_IN t_NN =_JJ n_NN is_VBZ :_: A_DT -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN a_DT ''_'' s_VBZ TURN_NNP ACCEPT_NNP IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN b_NN ''_'' s_NNS TURN_VBP ACCEPT_NNP IF_IN a_DT ''_'' s_VBZ TURN_NN For_IN all_DT preceding_VBG time_NN periods_NNS t_NN <_JJR n_NN ,_, if_IN -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- denotes_VBZ the_DT offer_NN made_VBN at_IN time_NN t_NN ,_, then_RB the_DT equilibrium_NN strategies_NNS are_VBP defined_VBN as_IN follows_VBZ :_: A_DT -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ 8_CD <_JJR :_: OFFER_NN TRADEOFFA_NN -LRB-_-LRB- P_NN ,_, UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN -RRB-_-RRB- IF_IN a_DT ''_'' s_VBZ TURN_NNP If_IN -LRB-_-LRB- Ua_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- ACCEPT_VBP else_RB REJECT_VB IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ 8_CD <_JJR :_: OFFER_NN TRADEOFFB_NN -LRB-_-LRB- P_NN ,_, UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN -RRB-_-RRB- IF_IN b_NN ''_'' s_NNS TURN_VBP If_IN -LRB-_-LRB- Ub_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- ACCEPT_VBP else_RB REJECT_VB IF_IN a_DT ''_'' s_NNS TURN_VBP The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD where_WRB UA_NNP -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ Ua_NN -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- and_CC UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ Ub_NN -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ._.
PROOF_NN ._.
We_PRP look_VBP ahead_RB to_TO the_DT last_JJ time_NN period_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, t_NN =_JJ n_NN -RRB-_-RRB- and_CC then_RB reason_NN backwards_RB ._.
To_TO begin_VB ,_, if_IN negotiation_NN reaches_VBZ the_DT deadline_NN -LRB-_-LRB- n_NN -RRB-_-RRB- ,_, then_RB the_DT agent_NN whose_WP$ turn_VB it_PRP is_VBZ takes_VBZ everything_NN and_CC leaves_VBZ nothing_NN for_IN its_PRP$ opponent_NN ._.
Hence_RB ,_, we_PRP get_VBP the_DT strategies_NNS A_NN -LRB-_-LRB- n_NN -RRB-_-RRB- and_CC B_NN -LRB-_-LRB- n_NN -RRB-_-RRB- as_IN given_VBN in_IN the_DT statement_NN of_IN the_DT theorem_NN ._.
In_IN all_PDT the_DT preceding_VBG time_NN periods_NNS -LRB-_-LRB- t_NN <_JJR n_NN -RRB-_-RRB- ,_, the_DT offering_NN agent_NN proposes_VBZ a_DT package_NN that_WDT gives_VBZ its_PRP$ opponent_NN a_DT cumulative_JJ utility_NN equal_JJ to_TO what_WP the_DT opponent_NN would_MD get_VB from_IN its_PRP$ own_JJ equilibrium_NN offer_NN for_IN the_DT next_JJ time_NN period_NN ._.
During_IN time_NN period_NN t_NN ,_, either_CC a_DT or_CC b_NN could_MD be_VB the_DT offering_NN agent_NN ._.
Consider_VB the_DT case_NN where_WRB a_DT makes_VBZ an_DT offer_NN at_IN t_NN ._.
The_DT package_NN that_IN a_DT offers_NNS at_IN t_NN gives_VBZ b_NN a_DT cumulative_JJ utility_NN of_IN Ub_NNP -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ._.
However_RB ,_, since_IN there_EX is_VBZ more_JJR than_IN one_CD issue_NN ,_, there_EX is_VBZ more_JJR than_IN one_CD package_NN that_WDT gives_VBZ b_NN this_DT cumulative_JJ utility_NN ._.
From_IN among_IN these_DT packages_NNS ,_, a_DT offers_VBZ the_DT one_CD that_WDT maximises_VBZ its_PRP$ own_JJ cumulative_JJ utility_NN -LRB-_-LRB- because_IN it_PRP is_VBZ a_DT utility_NN maximiser_NN -RRB-_-RRB- ._.
Thus_RB ,_, the_DT problem_NN for_IN a_DT is_VBZ to_TO find_VB the_DT package_NN -LSB-_-LRB- at_IN ,_, bt_NN -RSB-_-RRB- so_RB as_IN to_TO :_: maximize_VB mX_NN c_NN =_JJ #_# ka_FW c_NN -LRB-_-LRB- #_# bt_FW c_NN -RRB-_-RRB- t1_NN c_NN -LRB-_-LRB- #_# -RRB-_-RRB- such_JJ that_IN mX_NN c_NN =_JJ #_# bt_FW ckb_FW c_NN UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- bt_NN c_NN =_JJ #_# or_CC #_# for_IN #_# c_NN m_NN where_WRB UB_NNP -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t1_NN c_NN ,_, ka_FW c_NN ,_, and_CC kb_NN c_NN are_VBP constants_NNS and_CC bt_NN c_NN -LRB-_-LRB- #_# c_NN m_NN -RRB-_-RRB- is_VBZ a_DT variable_NN ._.
Assume_VB that_IN the_DT function_NN TRADEOFFA_NN takes_VBZ parameters_NNS P_NN ,_, UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, and_CC t_NN ,_, to_TO solve_VB the_DT maximisation_NN problem_NN given_VBN in_IN Equation_NN #_# and_CC returns_VBZ the_DT corresponding_JJ package_NN ._.
If_IN there_EX is_VBZ more_JJR than_IN one_CD package_NN that_WDT solves_VBZ Equation_NN #_# ,_, then_RB TRADEOFFA_NN returns_NNS any_DT one_CD of_IN them_PRP -LRB-_-LRB- because_IN agent_NN a_DT gets_VBZ equal_JJ utility_NN from_IN all_DT such_JJ packages_NNS and_CC so_RB does_VBZ agent_NN b_NN -RRB-_-RRB- ._.
The_DT function_NN TRADEOFFB_NN for_IN agent_NN b_NN is_VBZ analogous_JJ to_TO that_DT for_IN a_DT ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT equilibrium_NN strategy_NN for_IN the_DT agent_NN that_WDT receives_VBZ an_DT offer_NN is_VBZ as_IN follows_VBZ ._.
For_IN time_NN period_NN t_NN ,_, let_VB b_NN denote_VBP the_DT receiving_VBG agent_NN ._.
Then_RB ,_, b_NN accepts_VBZ -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- if_IN UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- Ub_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- ,_, otherwise_RB it_PRP rejects_VBZ the_DT offer_NN because_IN it_PRP can_MD get_VB a_DT higher_JJR utility_NN in_IN the_DT next_JJ time_NN period_NN ._.
The_DT equilibrium_NN strategy_NN for_IN a_DT as_RB receiving_VBG agent_NN is_VBZ defined_VBN analogously_RB ._.
In_IN this_DT way_NN ,_, we_PRP reason_VBP backwards_RB and_CC obtain_VB the_DT offers_NNS for_IN the_DT first_JJ time_NN period_NN ._.
Thus_RB ,_, we_PRP get_VBP the_DT equilibrium_NN strategies_NNS -LRB-_-LRB- A_NN -LRB-_-LRB- t_NN -RRB-_-RRB- and_CC B_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- given_VBN in_IN the_DT statement_NN of_IN the_DT theorem_NN ._.
The_DT following_VBG example_NN illustrates_VBZ how_WRB the_DT agents_NNS make_VBP tradeoffs_NNS using_VBG the_DT above_JJ equilibrium_NN strategies_NNS ._.
EXAMPLE_NN #_# ._.
Assume_VB there_EX are_VBP m_NN =_JJ #_# issues_NNS for_IN negotiation_NN ,_, the_DT deadline_NN for_IN both_DT issues_NNS is_VBZ n_NN =_JJ #_# ,_, and_CC the_DT discount_NN factor_NN for_IN both_DT issues_NNS for_IN both_DT agents_NNS is_VBZ =_JJ #_# /_: #_# ._.
Let_VB ka_FW 1_CD =_JJ #_# ,_, ka_FW 2_CD =_JJ #_# ,_, kb_NN 1_CD =_JJ #_# ,_, and_CC kb_NN 2_CD =_JJ #_# ._.
Let_VB agent_NN a_DT be_VB the_DT first_JJ mover_NN ._.
By_IN using_VBG backward_RB reasoning_NN ,_, a_DT knows_VBZ that_IN if_IN negotiation_NN reaches_VBZ the_DT second_JJ time_NN period_NN -LRB-_-LRB- which_WDT is_VBZ the_DT deadline_NN -RRB-_-RRB- ,_, then_RB b_NN will_MD get_VB a_DT hundred_CD percent_NN of_IN both_CC the_DT issues_NNS ._.
This_DT gives_VBZ b_NN a_DT cumulative_JJ utility_NN of_IN UB_NNP -LRB-_-LRB- #_# -RRB-_-RRB- =_JJ #_# /_: #_# +_CC #_# /_: #_# =_JJ #_# ._.
Thus_RB ,_, in_IN the_DT first_JJ time_NN period_NN ,_, if_IN b_NN gets_VBZ anything_NN less_JJR than_IN a_DT utility_NN of_IN #_# ,_, it_PRP will_MD reject_VB a_DT ''_'' s_VBZ offer_NN ._.
So_RB ,_, at_IN t_NN =_JJ #_# ,_, a_DT offers_VBZ the_DT package_NN where_WRB it_PRP gets_VBZ issue_NN #_# and_CC b_NN gets_VBZ issue_NN #_# ._.
This_DT gives_VBZ a_DT cumulative_JJ utility_NN of_IN #_# to_TO a_DT and_CC #_# to_TO b_NN ._.
Agent_NNP b_NN accepts_VBZ the_DT package_NN and_CC an_DT agreement_NN takes_VBZ place_NN in_IN the_DT first_JJ time_NN period_NN ._.
The_DT maximization_NN problem_NN in_IN Equation_NN #_# can_MD be_VB viewed_VBN as_IN the_DT 0-1_CD knapsack_NN problem3_NN ._.
In_IN the_DT 0-1_CD knapsack_NN problem_NN ,_, we_PRP have_VBP a_DT set_VBN 3_CD Note_NN that_WDT for_IN the_DT case_NN of_IN divisible_JJ issues_NNS this_DT is_VBZ the_DT fractional_JJ knapof_NN m_NN items_NNS where_WRB each_DT item_NN has_VBZ a_DT profit_NN and_CC a_DT weight_NN ._.
There_EX is_VBZ a_DT knapsack_NN with_IN a_DT given_VBN capacity_NN ._.
The_DT objective_NN is_VBZ to_TO fill_VB the_DT knapsack_NN with_IN items_NNS so_RB as_IN to_TO maximize_VB the_DT cumulative_JJ profit_NN of_IN the_DT items_NNS in_IN the_DT knapsack_NN ._.
This_DT problem_NN is_VBZ analogous_JJ to_TO the_DT negotiation_NN problem_NN we_PRP want_VBP to_TO solve_VB -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT maximization_NN problem_NN of_IN Equation_NN #_# -RRB-_-RRB- ._.
Since_IN ka_FW c_NN and_CC t1_NN c_NN are_VBP constants_NNS ,_, maximizing_VBG Pm_NN c_NN =_JJ #_# ka_FW c_NN -LRB-_-LRB- 1bt_JJ c_NN -RRB-_-RRB- t1_NN c_NN is_VBZ the_DT same_JJ as_IN minimizing_VBG Pm_NN c_NN =_JJ #_# ka_FW c_NN bt_NN c_NN ._.
Hence_RB Equation_NN #_# can_MD be_VB written_VBN as_IN :_: minimize_VB mX_NN c_NN =_JJ #_# ka_FW c_NN bt_NN c_NN -LRB-_-LRB- #_# -RRB-_-RRB- such_JJ that_IN mX_NN c_NN =_JJ #_# bt_FW ckb_FW c_NN UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- bt_NN c_NN =_JJ #_# or_CC #_# for_IN #_# c_NN m_NN Equation_NN #_# is_VBZ a_DT minimization_NN version_NN of_IN the_DT standard_JJ 0-1_CD knapsack_NN problem4_NN with_IN m_NN items_NNS where_WRB ka_FW c_NN represents_VBZ the_DT profit_NN for_IN item_NN c_NN ,_, kb_NN c_NN the_DT weight_NN for_IN item_NN c_NN ,_, and_CC UB_NNP -LRB-_-LRB- t_NN -RRB-_-RRB- the_DT knapsack_NN capacity_NN ._.
Example_NN #_# was_VBD for_IN two_CD issues_NNS and_CC so_IN it_PRP was_VBD easy_JJ to_TO find_VB the_DT equilibrium_NN offers_VBZ ._.
But_CC ,_, in_IN general_JJ ,_, it_PRP is_VBZ not_RB computationally_RB easy_JJ to_TO find_VB the_DT equilibrium_NN offers_NNS of_IN Theorem_NNP #_# ._.
The_DT following_VBG theorem_NN proves_VBZ this_DT ._.
THEOREM_NNP #_# ._.
For_IN the_DT package_NN deal_NN procedure_NN ,_, the_DT problem_NN of_IN finding_VBG the_DT equilibrium_NN offers_VBZ given_VBN in_IN Theorem_NNP #_# is_VBZ NP-hard_NN ._.
PROOF_NN ._.
Finding_VBG the_DT equilibrium_NN offers_VBZ given_VBN in_IN Theorem_NNP #_# requires_VBZ solving_VBG the_DT 0-1_CD knapsack_NN problem_NN given_VBN in_IN Equation_NN #_# ._.
Since_IN the_DT 0-1_CD knapsack_NN problem_NN is_VBZ NP-hard_JJ -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, the_DT problem_NN of_IN finding_VBG equilibrium_NN for_IN the_DT package_NN deal_NN is_VBZ also_RB NP-hard_JJ ._.
3_LS ._.
#_# Approximate_JJ equilibrium_NN Researchers_NNS in_IN the_DT area_NN of_IN algorithms_NNS have_VBP found_VBN time_NN efficient_JJ methods_NNS for_IN computing_VBG approximate_JJ solutions_NNS to_TO 0-1_CD knapsack_NN problems_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Hence_RB we_PRP use_VBP these_DT methods_NNS to_TO find_VB a_DT solution_NN to_TO our_PRP$ negotiation_NN problem_NN ._.
At_IN this_DT stage_NN ,_, we_PRP would_MD like_VB to_TO point_VB out_RP the_DT main_JJ difference_NN between_IN solving_VBG the_DT 0-1_CD knapsack_NN problem_NN and_CC solving_VBG our_PRP$ negotiation_NN problem_NN ._.
The_DT 0-1_CD knapsack_NN problem_NN involves_VBZ decision_NN making_NN by_IN a_DT single_JJ agent_NN regarding_VBG which_WDT items_NNS to_TO place_VB in_IN the_DT knapsack_NN ._.
On_IN the_DT other_JJ hand_NN ,_, our_PRP$ negotiation_NN problem_NN involves_VBZ two_CD players_NNS and_CC they_PRP are_VBP both_DT strategic_JJ ._.
Hence_RB ,_, in_IN our_PRP$ case_NN ,_, it_PRP is_VBZ not_RB enough_JJ to_TO just_RB find_VB an_DT approximate_JJ solution_NN to_TO the_DT knapsack_NN problem_NN ,_, we_PRP must_MD also_RB show_VB that_IN such_PDT an_DT approximation_NN forms_VBZ an_DT equilibrium_NN ._.
The_DT traditional_JJ approach_NN for_IN overcoming_VBG the_DT computational_JJ complexity_NN in_IN finding_VBG an_DT equilibrium_NN has_VBZ been_VBN to_TO use_VB an_DT approximate_JJ equilibrium_NN ._.
In_IN this_DT approach_NN ,_, a_DT strategy_NN profile_NN is_VBZ said_VBN to_TO form_VB an_DT approximate_JJ Nash_NNP equilibrium_NN if_IN neither_DT agent_NN can_MD gain_VB more_JJR than_IN the_DT constant_JJ by_IN deviating_VBG ._.
Hence_RB ,_, our_PRP$ aim_NN is_VBZ to_TO use_VB the_DT solution_NN to_TO the_DT 0-1_CD knapsack_NN problem_NN proposed_VBN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- and_CC show_VBP that_IN it_PRP forms_VBZ an_DT approximate_JJ equilibrium_NN to_TO our_PRP$ negotiation_NN problem_NN ._.
Before_IN doing_VBG so_RB ,_, we_PRP give_VBP a_DT brief_JJ overview_NN of_IN the_DT key_JJ ideas_NNS that_WDT underlie_VBP approximation_NN algorithms_NNS ._.
There_EX are_VBP two_CD key_JJ issues_NNS in_IN the_DT design_NN of_IN approximate_JJ algorithms_NNS -LSB-_-LRB- #_# -RSB-_-RRB- :_: sack_NN problem_NN ._.
The_DT factional_JJ knapsack_NN problem_NN is_VBZ computationally_RB easy_JJ ;_: it_PRP can_MD be_VB solved_VBN in_IN time_NN polynomial_JJ in_IN the_DT number_NN of_IN items_NNS in_IN the_DT knapsack_NN problem_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
In_IN contrast_NN ,_, the_DT 0-1_CD knapsack_NN problem_NN is_VBZ computationally_RB hard_JJ ._.
4_CD Note_NN that_WDT for_IN the_DT standard_JJ 0-1_CD knapsack_NN problem_NN the_DT weights_NNS ,_, profits_NNS and_CC the_DT capacity_NN are_VBP positive_JJ integers_NNS ._.
However_RB a_DT 0-1_CD knapsack_NN problem_NN with_IN fractions_NNS and_CC non_JJ positive_JJ values_NNS can_MD easily_RB be_VB transformed_VBN to_TO one_CD with_IN positive_JJ integers_NNS in_IN time_NN linear_JJ in_IN m_NN using_VBG the_DT methods_NNS given_VBN in_IN -LSB-_-LRB- #_# ,_, ##_NN -RSB-_-RRB- ._.
954_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- 1_CD ._.
the_DT quality_NN of_IN their_PRP$ solution_NN ,_, and_CC 2_CD ._.
the_DT time_NN taken_VBN to_TO compute_VB the_DT approximation_NN ._.
The_DT quality_NN of_IN an_DT approximate_JJ algorithm_NN is_VBZ determined_VBN by_IN comparing_VBG its_PRP$ performance_NN to_TO that_DT of_IN the_DT optimal_JJ algorithm_NN and_CC measuring_VBG the_DT relative_JJ error_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
The_DT relative_JJ error_NN is_VBZ defined_VBN as_IN -LRB-_-LRB- zz_NN -RRB-_-RRB- /_: z_SYM where_WRB z_SYM is_VBZ the_DT approximate_JJ solution_NN and_CC z_SYM the_DT optimal_JJ one_CD ._.
In_IN general_JJ ,_, we_PRP are_VBP interested_JJ in_IN finding_VBG approximate_JJ algorithms_NNS whose_WP$ relative_JJ error_NN is_VBZ bounded_VBN from_IN above_JJ by_IN a_DT certain_JJ constant_JJ ,_, i_FW ._.
e_LS ._.
,_, -LRB-_-LRB- z_SYM z_SYM -RRB-_-RRB- /_: z_SYM -LRB-_-LRB- #_# -RRB-_-RRB- Regarding_VBG the_DT second_JJ issue_NN of_IN time_NN complexity_NN ,_, we_PRP are_VBP interested_JJ in_IN finding_VBG fully_RB polynomial_JJ approximation_NN algorithms_NNS ._.
An_DT approximation_NN algorithm_NN is_VBZ said_VBN to_TO be_VB fully_RB polynomial_JJ if_IN for_IN any_DT >_JJR #_# it_PRP finds_VBZ a_DT solution_NN satisfying_VBG Equation_NN #_# in_IN time_NN polynomially_RB bounded_VBN by_IN size_NN of_IN the_DT problem_NN -LRB-_-LRB- for_IN the_DT 0-1_CD knapsack_NN problem_NN ,_, the_DT problem_NN size_NN is_VBZ equal_JJ to_TO the_DT number_NN of_IN items_NNS -RRB-_-RRB- and_CC by_IN #_# /_: -LSB-_-LRB- #_# -RSB-_-RRB- ._.
For_IN the_DT 0-1_CD knapsack_NN problem_NN ,_, Ibarra_NNP and_CC Kim_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- presented_VBD a_DT fully_RB polynomial_JJ approximation_NN method_NN ._.
This_DT method_NN is_VBZ based_VBN on_IN dynamic_JJ programming_NN ._.
It_PRP is_VBZ a_DT parametric_JJ method_NN that_WDT takes_VBZ as_IN a_DT parameter_NN and_CC for_IN any_DT >_JJR #_# ,_, finds_VBZ a_DT heuristic_NN solution_NN z_SYM with_IN relative_JJ error_NN at_IN most_JJS ,_, such_JJ that_IN the_DT time_NN and_CC space_NN complexity_NN grow_VB polynomially_RB with_IN the_DT number_NN of_IN items_NNS m_NN and_CC #_# /_: ._.
More_RBR specifically_RB ,_, the_DT space_NN and_CC time_NN complexity_NN are_VBP both_DT O_NN -LRB-_-LRB- m_NN /_: #_# -RRB-_-RRB- and_CC hence_RB polynomial_JJ in_IN m_NN and_CC #_# /_: -LRB-_-LRB- see_VB -LSB-_-LRB- ##_CD -RSB-_-RRB- for_IN the_DT detailed_JJ approximation_NN algorithm_NN and_CC proof_NN of_IN time_NN and_CC space_NN complexity_NN -RRB-_-RRB- ._.
Since_IN the_DT Ibarra_NNP and_CC Kim_NNP method_NN is_VBZ fully_RB polynomial_JJ ,_, we_PRP use_VBP it_PRP to_TO solve_VB our_PRP$ negotiation_NN problem_NN ._.
This_DT is_VBZ done_VBN as_IN follows_VBZ ._.
For_IN agent_NN a_DT ,_, let_VB APRX-TRADEOFFA_NN -LRB-_-LRB- P_NN ,_, UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN ,_, -RRB-_-RRB- denote_VBP a_DT procedure_NN that_WDT returns_VBZ an_DT approximate_JJ solution_NN to_TO Equation_NN #_# using_VBG the_DT Ibarra_NNP and_CC Kim_NNP method_NN ._.
The_DT procedure_NN APRX-TRADEOFFB_NN -LRB-_-LRB- P_NN ,_, UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN ,_, -RRB-_-RRB- for_IN agent_NN b_NN is_VBZ analogous_JJ ._.
For_IN #_# c_NN m_NN ,_, the_DT approximate_JJ equilibrium_NN offer_NN for_IN issue_NN c_NN at_IN time_NN t_NN is_VBZ denoted_VBN as_IN -LSB-_-LRB- at_IN c_NN ,_, bt_NN c_NN -RSB-_-RRB- where_WRB at_IN c_NN and_CC bt_NN c_NN denote_VBP the_DT shares_NNS for_IN agent_NN a_DT and_CC b_NN respectively_RB ._.
We_PRP denote_VBP the_DT equilibrium_NN package_NN at_IN time_NN t_NN as_IN -LSB-_-LRB- at_IN ,_, bt_NN -RSB-_-RRB- where_WRB at_IN Bm_NN -LRB-_-LRB- bt_NN Bm_NN -RRB-_-RRB- is_VBZ an_DT m_NN element_NN vector_NN that_WDT denotes_VBZ a_DT ''_'' s_NNS -LRB-_-LRB- b_NN ''_'' s_NNS -RRB-_-RRB- share_NN for_IN each_DT of_IN the_DT m_NN issues_NNS ._.
Also_RB ,_, as_IN before_RB ,_, for_IN #_# c_NN m_NN ,_, c_NN is_VBZ the_DT discount_NN factor_NN for_IN issue_NN c_NN ._.
Note_VB that_DT for_IN #_# t_NN n_NN ,_, at_IN c_NN +_CC bt_NN c_NN =_JJ #_# -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, the_DT sum_NN of_IN the_DT agents_NNS ''_'' shares_NNS -LRB-_-LRB- at_IN time_NN t_NN -RRB-_-RRB- for_IN each_DT pie_NN is_VBZ one_CD -RRB-_-RRB- ._.
Finally_RB ,_, for_IN time_NN period_NN t_NN -LRB-_-LRB- for_IN 1_CD t_NN n_NN -RRB-_-RRB- we_PRP let_VBD A_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -LRB-_-LRB- respectively_RB B_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- denote_VBP the_DT approximate_JJ equilibrium_NN strategy_NN for_IN agent_NN a_DT -LRB-_-LRB- respectively_RB b_LS -RRB-_-RRB- ._.
The_DT following_VBG theorem_NN uses_VBZ this_DT notation_NN and_CC characterizes_VBZ an_DT approximate_JJ equilibrium_NN for_IN multi-issue_JJ negotiation_NN ._.
THEOREM_NNP #_# ._.
For_IN the_DT package_NN deal_NN procedure_NN ,_, the_DT following_VBG strategies_NNS form_VBP an_DT approximate_JJ Nash_NNP equilibrium_NN ._.
The_DT equilibrium_NN strategy_NN for_IN t_NN =_JJ n_NN is_VBZ :_: A_DT -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN a_DT ''_'' s_VBZ TURN_NNP ACCEPT_NNP IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN b_NN ''_'' s_NNS TURN_VBP ACCEPT_NNP IF_IN a_DT ''_'' s_VBZ TURN_NN For_IN all_DT preceding_VBG time_NN periods_NNS t_NN <_JJR n_NN ,_, if_IN -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- denotes_VBZ the_DT offer_NN made_VBN at_IN time_NN t_NN ,_, then_RB the_DT equilibrium_NN strategies_NNS are_VBP defined_VBN as_IN follows_VBZ :_: A_DT -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ 8_CD <_JJR :_: OFFER_NN APRX-TRADEOFFA_NN -LRB-_-LRB- P_NN ,_, UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN ,_, -RRB-_-RRB- IF_IN a_DT ''_'' s_VBZ TURN_NNP If_IN -LRB-_-LRB- Ua_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- ACCEPT_VBP else_RB REJECT_VB IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ 8_CD <_JJR :_: OFFER_NN APRX-TRADEOFFB_NN -LRB-_-LRB- P_NN ,_, UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN ,_, -RRB-_-RRB- IF_IN b_NN ''_'' s_NNS TURN_VBP If_IN -LRB-_-LRB- Ub_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- ACCEPT_VBP else_RB REJECT_VB IF_IN a_DT ''_'' s_NNS TURN_VBP where_WRB UA_NNP -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ Ua_NN -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- and_CC UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ Ub_NN -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ._.
An_DT agreement_NN takes_VBZ place_NN at_IN t_NN =_JJ #_# ._.
PROOF_NN ._.
As_IN in_IN the_DT proof_NN for_IN Theorem_NNP #_# ,_, we_PRP use_VBP backward_RB reasoning_NN ._.
We_PRP first_RB obtain_VB the_DT strategies_NNS for_IN the_DT last_JJ time_NN period_NN t_NN =_JJ n_NN ._.
It_PRP is_VBZ straightforward_JJ to_TO get_VB these_DT strategies_NNS ;_: the_DT offering_NN agent_NN gets_VBZ a_DT hundred_CD percent_NN of_IN all_PDT the_DT issues_NNS ._.
Then_RB for_IN t_NN =_JJ n_NN #_# ,_, the_DT offering_NN agent_NN must_MD solve_VB the_DT maximization_NN problem_NN of_IN Equation_NN #_# by_IN substituting_VBG t_NN =_JJ n1_NN in_IN it_PRP ._.
For_IN agent_NN a_DT -LRB-_-LRB- b_NN -RRB-_-RRB- ,_, this_DT is_VBZ done_VBN by_IN APPROX-TRADEOFFA_NN -LRB-_-LRB- APPROX-TRADEOFFB_NN -RRB-_-RRB- ._.
These_DT two_CD functions_NNS are_VBP nothing_NN but_CC the_DT Ibarra_NNP and_CC Kim_NNP ''_'' s_VBZ approximation_NN method_NN for_IN solving_VBG the_DT 0-1_CD knapsack_NN problem_NN ._.
These_DT two_CD functions_NNS take_VBP as_IN a_DT parameter_NN and_CC use_VB the_DT Ibarra_NNP and_CC Kim_NNP ''_'' s_VBZ approximation_NN method_NN to_TO return_VB a_DT package_NN that_WDT approximately_RB maximizes_VBZ Equation_NN #_# ._.
Thus_RB ,_, the_DT relative_JJ error_NN for_IN these_DT two_CD functions_NNS is_VBZ the_DT same_JJ as_IN that_DT for_IN Ibarra_NNP and_CC Kim_NNP ''_'' s_VBZ method_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, it_PRP is_VBZ at_IN most_JJS where_WRB is_VBZ given_VBN in_IN Equation_NN #_# -RRB-_-RRB- ._.
Assume_VB that_IN a_DT is_VBZ the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN #_# ._.
Agent_NNP a_DT must_MD offer_VB a_DT package_NN that_WDT gives_VBZ b_NN a_DT cumulative_JJ utility_NN equal_JJ to_TO what_WP it_PRP would_MD get_VB from_IN its_PRP$ own_JJ approximate_JJ equilibrium_NN offer_NN for_IN the_DT next_JJ time_NN period_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, Ub_NNP -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- where_WRB -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- is_VBZ the_DT approximate_JJ equilibrium_NN package_NN for_IN the_DT next_JJ time_NN period_NN -RRB-_-RRB- ._.
Recall_VB that_DT for_IN the_DT last_JJ time_NN period_NN ,_, the_DT offering_NN agent_NN gets_VBZ a_DT hundred_CD percent_NN of_IN all_PDT the_DT issues_NNS ._.
Since_IN a_DT is_VBZ the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN #_# and_CC the_DT agents_NNS use_VBP the_DT alternating_VBG offers_NNS protocol_NN ,_, it_PRP is_VBZ b_NN ''_'' s_NNS turn_VBP at_IN t_NN =_JJ n_NN ._.
Thus_RB Ub_NNP -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- is_VBZ equal_JJ to_TO b_NN ''_'' s_NNS cumulative_JJ utility_NN from_IN receiving_VBG a_DT hundred_CD percent_NN of_IN all_PDT the_DT issues_NNS ._.
Using_VBG this_DT utility_NN as_IN the_DT capacity_NN of_IN the_DT knapsack_NN ,_, a_DT uses_NNS APPROX-TRADEOFFA_NN and_CC obtains_VBZ the_DT approximate_JJ equilibrium_NN package_NN for_IN t_NN =_JJ n_NN #_# ._.
On_IN the_DT other_JJ hand_NN ,_, if_IN b_NN is_VBZ the_DT offering_NN agent_NN at_IN t_NN =_JJ n_NN #_# ,_, it_PRP uses_VBZ APPROX-TRADEOFFB_NN to_TO obtain_VB the_DT approximate_JJ equilibrium_NN package_NN ._.
In_IN the_DT same_JJ way_NN for_IN t_NN <_JJR n_NN #_# ,_, the_DT offering_NN agent_NN -LRB-_-LRB- say_VB a_DT -RRB-_-RRB- uses_VBZ APPROX-TRADEOFFA_NN to_TO find_VB an_DT approximate_JJ equilibrium_NN package_NN that_WDT gives_VBZ b_NN a_DT utility_NN of_IN Ub_NNP -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ._.
By_IN reasoning_NN backwards_RB ,_, we_PRP obtain_VBP the_DT offer_NN for_IN time_NN period_NN t_NN =_JJ #_# ._.
If_IN a_DT -LRB-_-LRB- b_NN -RRB-_-RRB- is_VBZ the_DT offering_NN agent_NN ,_, it_PRP proposes_VBZ the_DT offer_NN APPROX-TRADEOFFA_NN -LRB-_-LRB- P_NN ,_, UB_NNP -LRB-_-LRB- #_# -RRB-_-RRB- ,_, #_# ,_, -RRB-_-RRB- -LRB-_-LRB- APPROX-TRADEOFFB_NN -LRB-_-LRB- P_NN ,_, UA_NN -LRB-_-LRB- #_# -RRB-_-RRB- ,_, #_# ,_, -RRB-_-RRB- -RRB-_-RRB- ._.
The_DT receiving_VBG agent_NN accepts_VBZ the_DT offer_NN ._.
This_DT is_VBZ because_IN the_DT relative_JJ error_NN in_IN its_PRP$ cumulative_JJ utility_NN from_IN the_DT offer_NN is_VBZ at_IN most_JJS ._.
An_DT agreement_NN therefore_RB takes_VBZ place_NN in_IN the_DT first_JJ time_NN period_NN ._.
THEOREM_NNP #_# ._.
The_DT time_NN complexity_NN of_IN finding_VBG the_DT approximate_JJ equilibrium_NN offer_NN for_IN the_DT first_JJ time_NN period_NN is_VBZ O_NN -LRB-_-LRB- nm_NN /_: #_# -RRB-_-RRB- ._.
PROOF_NN ._.
The_DT time_NN complexity_NN of_IN APPROX-TRADEOFFA_NN and_CC APPROXTRADEOFFB_NN is_VBZ the_DT same_JJ as_IN the_DT time_NN complexity_NN of_IN the_DT Ibarra_NNP and_CC Kim_NNP method_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- i_LS ._.
e_LS ._.
,_, O_NN -LRB-_-LRB- m_NN /_: #_# -RRB-_-RRB- -RRB-_-RRB- ._.
In_IN order_NN to_TO find_VB the_DT equilibrium_NN offer_NN for_IN the_DT first_JJ time_NN period_NN using_VBG backward_RB reasoning_NN ,_, APPROXTRADEOFFA_NN -LRB-_-LRB- or_CC APPROX_NNP -_: TRADEOFFB_NN -RRB-_-RRB- is_VBZ invoked_VBN n_NN times_NNS ._.
Hence_RB the_DT time_NN complexity_NN of_IN finding_VBG the_DT approximate_JJ equilibrium_NN offer_NN for_IN the_DT first_JJ time_NN period_NN is_VBZ O_NN -LRB-_-LRB- nm_NN /_: #_# -RRB-_-RRB- ._.
This_DT analysis_NN was_VBD done_VBN in_IN a_DT complete_JJ information_NN setting_NN ._.
However_RB an_DT extension_NN of_IN this_DT analysis_NN to_TO an_DT incomplete_JJ information_NN setting_VBG where_WRB the_DT agents_NNS have_VBP probability_NN distributions_NNS over_IN some_DT uncertain_JJ parameter_NN is_VBZ straightforward_JJ ,_, as_RB long_RB as_IN the_DT negotiation_NN is_VBZ done_VBN offline_NN ;_: i_LS ._.
e_LS ._.
,_, the_DT agents_NNS know_VBP their_PRP$ preference_NN for_IN each_DT individual_JJ issue_NN before_IN negotiation_NN begins_VBZ ._.
For_IN instance_NN ,_, consider_VB the_DT case_NN where_WRB different_JJ agents_NNS have_VBP different_JJ discount_NN factors_NNS ,_, and_CC each_DT agent_NN is_VBZ uncertain_JJ about_IN its_PRP$ opponent_NN ''_'' s_NNS discount_NN factor_NN although_IN it_PRP knows_VBZ its_PRP$ own_JJ ._.
This_DT uncertainty_NN is_VBZ modelled_VBN with_IN a_DT probability_NN distribution_NN over_IN the_DT possible_JJ values_NNS for_IN the_DT opponent_NN ''_'' s_NNS discount_NN factor_NN and_CC having_VBG this_DT distribution_NN as_IN common_JJ knowledge_NN to_TO the_DT agents_NNS ._.
All_DT our_PRP$ analysis_NN for_IN the_DT complete_JJ information_NN setting_NN still_RB holds_VBZ for_IN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD this_DT incomplete_JJ information_NN setting_NN ,_, except_IN for_IN the_DT fact_NN that_IN an_DT agent_NN must_MD now_RB use_VB the_DT given_VBN probability_NN distribution_NN and_CC find_VB its_PRP$ opponent_NN ''_'' s_NNS expected_VBN utility_NN instead_RB of_IN its_PRP$ actual_JJ utility_NN ._.
Hence_RB ,_, instead_RB of_IN analyzing_VBG an_DT incomplete_JJ information_NN setting_VBG for_IN offline_JJ negotiation_NN ,_, we_PRP focus_VBP on_IN online_JJ multi-issue_JJ negotiation_NN ._.
4_LS ._.
ONLINE_NNP MULTI-ISSUE_NNP NEGOTIATION_NNP We_PRP now_RB consider_VBP a_DT more_RBR general_JJ and_CC ,_, arguably_RB more_RBR realistic_JJ ,_, version_NN of_IN multi-issue_JJ negotiation_NN ,_, where_WRB the_DT agents_NNS are_VBP uncertain_JJ about_IN the_DT issues_NNS they_PRP will_MD have_VB to_TO negotiate_VB about_RB in_IN future_NN ._.
In_IN this_DT setting_NN ,_, when_WRB negotiating_VBG an_DT issue_NN ,_, the_DT agents_NNS know_VBP that_IN they_PRP will_MD negotiate_VB more_RBR issues_NNS in_IN the_DT future_NN ,_, but_CC they_PRP are_VBP uncertain_JJ about_IN the_DT details_NNS of_IN those_DT issues_NNS ._.
As_IN before_RB ,_, let_VB m_NN be_VB the_DT total_JJ number_NN of_IN issues_NNS that_WDT are_VBP up_RB for_IN negotiation_NN ._.
The_DT agents_NNS have_VBP a_DT probability_NN distribution_NN over_IN the_DT possible_JJ values_NNS of_IN ka_FW c_NN and_CC kb_NN c_NN ._.
For_IN #_# c_NN m_NN let_VB ka_FW c_NN and_CC kb_NN c_NN be_VB uniformly_RB distributed_VBN over_IN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
This_DT probability_NN distribution_NN ,_, n_NN ,_, and_CC m_NN are_VBP common_JJ knowledge_NN to_TO the_DT agents_NNS ._.
However_RB ,_, the_DT agents_NNS come_VBP to_TO know_VB ka_FW c_NN and_CC kb_NN c_NN only_RB just_RB before_IN negotiation_NN for_IN issue_NN c_NN begins_VBZ ._.
Once_RB the_DT agents_NNS reach_VBP an_DT agreement_NN on_IN issue_NN c_NN ,_, it_PRP can_MD not_RB be_VB re-negotiated_VBN ._.
This_DT scenario_NN requires_VBZ online_JJ negotiation_NN since_IN the_DT agents_NNS must_MD make_VB decisions_NNS about_IN an_DT issue_NN prior_RB to_TO having_VBG the_DT information_NN about_IN the_DT future_JJ issues_NNS -LSB-_-LRB- #_# -RSB-_-RRB- ._.
We_PRP first_RB give_VB a_DT brief_JJ introduction_NN to_TO online_JJ problems_NNS and_CC then_RB draw_VB an_DT analogy_NN between_IN the_DT online_JJ knapsack_NN problem_NN and_CC the_DT negotiation_NN problem_NN we_PRP want_VBP to_TO solve_VB ._.
In_IN an_DT online_JJ problem_NN ,_, data_NNS is_VBZ given_VBN to_TO the_DT algorithm_NN incrementally_RB ,_, one_CD unit_NN at_IN a_DT time_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
The_DT online_JJ algorithm_NN must_MD also_RB produce_VB the_DT output_NN incrementally_RB :_: after_IN seeing_VBG i_FW units_NNS of_IN input_NN it_PRP must_MD output_NN the_DT ith_NN unit_NN of_IN output_NN ._.
Since_IN decisions_NNS about_IN the_DT output_NN are_VBP made_VBN with_IN incomplete_JJ knowledge_NN about_IN the_DT entire_JJ input_NN ,_, an_DT online_JJ algorithm_NN often_RB can_MD not_RB produce_VB an_DT optimal_JJ solution_NN ._.
Such_PDT an_DT algorithm_NN can_MD only_RB approximate_JJ the_DT performance_NN of_IN the_DT optimal_JJ algorithm_NN that_WDT sees_VBZ all_PDT the_DT inputs_NNS in_IN advance_NN ._.
In_IN the_DT design_NN of_IN online_JJ algorithms_NNS ,_, the_DT main_JJ aim_NN is_VBZ to_TO achieve_VB a_DT performance_NN that_WDT is_VBZ close_JJ to_TO that_DT of_IN the_DT optimal_JJ offline_JJ algorithm_NN on_IN each_DT input_NN ._.
An_DT online_JJ algorithm_NN is_VBZ said_VBN to_TO be_VB stochastic_JJ if_IN it_PRP makes_VBZ decisions_NNS on_IN the_DT basis_NN of_IN the_DT probability_NN distributions_NNS for_IN the_DT future_JJ inputs_NNS ._.
The_DT performance_NN of_IN stochastic_JJ online_JJ algorithms_NNS is_VBZ assessed_VBN in_IN terms_NNS of_IN the_DT expected_VBN difference_NN between_IN the_DT optimum_NN and_CC the_DT approximate_JJ solution_NN -LRB-_-LRB- denoted_VBN E_NN -LSB-_-LRB- z_SYM m_NN zm_NN -RSB-_-RRB- where_WRB z_SYM m_NN is_VBZ the_DT optimal_JJ and_CC zm_NN the_DT approximate_JJ solution_NN -RRB-_-RRB- ._.
Note_VB that_IN the_DT subscript_JJ m_NN is_VBZ used_VBN to_TO indicate_VB the_DT fact_NN that_IN this_DT difference_NN depends_VBZ on_IN m_NN ._.
We_PRP now_RB describe_VBP the_DT protocol_NN for_IN online_JJ negotiation_NN and_CC then_RB obtain_VB an_DT approximate_JJ equilibrium_NN ._.
The_DT protocol_NN is_VBZ defined_VBN as_IN follows_VBZ ._.
Let_VB agent_NN a_DT denote_VBP the_DT first_JJ mover_NN -LRB-_-LRB- since_IN we_PRP focus_VBP on_IN the_DT package_NN deal_NN procedure_NN ,_, the_DT first_JJ mover_NN is_VBZ the_DT same_JJ for_IN all_PDT the_DT m_NN issues_NNS -RRB-_-RRB- ._.
Step_NN #_# ._.
For_IN c_NN =_JJ #_# ,_, the_DT agents_NNS are_VBP given_VBN the_DT values_NNS of_IN ka_FW c_NN and_CC kb_NN c_NN ._.
These_DT two_CD values_NNS are_VBP now_RB common5_JJ knowledge_NN ._.
Step_NN #_# ._.
The_DT agents_NNS settle_VBP issue_NN c_NN using_VBG the_DT alternating_VBG offers_NNS protocol_NN described_VBN in_IN Section_NN #_# ._.
Negotiation_NN for_IN issue_NN c_NN must_MD end_VB within_IN n_NN time_NN periods_NNS from_IN the_DT start_NN of_IN negotiation_NN on_IN the_DT issue_NN ._.
If_IN an_DT agreement_NN is_VBZ not_RB reached_VBN within_IN this_DT time_NN ,_, then_RB negotiation_NN fails_VBZ on_IN this_DT and_CC on_IN all_DT remaining_VBG issues_NNS ._.
Step_NN #_# ._.
The_DT above_JJ steps_NNS are_VBP repeated_VBN for_IN issues_NNS c_NN =_JJ #_# ,_, #_# ,_, ..._: ,_, m_NN ._.
Negotiation_NN for_IN issue_NN c_NN -LRB-_-LRB- #_# c_NN m_NN -RRB-_-RRB- begins_VBZ in_IN the_DT time_NN period_NN following_VBG an_DT agreement_NN on_IN issue_NN c_NN #_# ._.
5_CD We_PRP assume_VBP common_JJ knowledge_NN because_IN it_PRP simplifies_VBZ exposition_NN ._.
However_RB ,_, if_IN ka_FW c_NN -LRB-_-LRB- kb_NN c_NN -RRB-_-RRB- is_VBZ a_DT ''_'' s_NNS -LRB-_-LRB- b_NN ''_'' s_NNS -RRB-_-RRB- private_JJ knowledge_NN ,_, then_RB our_PRP$ analysis_NN will_MD still_RB hold_VB but_CC now_RB an_DT agent_NN must_MD find_VB its_PRP$ opponent_NN ''_'' s_NNS expected_VBN utility_NN on_IN the_DT basis_NN of_IN the_DT p_NN ._.
d_NN ._.
fs_NN for_IN ka_FW c_NN and_CC kb_NN c_NN ._.
Thus_RB ,_, during_IN time_NN period_NN t_NN ,_, the_DT problem_NN for_IN the_DT offering_NN agent_NN -LRB-_-LRB- say_VB a_DT -RRB-_-RRB- is_VBZ to_TO find_VB the_DT optimal_JJ offer_NN for_IN issue_NN c_NN on_IN the_DT basis_NN of_IN ka_FW c_NN and_CC kb_NN c_NN and_CC the_DT probability_NN distribution_NN for_IN ka_FW i_FW and_CC kb_NN i_FW -LRB-_-LRB- c_NN <_JJR i_FW m_NN -RRB-_-RRB- ._.
In_IN order_NN to_TO solve_VB this_DT online_JJ negotiation_NN problem_NN we_PRP draw_VBP analogy_NN with_IN the_DT online_JJ knapsack_NN problem_NN ._.
Before_IN doing_VBG so_RB ,_, however_RB ,_, we_PRP give_VBP a_DT brief_JJ overview_NN of_IN the_DT online_JJ knapsack_NN problem_NN ._.
In_IN the_DT online_JJ knapsack_NN problem_NN ,_, there_EX are_VBP m_NN items_NNS ._.
The_DT agent_NN must_MD examine_VB the_DT m_NN items_NNS one_CD at_IN a_DT time_NN according_VBG to_TO the_DT order_NN they_PRP are_VBP input_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, as_IN their_PRP$ profit_NN and_CC size_NN coefficients_NNS become_VBP known_VBN -RRB-_-RRB- ._.
Hence_RB ,_, the_DT algorithm_NN is_VBZ required_VBN to_TO decide_VB whether_IN or_CC not_RB to_TO include_VB each_DT item_NN in_IN the_DT knapsack_NN as_RB soon_RB as_IN its_PRP$ weight_NN and_CC profit_NN become_VBP known_VBN ,_, without_IN knowledge_NN concerning_VBG the_DT items_NNS still_RB to_TO be_VB seen_VBN ,_, except_IN for_IN their_PRP$ total_JJ number_NN ._.
Note_VB that_DT since_IN the_DT agents_NNS have_VBP a_DT probability_NN distribution_NN over_IN the_DT weights_NNS and_CC profits_NNS of_IN the_DT future_JJ items_NNS ,_, this_DT is_VBZ a_DT case_NN of_IN stochastic_JJ online_JJ knapsack_NN problem_NN ._.
Our_PRP$ online_JJ negotiation_NN problem_NN is_VBZ analogous_JJ to_TO the_DT online_JJ knapsack_NN problem_NN ._.
This_DT analogy_NN is_VBZ described_VBN in_IN detail_NN in_IN the_DT proof_NN for_IN Theorem_NNP #_# ._.
Again_RB ,_, researchers_NNS in_IN algorithms_NNS have_VBP developed_VBN time_NN efficient_JJ approximate_JJ solutions_NNS to_TO the_DT online_JJ knapsack_NN problem_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Hence_RB we_PRP use_VBP this_DT solution_NN and_CC show_VBP that_IN it_PRP forms_VBZ an_DT equilibrium_NN ._.
The_DT following_VBG theorem_NN characterizes_VBZ an_DT approximate_JJ equilibrium_NN for_IN online_JJ negotiation_NN ._.
Here_RB the_DT agents_NNS have_VBP to_TO choose_VB a_DT strategy_NN without_IN knowing_VBG the_DT features_NNS of_IN the_DT future_JJ issues_NNS ._.
Because_IN of_IN this_DT information_NN incompleteness_NN ,_, the_DT relevant_JJ equilibrium_NN solution_NN is_VBZ that_IN of_IN a_DT Bayes_NNP ''_'' Nash_NNP Equilibrium_NNP -LRB-_-LRB- BNE_NNP -RRB-_-RRB- in_IN which_WDT each_DT agent_NN plays_VBZ the_DT best_JJS response_NN to_TO the_DT other_JJ agents_NNS with_IN respect_NN to_TO their_PRP$ expected_VBN utilities_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
However_RB ,_, finding_VBG an_DT agent_NN ''_'' s_NNS BNE_NNP strategy_NN is_VBZ analogous_JJ to_TO solving_VBG the_DT online_JJ 0-1_CD knapsack_NN problem_NN ._.
Also_RB ,_, the_DT online_JJ knapsack_NN can_MD only_RB be_VB solved_VBN approximately_RB -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Hence_RB the_DT relevant_JJ equilibrium_NN solution_NN concept_NN is_VBZ approximate_JJ BNE_NN -LRB-_-LRB- see_VB -LSB-_-LRB- ##_CD -RSB-_-RRB- for_IN example_NN -RRB-_-RRB- ._.
The_DT following_VBG theorem_NN finds_VBZ this_DT equilibrium_NN using_VBG procedures_NNS ONLINE_NNP -_: TRADEOFFA_NN and_CC ONLINE-TRADEOFFB_NN which_WDT are_VBP defined_VBN in_IN the_DT proof_NN of_IN the_DT theorem_NN ._.
For_IN a_DT given_VBN time_NN period_NN ,_, we_PRP let_VBD zm_NN denote_VBP the_DT approximately_RB optimal_JJ solution_NN generated_VBN by_IN ONLINE-TRADEOFFA_NN -LRB-_-LRB- or_CC ONLINE-TRADEOFFB_NN -RRB-_-RRB- and_CC z_SYM m_NN the_DT actual_JJ optimum_NN ._.
THEOREM_NNP #_# ._.
For_IN the_DT package_NN deal_NN procedure_NN ,_, the_DT following_VBG strategies_NNS form_VBP an_DT approximate_JJ Bayes_NNP ''_'' Nash_NNP equilibrium_NN ._.
The_DT equilibrium_NN strategy_NN for_IN t_NN =_JJ n_NN is_VBZ :_: A_DT -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN a_DT ''_'' s_VBZ TURN_NNP ACCEPT_NNP IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ j_NN OFFER_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- IF_IN b_NN ''_'' s_NNS TURN_VBP ACCEPT_NNP IF_IN a_DT ''_'' s_VBZ TURN_NN For_IN all_DT preceding_VBG time_NN periods_NNS t_NN <_JJR n_NN ,_, if_IN -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- denotes_VBZ the_DT offer_NN made_VBN at_IN time_NN t_NN ,_, then_RB the_DT equilibrium_NN strategies_NNS are_VBP defined_VBN as_IN follows_VBZ :_: A_DT -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ 8_CD <_JJR :_: OFFER_NN ONLINE-TRADEOFFA_NN -LRB-_-LRB- P_NN ,_, UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN -RRB-_-RRB- IF_IN a_DT ''_'' s_VBZ TURN_NNP If_IN -LRB-_-LRB- Ua_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- ACCEPT_VBP else_RB REJECT_VB IF_IN b_NN ''_'' s_NNS TURN_VBP B_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ 8_CD <_JJR :_: OFFER_NN ONLINE-TRADEOFFB_NN -LRB-_-LRB- P_NN ,_, UA_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, t_NN -RRB-_-RRB- IF_IN b_NN ''_'' s_NNS TURN_VBP If_IN -LRB-_-LRB- Ub_NN -LRB-_-LRB- -LSB-_-LRB- xt_NN ,_, yt_NN -RSB-_-RRB- ,_, t_NN -RRB-_-RRB- UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- ACCEPT_VBP else_RB REJECT_VB IF_IN a_DT ''_'' s_NNS TURN_VBP where_WRB UA_NNP -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ Ua_NN -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- and_CC UB_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ Ub_NN -LRB-_-LRB- -LSB-_-LRB- at_IN +_CC #_# ,_, bt_NN +_CC #_# -RSB-_-RRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ._.
An_DT agreement_NN on_IN issue_NN c_NN takes_VBZ place_NN at_IN t_NN =_JJ c_NN ._.
For_IN a_DT given_VBN time_NN period_NN ,_, the_DT expected_VBN difference_NN between_IN the_DT solution_NN generated_VBN by_IN the_DT optimal_JJ strategy_NN and_CC that_IN by_IN the_DT approximate_JJ strategy_NN is_VBZ E_NN -LSB-_-LRB- z_SYM m_NN zm_NN -RSB-_-RRB- =_JJ O_NN -LRB-_-LRB- m_NN -RRB-_-RRB- ._.
956_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- PROOF_NN ._.
As_IN in_IN Theorem_NNP #_# we_PRP find_VBP the_DT equilibrium_NN offer_NN for_IN time_NN period_NN t_NN =_JJ #_# using_VBG backward_RB induction_NN ._.
Let_VB a_DT be_VB the_DT offering_NN agent_NN for_IN t_NN =_JJ #_# for_IN all_PDT the_DT m_NN issues_NNS ._.
Consider_VB the_DT last_JJ time_NN period_NN t_NN =_JJ n_NN -LRB-_-LRB- recall_NN from_IN Step_NN #_# of_IN the_DT online_NN protocol_NN that_WDT n_NN is_VBZ the_DT deadline_NN for_IN completing_VBG negotiation_NN on_IN the_DT first_JJ issue_NN -RRB-_-RRB- ._.
Since_IN the_DT first_JJ mover_NN is_VBZ the_DT same_JJ for_IN all_PDT the_DT issues_NNS ,_, and_CC the_DT agents_NNS make_VBP offers_NNS alternately_RB ,_, the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN is_VBZ also_RB the_DT same_JJ for_IN all_PDT the_DT m_NN issues_NNS ._.
Assume_VB that_DT b_NN is_VBZ the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN ._.
As_IN in_IN Section_NN #_# ,_, the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN gets_VBZ a_DT hundred_CD percent_NN of_IN all_PDT the_DT m_NN issues_NNS ._.
Since_IN b_NN is_VBZ the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN ,_, his_PRP$ utility_NN for_IN this_DT time_NN period_NN is_VBZ :_: UB_NN -LRB-_-LRB- n_NN -RRB-_-RRB- =_JJ kb_NN 1n1_NN 1_CD +_CC #_# /_: #_# mX_NNP i_FW =_JJ #_# i_FW -LRB-_-LRB- n1_NN -RRB-_-RRB- i_FW -LRB-_-LRB- #_# -RRB-_-RRB- Recall_VBP that_IN ka_FW i_FW and_CC kb_NN i_FW -LRB-_-LRB- for_IN c_NN <_JJR i_FW m_NN -RRB-_-RRB- are_VBP not_RB known_VBN to_TO the_DT agents_NNS ._.
Hence_RB ,_, the_DT agents_NNS can_MD only_RB find_VB their_PRP$ expected_VBN utilities_NNS from_IN the_DT future_JJ issues_NNS on_IN the_DT basis_NN of_IN the_DT probability_NN distribution_NN functions_NNS for_IN ka_FW i_FW and_CC kb_NN i_FW ._.
However_RB ,_, during_IN the_DT negotiation_NN for_IN issue_NN c_NN the_DT agents_NNS know_VBP ka_FW c_NN but_CC not_RB kb_NN c_NN ._.
Hence_RB ,_, a_DT computes_VBZ UB_NNP -LRB-_-LRB- n_NN -RRB-_-RRB- as_IN follows_VBZ ._.
Agent_NNP b_NN ''_'' s_NNS utility_NN from_IN issue_NN c_NN =_JJ #_# is_VBZ kb_NN 1n1_NN 1_CD -LRB-_-LRB- which_WDT is_VBZ the_DT first_JJ term_NN of_IN Equation_NN #_# -RRB-_-RRB- ._.
Then_RB ,_, on_IN the_DT basis_NN of_IN the_DT probability_NN distribution_NN functions_NNS for_IN ka_FW i_FW and_CC kb_NN i_FW ,_, agent_NN a_DT computes_NN b_NN ''_'' s_NNS expected_VBN utility_NN from_IN each_DT future_JJ issue_NN i_FW as_IN i_FW -LRB-_-LRB- n1_NN -RRB-_-RRB- i_FW /_: #_# -LRB-_-LRB- since_IN ka_FW i_FW and_CC kb_NN i_FW are_VBP uniformly_RB distributed_VBN on_IN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- -RRB-_-RRB- ._.
Thus_RB ,_, b_NN ''_'' s_NNS expected_VBN cumulative_JJ utility_NN from_IN these_DT m_NN c_NN issues_NNS is_VBZ 1_CD /_: #_# Pm_FW i_FW =_JJ #_# i_FW -LRB-_-LRB- n1_NN -RRB-_-RRB- i_FW -LRB-_-LRB- which_WDT is_VBZ the_DT second_JJ term_NN of_IN Equation_NN #_# -RRB-_-RRB- ._.
Now_RB ,_, in_IN order_NN to_TO decide_VB what_WP to_TO offer_VB for_IN issue_NN c_NN =_JJ #_# ,_, the_DT offering_NN agent_NN for_IN t_NN =_JJ n_NN #_# -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, agent_NN a_DT -RRB-_-RRB- must_MD solve_VB the_DT following_VBG online_JJ knapsack_NN problem_NN :_: maximize_VB m_NN i_FW =_JJ 1ka_FW i_FW -LRB-_-LRB- #_# bt_FW i_FW -RRB-_-RRB- n1_NN i_FW -LRB-_-LRB- #_# -RRB-_-RRB- such_JJ that_IN m_NN i_FW =_JJ 1kb_JJ i_FW bt_FW i_FW UB_NN -LRB-_-LRB- n_NN -RRB-_-RRB- bt_NN i_FW =_JJ #_# or_CC #_# for_IN #_# i_FW m_NN The_DT only_JJ variables_NNS in_IN the_DT above_JJ maximization_NN problem_NN are_VBP bt_NN i_FW ._.
Now_RB ,_, maximizing_VBG m_NN i_FW =_JJ 1ka_FW i_FW -LRB-_-LRB- 1bt_JJ i_LS -RRB-_-RRB- n1_NN i_FW is_VBZ the_DT same_JJ as_IN minimizing_VBG m_NN i_FW =_JJ 1ka_FW i_FW bt_FW i_FW since_IN n1_NN i_FW and_CC ka_FW i_FW are_VBP constants_NNS ._.
Thus_RB ,_, we_PRP write_VBP Equation_NN #_# as_IN :_: minimize_VB m_NN i_FW =_JJ 1ka_FW i_FW bt_FW i_FW -LRB-_-LRB- #_# -RRB-_-RRB- such_JJ that_IN m_NN i_FW =_JJ 1kb_JJ i_FW bt_FW i_FW UB_NN -LRB-_-LRB- n_NN -RRB-_-RRB- bt_NN i_FW =_JJ #_# or_CC #_# for_IN #_# i_FW m_NN The_DT above_IN optimization_NN problem_NN is_VBZ analogous_JJ to_TO the_DT online_JJ 0-1_CD knapsack_NN problem_NN ._.
An_DT algorithm_NN to_TO solve_VB the_DT online_JJ knapsack_NN problem_NN has_VBZ already_RB proposed_VBN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ._.
This_DT algorithm_NN is_VBZ called_VBN the_DT fixed-choice_JJ online_NN algorithm_NN ._.
It_PRP has_VBZ time_NN complexity_NN linear_NN in_IN the_DT number_NN of_IN items_NNS -LRB-_-LRB- m_NN -RRB-_-RRB- in_IN the_DT knapsack_NN problem_NN ._.
We_PRP use_VBP this_DT to_TO solve_VB our_PRP$ online_JJ negotiation_NN problem_NN ._.
Thus_RB ,_, our_PRP$ ONLINE-TRADEOFFA_NN algorithm_NN is_VBZ nothing_NN but_CC the_DT fixed-choice_JJ online_NN algorithm_NN and_CC therefore_RB has_VBZ the_DT same_JJ time_NN complexity_NN as_IN the_DT latter_JJ ._.
This_DT algorithm_NN takes_VBZ the_DT values_NNS of_IN ka_FW i_FW and_CC kb_NN i_FW one_CD at_IN a_DT time_NN and_CC generates_VBZ an_DT approximate_JJ solution_NN to_TO the_DT above_JJ knapsack_NN problem_NN ._.
The_DT expected_VBN difference_NN between_IN the_DT optimum_NN and_CC approximate_JJ solution_NN is_VBZ E_NN -LSB-_-LRB- z_SYM m_NN zm_NN -RSB-_-RRB- =_JJ O_NN -LRB-_-LRB- m_NN -RRB-_-RRB- -LSB-_-LRB- ##_CD -RSB-_-RRB- -LRB-_-LRB- see_VB -LSB-_-LRB- ##_CD -RSB-_-RRB- for_IN the_DT detailed_JJ fixed-choice_JJ online_NN algorithm_NN and_CC a_DT proof_NN for_IN E_NN -LSB-_-LRB- z_SYM m_NN zm_NN -RSB-_-RRB- =_JJ O_NN -LRB-_-LRB- m_NN -RRB-_-RRB- -RRB-_-RRB- ._.
The_DT fixed-choice_JJ online_NN algorithm_NN of_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- is_VBZ a_DT generalization_NN of_IN the_DT basic_JJ greedy_JJ algorithm_NN for_IN the_DT offline_JJ knapsack_NN problem_NN ;_: the_DT idea_NN behind_IN it_PRP is_VBZ as_IN follows_VBZ ._.
A_DT threshold_NN value_NN is_VBZ determined_VBN on_IN the_DT basis_NN of_IN the_DT information_NN regarding_VBG weights_NNS and_CC profits_NNS for_IN the_DT 0-1_CD knapsack_NN problem_NN ._.
The_DT method_NN then_RB includes_VBZ into_IN the_DT knapsack_NN all_DT items_NNS whose_WP$ profit_NN density_NN -LRB-_-LRB- profit_NN density_NN of_IN an_DT item_NN is_VBZ its_PRP$ profit_NN per_IN unit_NN weight_NN -RRB-_-RRB- exceeds_VBZ the_DT threshold_NN until_IN either_CC the_DT knapsack_NN is_VBZ filled_VBN or_CC all_PDT the_DT m_NN items_NNS have_VBP been_VBN considered_VBN ._.
In_IN more_JJR detail_NN ,_, the_DT algorithm_NN ONLINE-TRADEOFFA_NN works_VBZ as_IN follows_VBZ ._.
It_PRP first_RB gets_VBZ the_DT values_NNS of_IN ka_FW 1_CD and_CC kb_NN 1_CD and_CC finds_VBZ bt_NN c_NN ._.
Since_IN we_PRP have_VBP a_DT 0-1_CD knapsack_NN problem_NN ,_, bt_NN c_NN can_MD be_VB either_CC zero_CD or_CC one_CD ._.
Now_RB ,_, if_IN bt_NN c_NN =_JJ #_# for_IN t_NN =_JJ n_NN ,_, then_RB bt_NN c_NN must_MD be_VB one_CD for_IN #_# t_NN <_JJR n_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, a_DT must_MD offer_VB bt_NN c_NN =_JJ #_# at_IN t_NN =_JJ #_# -RRB-_-RRB- ._.
If_IN bt_NN c_NN =_JJ #_# for_IN t_NN =_JJ n_NN ,_, but_CC a_DT offers_NNS bt_VBP c_NN =_JJ #_# at_IN t_NN =_JJ #_# ,_, then_RB agent_NN b_NN gets_VBZ less_JJR utility_NN than_IN what_WP it_PRP expects_VBZ from_IN a_DT ''_'' s_VBZ offer_NN and_CC rejects_VBZ the_DT proposal_NN ._.
Thus_RB ,_, if_IN bt_NN c_NN =_JJ #_# for_IN t_NN =_JJ n_NN ,_, then_RB the_DT optimal_JJ strategy_NN for_IN a_DT is_VBZ to_TO offer_VB bt_NN c_NN =_JJ #_# at_IN t_NN =_JJ #_# ._.
Agent_NNP b_NN accepts_VBZ the_DT offer_NN ._.
Thus_RB ,_, negotiation_NN on_IN the_DT first_JJ issue_NN starts_VBZ at_IN t_NN =_JJ #_# and_CC an_DT agreement_NN on_IN it_PRP is_VBZ also_RB reached_VBN at_IN t_NN =_JJ #_# ._.
In_IN the_DT next_JJ time_NN period_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, t_NN =_JJ #_# -RRB-_-RRB- ,_, negotiation_NN proceeds_NNS to_TO the_DT next_JJ issue_NN ._.
The_DT deadline_NN for_IN the_DT second_JJ issue_NN is_VBZ n_NN time_NN periods_NNS from_IN the_DT start_NN of_IN negotiation_NN on_IN the_DT issue_NN ._.
For_IN c_NN =_JJ #_# ,_, the_DT algorithm_NN ONLINE-TRADEOFFA_NN is_VBZ given_VBN the_DT values_NNS of_IN ka_FW 2_CD and_CC kb_NN 2_CD and_CC finds_VBZ bt_NN c_NN as_IN described_VBN above_IN ._.
Agent_NNP offers_VBZ bc_NN at_IN t_NN =_JJ #_# and_CC b_NN accepts_VBZ ._.
Thus_RB ,_, negotiation_NN on_IN the_DT second_JJ issue_NN starts_VBZ at_IN t_NN =_JJ #_# and_CC an_DT agreement_NN on_IN it_PRP is_VBZ also_RB reached_VBN at_IN t_NN =_JJ #_# ._.
This_DT process_NN repeats_NNS for_IN the_DT remaining_VBG issues_NNS c_NN =_JJ #_# ,_, ..._: ,_, m_NN ._.
Thus_RB ,_, each_DT issue_NN is_VBZ agreed_VBN upon_IN in_IN the_DT same_JJ time_NN period_NN in_IN which_WDT it_PRP starts_VBZ ._.
As_IN negotiation_NN for_IN the_DT next_JJ issue_NN starts_VBZ in_IN the_DT following_JJ time_NN period_NN ,_, agreement_NN on_IN issue_NN i_FW occurs_VBZ at_IN time_NN t_NN =_JJ i_LS ._.
On_IN the_DT other_JJ hand_NN ,_, if_IN b_NN is_VBZ the_DT offering_NN agent_NN at_IN t_NN =_JJ #_# ,_, he_PRP uses_VBZ the_DT algorithm_NN ONLINE-TRADEOFFB_NN which_WDT is_VBZ defined_VBN analogously_RB ._.
Thus_RB ,_, irrespective_RB of_IN who_WP makes_VBZ the_DT first_JJ move_NN ,_, all_PDT the_DT m_NN issues_NNS are_VBP settled_VBN at_IN time_NN t_NN =_JJ m_NN ._.
THEOREM_NNP #_# ._.
The_DT time_NN complexity_NN of_IN finding_VBG the_DT approximate_JJ equilibrium_NN offers_NNS of_IN Theorem_NNP #_# is_VBZ linear_JJ in_IN m_NN ._.
PROOF_NN ._.
The_DT time_NN complexity_NN of_IN ONLINE-TRADEOFFA_NN and_CC ONLINETRADEOFFB_NN is_VBZ the_DT same_JJ as_IN the_DT time_NN complexity_NN of_IN the_DT fixed-choice_JJ online_NN algorithm_NN of_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ._.
Since_IN the_DT latter_JJ has_VBZ time_NN complexity_NN linear_NN in_IN m_NN ,_, the_DT time_NN complexity_NN of_IN ONLINE-TRADEOFFA_NN and_CC ONLINETRADEOFFB_NN is_VBZ also_RB linear_JJ in_IN m_NN ._.
It_PRP is_VBZ worth_JJ noting_VBG that_IN ,_, for_IN the_DT 0-1_CD knapsack_NN problem_NN ,_, the_DT lower_JJR bound_VBN on_IN the_DT expected_VBN difference_NN between_IN the_DT optimum_NN and_CC the_DT solution_NN found_VBN by_IN any_DT online_JJ algorithm_NN is_VBZ -LRB-_-LRB- #_# -RRB-_-RRB- -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Thus_RB ,_, it_PRP follows_VBZ that_IN this_DT lower_JJR bound_VBN also_RB holds_VBZ for_IN our_PRP$ negotiation_NN problem_NN ._.
5_CD ._.
RELATED_JJ WORK_VBP Work_NN on_IN multi-issue_JJ negotiation_NN can_MD be_VB divided_VBN into_IN two_CD main_JJ types_NNS :_: that_IN for_IN indivisible_JJ issues_NNS and_CC that_IN for_IN divisible_JJ issues_NNS ._.
We_PRP first_RB describe_VBP the_DT existing_VBG work_NN for_IN the_DT case_NN of_IN divisible_JJ issues_NNS ._.
Since_IN Schelling_VBG -LSB-_-LRB- ##_CD -RSB-_-RRB- first_RB noted_VBD that_IN the_DT outcome_NN of_IN negotiation_NN depends_VBZ on_IN the_DT choice_NN of_IN negotiation_NN procedure_NN ,_, much_JJ research_NN effort_NN has_VBZ been_VBN devoted_VBN to_TO the_DT study_NN of_IN different_JJ procedures_NNS for_IN negotiating_VBG multiple_JJ issues_NNS ._.
However_RB ,_, most_JJS of_IN this_DT work_NN has_VBZ focussed_VBN on_IN the_DT sequential_JJ procedure_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
For_IN this_DT procedure_NN ,_, a_DT key_JJ issue_NN is_VBZ the_DT negotiation_NN agenda_NN ._.
Here_RB the_DT term_NN agenda_NN refers_VBZ to_TO the_DT order_NN in_IN which_WDT the_DT issues_NNS are_VBP negotiated_VBN ._.
The_DT agenda_NN is_VBZ important_JJ because_IN each_DT agent_NN ''_'' s_NNS cumulative_JJ utility_NN depends_VBZ on_IN the_DT agenda_NN ;_: if_IN we_PRP change_VBP the_DT agenda_NN then_RB these_DT utilities_NNS change_VBP ._.
Hence_RB ,_, the_DT agents_NNS must_MD decide_VB what_WP agenda_NN they_PRP will_MD use_VB ._.
Now_RB ,_, the_DT agenda_NN can_MD be_VB decided_VBN before_IN negotiating_VBG the_DT issues_NNS -LRB-_-LRB- such_PDT an_DT agenda_NN is_VBZ called_VBN exogenous_JJ -RRB-_-RRB- or_CC it_PRP may_MD be_VB decided_VBN during_IN the_DT process_NN of_IN negotiation_NN -LRB-_-LRB- such_PDT an_DT agenda_NN is_VBZ called_VBN endogenous_JJ -RRB-_-RRB- ._.
For_IN instance_NN ,_, Fershtman_NN -LSB-_-LRB- #_# -RSB-_-RRB- analyze_VBP sequential_JJ negotiation_NN with_IN exogenous_JJ agenda_NN ._.
A_DT number_NN of_IN researchers_NNS have_VBP also_RB studied_VBN negotiations_NNS with_IN an_DT endogenous_JJ agenda_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN contrast_NN to_TO the_DT above_JJ work_NN that_WDT mainly_RB deals_VBZ with_IN sequential_JJ negotiation_NN ,_, -LSB-_-LRB- #_# -RSB-_-RRB- studies_NNS the_DT equilibrium_NN for_IN the_DT package_NN deal_NN procedure_NN ._.
However_RB ,_, all_PDT the_DT above_JJ mentioned_VBN work_NN differs_VBZ from_IN ours_PRP in_IN that_IN we_PRP focus_VBP on_IN indivisible_JJ issues_NNS while_IN others_NNS focus_VBP on_IN the_DT case_NN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD where_WRB each_DT issue_NN is_VBZ divisible_JJ ._.
Specifically_RB ,_, no_DT previous_JJ work_NN has_VBZ determined_VBN an_DT approximate_JJ equilibrium_NN for_IN multi-issue_JJ negotiation_NN or_CC for_IN online_JJ negotiation_NN ._.
Existing_VBG work_NN for_IN the_DT case_NN of_IN indivisible_JJ issues_NNS has_VBZ mostly_RB dealt_VBN with_IN task_NN allocation_NN problems_NNS -LRB-_-LRB- for_IN tasks_NNS that_WDT can_MD not_RB be_VB partioned_VBN -RRB-_-RRB- to_TO a_DT group_NN of_IN agents_NNS ._.
The_DT problem_NN of_IN task_NN allocation_NN has_VBZ been_VBN previously_RB studied_VBN in_IN the_DT context_NN of_IN coalitions_NNS involving_VBG more_JJR than_IN two_CD agents_NNS ._.
For_IN example_NN -LSB-_-LRB- ##_NN -RSB-_-RRB- analyze_VBP the_DT problem_NN for_IN the_DT case_NN where_WRB the_DT agents_NNS act_VBP so_RB as_RB to_TO maximize_VB the_DT benefit_NN of_IN the_DT system_NN as_IN a_DT whole_NN ._.
In_IN contrast_NN ,_, our_PRP$ focus_NN is_VBZ on_IN two_CD agents_NNS where_WRB both_DT of_IN them_PRP are_VBP self-interested_JJ and_CC want_VBP to_TO maximize_VB their_PRP$ individual_JJ utilities_NNS ._.
On_IN the_DT other_JJ hand_NN -LSB-_-LRB- ##_NN -RSB-_-RRB- focus_NN on_IN the_DT use_NN of_IN contracts_NNS for_IN task_NN allocation_NN to_TO multiple_JJ self_NN interested_JJ agents_NNS but_CC this_DT work_NN concerns_NNS finding_VBG ways_NNS of_IN decommitting_VBG contracts_NNS -LRB-_-LRB- after_IN the_DT initial_JJ allocation_NN has_VBZ been_VBN done_VBN -RRB-_-RRB- so_RB as_IN to_TO improve_VB an_DT agent_NN ''_'' s_NNS utility_NN ._.
In_IN contrast_NN ,_, our_PRP$ focuses_VBZ on_IN negotiation_NN regarding_VBG who_WP will_MD carry_VB out_RP which_WDT task_NN ._.
Finally_RB ,_, online_NN and_CC approximate_JJ mechanisms_NNS have_VBP been_VBN studied_VBN in_IN the_DT context_NN of_IN auctions_NNS -LSB-_-LRB- ##_NNS ,_, #_# -RSB-_-RRB- but_CC not_RB for_IN bilateral_JJ negotiations_NNS -LRB-_-LRB- which_WDT is_VBZ the_DT focus_NN of_IN our_PRP$ work_NN -RRB-_-RRB- ._.
6_CD ._.
CONCLUSIONS_NNS This_DT paper_NN has_VBZ studied_VBN bilateral_JJ multi-issue_JJ negotiation_NN between_IN self-interested_JJ autonomous_JJ agents_NNS with_IN time_NN constraints_NNS ._.
The_DT issues_NNS are_VBP indivisible_JJ and_CC different_JJ agents_NNS value_NN different_JJ issues_NNS differently_RB ._.
Thus_RB ,_, the_DT problem_NN is_VBZ for_IN the_DT agents_NNS to_TO decide_VB how_WRB to_TO allocate_VB the_DT issues_NNS between_IN themselves_PRP so_RB as_IN to_TO maximize_VB their_PRP$ individual_JJ utilities_NNS ._.
Specifically_RB ,_, we_PRP first_RB showed_VBD that_IN finding_VBG the_DT equilibrium_NN offers_VBZ is_VBZ an_DT NP-hard_JJ problem_NN even_RB in_IN a_DT complete_JJ information_NN setting_NN ._.
We_PRP then_RB presented_VBD approximately_RB optimal_JJ negotiation_NN strategies_NNS and_CC showed_VBD that_IN they_PRP form_VBP an_DT equilibrium_NN ._.
These_DT strategies_NNS have_VBP polynomial_JJ time_NN complexity_NN ._.
We_PRP also_RB analysed_VBD the_DT difference_NN between_IN the_DT true_JJ optimum_NN and_CC the_DT approximate_JJ optimum_NN ._.
Finally_RB ,_, we_PRP extended_VBD the_DT analysis_NN to_TO online_JJ negotiation_NN where_WRB the_DT issues_NNS become_VBP available_JJ at_IN different_JJ time_NN points_NNS and_CC the_DT agents_NNS are_VBP uncertain_JJ about_IN the_DT features_NNS of_IN these_DT issues_NNS ._.
Specifically_RB ,_, we_PRP showed_VBD that_IN an_DT approximate_JJ equilibrium_NN exists_VBZ for_IN online_JJ negotiation_NN and_CC analysed_VBD the_DT approximation_NN error_NN ._.
These_DT approximate_JJ strategies_NNS also_RB have_VBP polynomial_JJ time_NN complexity_NN ._.
There_EX are_VBP several_JJ interesting_JJ directions_NNS for_IN future_JJ work_NN ._.
First_RB ,_, for_IN online_JJ negotiation_NN ,_, we_PRP assumed_VBD that_IN the_DT constants_NNS ka_FW c_NN and_CC kb_NN c_NN are_VBP both_DT uniformly_RB distributed_VBN ._.
It_PRP will_MD be_VB interesting_JJ to_TO analyze_VB the_DT case_NN where_WRB ka_FW c_NN and_CC kb_NN c_NN have_VBP other_JJ ,_, possibly_RB different_JJ ,_, probability_NN distributions_NNS ._.
Apart_RB from_IN this_DT ,_, we_PRP treated_VBD the_DT number_NN of_IN issues_NNS as_IN being_VBG common_JJ knowledge_NN to_TO the_DT agents_NNS ._.
In_IN future_NN ,_, it_PRP will_MD be_VB interesting_JJ to_TO treat_VB the_DT number_NN of_IN issues_NNS as_IN uncertain_JJ ._.
7_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- G_NN ._.
Ausiello_NNP ,_, P_NN ._.
Crescenzi_NNP ,_, G_NNP ._.
Gambosi_NNP ,_, V_NNP ._.
Kann_NNP ,_, A_NNP ._.
Marchetti-Spaccamela_NNP ,_, and_CC M_NN ._.
Protasi_NNP ._.
Complexity_NN and_CC approximation_NN :_: Combinatorial_JJ optimization_NN problems_NNS and_CC their_PRP$ approximability_NN properties_NNS ._.
Springer_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- M_NN ._.
Bac_NN and_CC H_NN ._.
Raff_NNP ._.
Issue-by-issue_JJ negotiations_NNS :_: the_DT role_NN of_IN information_NN and_CC time_NN preference_NN ._.
Games_NNPS and_CC Economic_NNP Behavior_NNP ,_, ##_CD :_: 125-134_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
Borodin_NNP and_CC R_NN ._.
El-Yaniv_NNP ._.
Online_NNP Computation_NNP and_CC Competitive_JJ Analysis_NN ._.
Cambridge_NNP University_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
J_NN ._.
Brams_NNS ._.
Fair_NNP division_NN :_: from_IN cake_NN cutting_VBG to_TO dispute_VB resolution_NN ._.
Cambridge_NNP University_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- L_NN ._.
A_DT ._.
Busch_NNP and_CC I_PRP ._.
J_NN ._.
Horstman_NNP ._.
Bargaining_VBG frictions_NNS ,_, bargaining_NN procedures_NNS and_CC implied_VBD costs_NNS in_IN multiple-issue_JJ bargaining_NN ._.
Economica_NNP ,_, ##_CD :_: 669-680_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
S_NN ._.
Fatima_NNP ,_, M_NN ._.
Wooldridge_NNP ,_, and_CC N_NN ._.
R_NN ._.
Jennings_NNP ._.
Multi-issue_JJ negotiation_NN with_IN deadlines_NNS ._.
Journal_NNP of_IN Artificial_NNP Intelligence_NNP Research_NNP ,_, ##_CD :_: 381-417_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- C_NN ._.
Fershtman_NNP ._.
The_DT importance_NN of_IN the_DT agenda_NN in_IN bargaining_NN ._.
Games_NNPS and_CC Economic_NNP Behavior_NNP ,_, #_# :_: 224-238_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- F_NN ._.
Glover_NNP ._.
A_DT multiphase_JJ dual_JJ algorithm_NN for_IN the_DT zero-one_JJ integer_NN programming_NN problem_NN ._.
Operations_NNP Research_NNP ,_, 13_CD :_: 879-919_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- M_NN ._.
T_NN ._.
Hajiaghayi_NNP ,_, R_NN ._.
Kleinberg_NNP ,_, and_CC D_NN ._.
C_NN ._.
Parkes_NNP ._.
Adaptive_JJ limited-supply_JJ online_NN auctions_NNS ._.
In_IN ACM_NNP Conference_NNP on_IN Electronic_NNP Commerce_NNP -LRB-_-LRB- ACMEC-04_NN -RRB-_-RRB- ,_, pages_NNS 71-80_CD ,_, New_NNP York_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- O_NN ._.
H_NN ._.
Ibarra_NNP and_CC C_NNP ._.
E_NN ._.
Kim_NNP ._.
Fast_JJ approximation_NN algorithms_NNS for_IN the_DT knapsack_NN and_CC sum_NN of_IN subset_NN problems_NNS ._.
Journal_NNP of_IN ACM_NNP ,_, ##_CD :_: 463-468_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- R_NN ._.
Inderst_NNP ._.
Multi-issue_JJ bargaining_NN with_IN endogenous_JJ agenda_NN ._.
Games_NNPS and_CC Economic_NNP Behavior_NNP ,_, ##_CD :_: 64-82_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- R_NN ._.
Keeney_NNP and_CC H_NNP ._.
Raiffa_NNP ._.
Decisions_NNS with_IN Multiple_JJ Objectives_NNS :_: Preferences_NNPS and_CC Value_NNP Trade-offs_NNS ._.
New_NNP York_NNP :_: John_NNP Wiley_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Kraus_NNP ._.
Strategic_NNP negotiation_NN in_IN multi-agent_JJ environments_NNS ._.
The_DT MIT_NNP Press_NNP ,_, Cambridge_NNP ,_, Massachusetts_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
Lehman_NNP ,_, L_NNP ._.
I_PRP ._.
O_NN ''_'' Callaghan_NNP ,_, and_CC Y_NN ._.
Shoham_NNP ._.
Truth_NN revelation_NN in_IN approximately_RB efficient_JJ combinatorial_JJ auctions_NNS ._.
Journal_NNP of_IN the_DT ACM_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 577-602_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- A_DT ._.
Lomuscio_NNP ,_, M_NN ._.
Wooldridge_NNP ,_, and_CC N_NN ._.
R_NN ._.
Jennings_NNP ._.
A_DT classification_NN scheme_NN for_IN negotiation_NN in_IN electronic_JJ commerce_NN ._.
International_NNP Journal_NNP of_IN Group_NNP Decision_NNP and_CC Negotiation_NNP ,_, 12_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 31-56_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- A_DT ._.
Marchetti-Spaccamela_NNP and_CC C_NNP ._.
Vercellis_NNP ._.
Stochastic_JJ online_JJ knapsack_NN problems_NNS ._.
Mathematical_JJ Programming_NN ,_, 68_CD :_: 73-104_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Martello_NNP and_CC P_NN ._.
Toth_NNP ._.
Knapsack_NN problems_NNS :_: Algorithms_NNS and_CC computer_NN implementations_NNS ._.
John_NNP Wiley_NNP and_CC Sons_NNPS ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- M_NN ._.
J_NN ._.
Osborne_NNP and_CC A_NNP ._.
Rubinstein_NN ._.
A_DT Course_NNP in_IN Game_NNP Theory_NNP ._.
The_DT MIT_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- H_NN ._.
Raiffa_NNP ._.
The_DT Art_NNP and_CC Science_NNP of_IN Negotiation_NNP ._.
Harvard_NNP University_NNP Press_NNP ,_, Cambridge_NNP ,_, USA_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- J_NN ._.
S_NN ._.
Rosenschein_NNP and_CC G_NNP ._.
Zlotkin_NNP ._.
Rules_NNS of_IN Encounter_NN ._.
MIT_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- A_DT ._.
Rubinstein_NN ._.
Perfect_NNP equilibrium_NN in_IN a_DT bargaining_NN model_NN ._.
Econometrica_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 97-109_CD ,_, January_NNP ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- T_NN ._.
Sandholm_NNP and_CC V_NNP ._.
Lesser_RBR ._.
Levelled_VBN commitment_NN contracts_NNS and_CC strategic_JJ breach_NN ._.
Games_NNPS and_CC Economic_NNP Behavior_NNP :_: Special_JJ Issue_NN on_IN AI_NNP and_CC Economics_NNP ,_, ##_CD :_: 212-270_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- T_NN ._.
Sandholm_NNP and_CC N_NNP ._.
Vulkan_NNP ._.
Bargaining_VBG with_IN deadlines_NNS ._.
In_IN AAAI-99_NN ,_, pages_NNS 44-51_CD ,_, Orlando_NNP ,_, FL_NN ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- T_NN ._.
C_NN ._.
Schelling_NNP ._.
An_DT essay_NN on_IN bargaining_NN ._.
American_JJ Economic_NNP Review_NNP ,_, ##_CD :_: 281-306_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- O_NN ._.
Shehory_NNP and_CC S_NN ._.
Kraus_NNP ._.
Methods_NNS for_IN task_NN allocation_NN via_IN agent_NN coalition_NN formation_NN ._.
Artificial_JJ Intelligence_NNP Journal_NNP ,_, 101_CD -LRB-_-LRB- 1-2_CD -RRB-_-RRB- :_: 165-200_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Singh_NNP ,_, V_NNP ._.
Soni_NNP ,_, and_CC M_NN ._.
Wellman_NNP ._.
Computing_NNP approximate_JJ Bayes_NNP Nash_NNP equilibria_NNP in_IN tree_NN games_NNS of_IN incomplete_JJ information_NN ._.
In_IN Proceedings_NNP of_IN the_DT ACM_NNP Conference_NN on_IN Electronic_JJ Commerce_NNP ACM-EC_NNP ,_, pages_NNS 81-90_CD ,_, New_NNP York_NNP ,_, May_NNP ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- I_PRP ._.
Stahl_NNP ._.
Bargaining_VBG Theory_NNP ._.
Economics_NNP Research_NNP Institute_NNP ,_, Stockholm_NNP School_NNP of_IN Economics_NNP ,_, Stockholm_NNP ,_, ####_CD ._.
958_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB-
