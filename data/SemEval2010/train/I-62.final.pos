A_DT Q-decomposition_NN and_CC Bounded_JJ RTDP_NN Approach_NN to_TO Resource_NNP Allocation_NNP Pierrick_NNP Plamondon_NNP and_CC Brahim_NNP Chaib-draa_NNP Computer_NNP Science_NNP &_CC Software_NNP Engineering_NNP Dept_NNP Laval_NNP University_NNP Qubec_NNP ,_, Canada_NNP -LCB-_-LRB- plamon_NNS ,_, chaib_NN -RCB-_-RRB- @_SYM damas_FW ._.
ift_NN ._.
ulaval_NN ._.
ca_MD Abder_NNP Rezak_NNP Benaskeur_NNP Decision_NNP Support_NN Systems_NNP Section_NNP Defence_NNP R_NNP &_CC D_NNP Canada_NNP -_: Valcartier_NNP Qubec_NNP ,_, Canada_NNP abderrezak_NN ._.
benaskeur_NN @_SYM drdc-rddc_NN ._.
gc_NN ._.
ca_MD ABSTRACT_NN This_DT paper_NN contributes_VBZ to_TO solve_VB effectively_RB stochastic_JJ resource_NN allocation_NN problems_NNS known_VBN to_TO be_VB NP-Complete_JJ ._.
To_TO address_VB this_DT complex_NN resource_NN management_NN problem_NN ,_, a_DT Qdecomposition_NNP approach_NN is_VBZ proposed_VBN when_WRB the_DT resources_NNS which_WDT are_VBP already_RB shared_VBN among_IN the_DT agents_NNS ,_, but_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN may_MD influence_VB the_DT reward_NN obtained_VBN by_IN at_IN least_JJS another_DT agent_NN ._.
The_DT Q-decomposition_NN allows_VBZ to_TO coordinate_VB these_DT reward_NN separated_JJ agents_NNS and_CC thus_RB permits_VBZ to_TO reduce_VB the_DT set_NN of_IN states_NNS and_CC actions_NNS to_TO consider_VB ._.
On_IN the_DT other_JJ hand_NN ,_, when_WRB the_DT resources_NNS are_VBP available_JJ to_TO all_DT agents_NNS ,_, no_DT Qdecomposition_NN is_VBZ possible_JJ and_CC we_PRP use_VBP heuristic_NN search_NN ._.
In_IN particular_JJ ,_, the_DT bounded_VBN Real-time_JJ Dynamic_NNP Programming_NNP -LRB-_-LRB- bounded_VBN rtdp_NN -RRB-_-RRB- is_VBZ used_VBN ._.
Bounded_JJ rtdp_NN concentrates_VBZ the_DT planning_NN on_IN significant_JJ states_NNS only_RB and_CC prunes_VBZ the_DT action_NN space_NN ._.
The_DT pruning_NN is_VBZ accomplished_VBN by_IN proposing_VBG tight_JJ upper_JJ and_CC lower_JJR bounds_NNS on_IN the_DT value_NN function_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNS I_PRP ._.
#_# ._.
#_# -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Problem_NNP Solving_VBG ,_, Control_NNP Methods_NNS ,_, and_CC Search_VB ;_: I_PRP ._.
#_# ._.
##_NN -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Distributed_VBN Artificial_NNP Intelligence_NNP ._.
General_NNP Terms_NNS Algorithms_NNS ,_, Performance_NNP ,_, Experimentation_NN ._.
1_LS ._.
INTRODUCTION_NN This_DT paper_NN aims_VBZ to_TO contribute_VB to_TO solve_VB complex_JJ stochastic_JJ resource_NN allocation_NN problems_NNS ._.
In_IN general_JJ ,_, resource_NN allocation_NN problems_NNS are_VBP known_VBN to_TO be_VB NP-Complete_JJ -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
In_IN such_JJ problems_NNS ,_, a_DT scheduling_NN process_NN suggests_VBZ the_DT action_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
resources_NNS to_TO allocate_VB -RRB-_-RRB- to_TO undertake_VB to_TO accomplish_VB certain_JJ tasks_NNS ,_, according_VBG to_TO the_DT perfectly_RB observable_JJ state_NN of_IN the_DT environment_NN ._.
When_WRB executing_VBG an_DT action_NN to_TO realize_VB a_DT set_NN of_IN tasks_NNS ,_, the_DT stochastic_JJ nature_NN of_IN the_DT problem_NN induces_VBZ probabilities_NNS on_IN the_DT next_JJ visited_VBN state_NN ._.
In_IN general_JJ ,_, the_DT number_NN of_IN states_NNS is_VBZ the_DT combination_NN of_IN all_DT possible_JJ specific_JJ states_NNS of_IN each_DT task_NN and_CC available_JJ resources_NNS ._.
In_IN this_DT case_NN ,_, the_DT number_NN of_IN possible_JJ actions_NNS in_IN a_DT state_NN is_VBZ the_DT combination_NN of_IN each_DT individual_JJ possible_JJ resource_NN assignment_NN to_TO the_DT tasks_NNS ._.
The_DT very_RB high_JJ number_NN of_IN states_NNS and_CC actions_NNS in_IN this_DT type_NN of_IN problem_NN makes_VBZ it_PRP very_RB complex_JJ ._.
There_EX can_MD be_VB many_JJ types_NNS of_IN resource_NN allocation_NN problems_NNS ._.
Firstly_RB ,_, if_IN the_DT resources_NNS are_VBP already_RB shared_VBN among_IN the_DT agents_NNS ,_, and_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN does_VBZ not_RB influence_VB the_DT state_NN of_IN another_DT agent_NN ,_, the_DT globally_RB optimal_JJ policy_NN can_MD be_VB computed_VBN by_IN planning_VBG separately_RB for_IN each_DT agent_NN ._.
A_DT second_JJ type_NN of_IN resource_NN allocation_NN problem_NN is_VBZ where_WRB the_DT resources_NNS are_VBP already_RB shared_VBN among_IN the_DT agents_NNS ,_, but_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN may_MD influence_VB the_DT reward_NN obtained_VBN by_IN at_IN least_JJS another_DT agent_NN ._.
To_TO solve_VB this_DT problem_NN efficiently_RB ,_, we_PRP adapt_VBP Qdecomposition_NN proposed_VBN by_IN Russell_NNP and_CC Zimdars_NNP -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN our_PRP$ Q-decomposition_JJ approach_NN ,_, a_DT planning_NN agent_NN manages_VBZ each_DT task_NN and_CC all_DT agents_NNS have_VBP to_TO share_VB the_DT limited_JJ resources_NNS ._.
The_DT planning_NN process_NN starts_VBZ with_IN the_DT initial_JJ state_NN s0_NN ._.
In_IN s0_NN ,_, each_DT agent_NN computes_VBZ their_PRP$ respective_JJ Q-value_JJ ._.
Then_RB ,_, the_DT planning_VBG agents_NNS are_VBP coordinated_VBN through_IN an_DT arbitrator_NN to_TO find_VB the_DT highest_JJS global_JJ Q-value_JJ by_IN adding_VBG the_DT respective_JJ possible_JJ Q-values_NNS of_IN each_DT agents_NNS ._.
When_WRB implemented_VBN with_IN heuristic_NN search_NN ,_, since_IN the_DT number_NN of_IN states_NNS and_CC actions_NNS to_TO consider_VB when_WRB computing_VBG the_DT optimal_JJ policy_NN is_VBZ exponentially_RB reduced_VBN compared_VBN to_TO other_JJ known_JJ approaches_NNS ,_, Q-decomposition_NN allows_VBZ to_TO formulate_VB the_DT first_JJ optimal_JJ decomposed_VBN heuristic_NN search_NN algorithm_NN in_IN a_DT stochastic_JJ environments_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, when_WRB the_DT resources_NNS are_VBP available_JJ to_TO all_DT agents_NNS ,_, no_DT Q-decomposition_NN is_VBZ possible_JJ ._.
A_DT common_JJ way_NN of_IN addressing_VBG this_DT large_JJ stochastic_JJ problem_NN is_VBZ by_IN using_VBG Markov_NNP Decision_NNP Processes_NNP -LRB-_-LRB- mdps_NNS -RRB-_-RRB- ,_, and_CC in_IN particular_JJ real-time_JJ search_NN where_WRB many_JJ algorithms_NNS have_VBP been_VBN developed_VBN recently_RB ._.
For_IN instance_NN Real-Time_NNP Dynamic_NNP Programming_NNP -LRB-_-LRB- rtdp_NN -RRB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- ,_, lrtdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, hdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, and_CC lao_NN -LSB-_-LRB- #_# -RSB-_-RRB- are_VBP all_DT state-of-the-art_JJ heuristic_NN search_NN approaches_NNS in_IN a_DT stochastic_JJ environment_NN ._.
Because_IN of_IN its_PRP$ anytime_RB quality_NN ,_, an_DT interesting_JJ approach_NN is_VBZ rtdp_JJ introduced_VBN by_IN Barto_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- which_WDT updates_NNS states_NNS in_IN trajectories_NNS from_IN an_DT initial_JJ state_NN s0_NN to_TO a_DT goal_NN state_NN sg_NN ._.
rtdp_NN is_VBZ used_VBN in_IN this_DT paper_NN to_TO solve_VB efficiently_RB a_DT constrained_VBN resource_NN allocation_NN problem_NN ._.
rtdp_NN is_VBZ much_RB more_RBR effective_JJ if_IN the_DT action_NN space_NN can_MD be_VB pruned_VBN of_IN sub-optimal_JJ actions_NNS ._.
To_TO do_VB this_DT ,_, McMahan_NNP et_NNP 1212_CD 978-81-904262-7-5_CD -LRB-_-LRB- RPS_NN -RRB-_-RRB- c_NN ####_CD IFAAMAS_NNP al_NNP ._.
-LSB-_-LRB- #_# -RSB-_-RRB- ,_, Smith_NNP and_CC Simmons_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, and_CC Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- proposed_VBD solving_VBG a_DT stochastic_JJ problem_NN using_VBG a_DT rtdp_NN type_NN heuristic_NN search_NN with_IN upper_JJ and_CC lower_JJR bounds_NNS on_IN the_DT value_NN of_IN states_NNS ._.
McMahan_NNP et_FW al_FW ._.
-LSB-_-LRB- #_# -RSB-_-RRB- and_CC Smith_NNP and_CC Simmons_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- suggested_VBD ,_, in_IN particular_JJ ,_, an_DT efficient_JJ trajectory_NN of_IN state_NN updates_NNS to_TO further_JJ speed_NN up_RP the_DT convergence_NN ,_, when_WRB given_VBN upper_JJ and_CC lower_JJR bounds_NNS ._.
This_DT efficient_JJ trajectory_NN of_IN state_NN updates_NNS can_MD be_VB combined_VBN to_TO the_DT approach_NN proposed_VBN here_RB since_IN this_DT paper_NN focusses_VBZ on_IN the_DT definition_NN of_IN tight_JJ bounds_NNS ,_, and_CC efficient_JJ state_NN update_VB for_IN a_DT constrained_VBN resource_NN allocation_NN problem_NN ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT approach_NN by_IN Singh_NNP and_CC Cohn_NNP is_VBZ suitable_JJ to_TO our_PRP$ case_NN ,_, and_CC extended_VBN in_IN this_DT paper_NN using_VBG ,_, in_IN particular_JJ ,_, the_DT concept_NN of_IN marginal_JJ revenue_NN -LSB-_-LRB- #_# -RSB-_-RRB- to_TO elaborate_VB tight_JJ bounds_NNS ._.
This_DT paper_NN proposes_VBZ new_JJ algorithms_NNS to_TO define_VB upper_JJ and_CC lower_JJR bounds_NNS in_IN the_DT context_NN of_IN a_DT rtdp_NN heuristic_NN search_NN approach_NN ._.
Our_PRP$ marginal_JJ revenue_NN bounds_NNS are_VBP compared_VBN theoretically_RB and_CC empirically_RB to_TO the_DT bounds_NNS proposed_VBN by_IN Singh_NNP and_CC Cohn_NNP ._.
Also_RB ,_, even_RB if_IN the_DT algorithm_NN used_VBN to_TO obtain_VB the_DT optimal_JJ policy_NN is_VBZ rtdp_NN ,_, our_PRP$ bounds_NNS can_MD be_VB used_VBN with_IN any_DT other_JJ algorithm_NN to_TO solve_VB an_DT mdp_NN ._.
The_DT only_JJ condition_NN on_IN the_DT use_NN of_IN our_PRP$ bounds_NNS is_VBZ to_TO be_VB in_IN the_DT context_NN of_IN stochastic_JJ constrained_VBN resource_NN allocation_NN ._.
The_DT problem_NN is_VBZ now_RB modelled_VBN ._.
2_LS ._.
PROBLEM_NN FORMULATION_NN A_NN simple_JJ resource_NN allocation_NN problem_NN is_VBZ one_CD where_WRB there_EX are_VBP the_DT following_VBG two_CD tasks_NNS to_TO realize_VB :_: ta1_NN =_JJ -LCB-_-LRB- wash_VB the_DT dishes_NNS -RCB-_-RRB- ,_, and_CC ta2_NN =_JJ -LCB-_-LRB- clean_JJ the_DT floor_NN -RCB-_-RRB- ._.
These_DT two_CD tasks_NNS are_VBP either_RB in_IN the_DT realized_VBN state_NN ,_, or_CC not_RB realized_VBN state_NN ._.
To_TO realize_VB the_DT tasks_NNS ,_, two_CD type_NN of_IN resources_NNS are_VBP assumed_VBN :_: res1_NN =_JJ -LCB-_-LRB- brush_NN -RCB-_-RRB- ,_, and_CC res2_NN =_JJ -LCB-_-LRB- detergent_NN -RCB-_-RRB- ._.
A_DT computer_NN has_VBZ to_TO compute_VB the_DT optimal_JJ allocation_NN of_IN these_DT resources_NNS to_TO cleaner_JJR robots_NNS to_TO realize_VB their_PRP$ tasks_NNS ._.
In_IN this_DT problem_NN ,_, a_DT state_NN represents_VBZ a_DT conjunction_NN of_IN the_DT particular_JJ state_NN of_IN each_DT task_NN ,_, and_CC the_DT available_JJ resources_NNS ._.
The_DT resources_NNS may_MD be_VB constrained_VBN by_IN the_DT amount_NN that_WDT may_MD be_VB used_VBN simultaneously_RB -LRB-_-LRB- local_JJ constraint_NN -RRB-_-RRB- ,_, and_CC in_IN total_JJ -LRB-_-LRB- global_JJ constraint_NN -RRB-_-RRB- ._.
Furthermore_RB ,_, the_DT higher_JJR is_VBZ the_DT number_NN of_IN resources_NNS allocated_VBN to_TO realize_VB a_DT task_NN ,_, the_DT higher_JJR is_VBZ the_DT expectation_NN of_IN realizing_VBG the_DT task_NN ._.
For_IN this_DT reason_NN ,_, when_WRB the_DT specific_JJ states_NNS of_IN the_DT tasks_NNS change_NN ,_, or_CC when_WRB the_DT number_NN of_IN available_JJ resources_NNS changes_NNS ,_, the_DT value_NN of_IN this_DT state_NN may_MD change_VB ._.
When_WRB executing_VBG an_DT action_NN a_DT in_IN state_NN s_NNS ,_, the_DT specific_JJ states_NNS of_IN the_DT tasks_NNS change_VBP stochastically_RB ,_, and_CC the_DT remaining_VBG resource_NN are_VBP determined_VBN with_IN the_DT resource_NN available_JJ in_IN s_NNS ,_, subtracted_VBN from_IN the_DT resources_NNS used_VBN by_IN action_NN a_DT ,_, if_IN the_DT resource_NN is_VBZ consumable_JJ ._.
Indeed_RB ,_, our_PRP$ model_NN may_MD consider_VB consumable_JJ and_CC non-consumable_JJ resource_NN types_NNS ._.
A_DT consumable_JJ resource_NN type_NN is_VBZ one_CD where_WRB the_DT amount_NN of_IN available_JJ resource_NN is_VBZ decreased_VBN when_WRB it_PRP is_VBZ used_VBN ._.
On_IN the_DT other_JJ hand_NN ,_, a_DT nonconsumable_JJ resource_NN type_NN is_VBZ one_CD where_WRB the_DT amount_NN of_IN available_JJ resource_NN is_VBZ unchanged_JJ when_WRB it_PRP is_VBZ used_VBN ._.
For_IN example_NN ,_, a_DT brush_NN is_VBZ a_DT non-consumable_JJ resource_NN ,_, while_IN the_DT detergent_NN is_VBZ a_DT consumable_JJ resource_NN ._.
2_LS ._.
#_# Resource_NNP Allocation_NNP as_IN a_DT MDPs_NNS In_IN our_PRP$ problem_NN ,_, the_DT transition_NN function_NN and_CC the_DT reward_NN function_NN are_VBP both_DT known_VBN ._.
A_DT Markov_NNP Decision_NNP Process_VB -LRB-_-LRB- mdp_NN -RRB-_-RRB- framework_NN is_VBZ used_VBN to_TO model_VB our_PRP$ stochastic_JJ resource_NN allocation_NN problem_NN ._.
mdps_NNS have_VBP been_VBN widely_RB adopted_VBN by_IN researchers_NNS today_NN to_TO model_VB a_DT stochastic_JJ process_NN ._.
This_DT is_VBZ due_JJ to_TO the_DT fact_NN that_IN mdps_NNS provide_VBP a_DT well-studied_JJ and_CC simple_JJ ,_, yet_RB very_RB expressive_JJ model_NN of_IN the_DT world_NN ._.
An_DT mdp_NN in_IN the_DT context_NN of_IN a_DT resource_NN allocation_NN problem_NN with_IN limited_JJ resources_NNS is_VBZ defined_VBN as_IN a_DT tuple_JJ Res_NNP ,_, T_NN a_DT ,_, S_NN ,_, A_NN ,_, P_NN ,_, W_NN ,_, R_NN ,_, ,_, where_WRB :_: Res_NNP =_JJ res1_NN ,_, ..._: ,_, res_VBZ |_CD Res_NNP |_CD is_VBZ a_DT finite_JJ set_NN of_IN resource_NN types_NNS available_JJ for_IN a_DT planning_NN process_NN ._.
Each_DT resource_NN type_NN may_MD have_VB a_DT local_JJ resource_NN constraint_NN Lres_VBZ on_IN the_DT number_NN that_WDT may_MD be_VB used_VBN in_IN a_DT single_JJ step_NN ,_, and_CC a_DT global_JJ resource_NN constraint_NN Gres_NNP on_IN the_DT number_NN that_WDT may_MD be_VB used_VBN in_IN total_NN ._.
The_DT global_JJ constraint_NN only_RB applies_VBZ for_IN consumable_JJ resource_NN types_NNS -LRB-_-LRB- Resc_NN -RRB-_-RRB- and_CC the_DT local_JJ constraints_NNS always_RB apply_VBP to_TO consumable_JJ and_CC nonconsumable_JJ resource_NN types_NNS ._.
T_NN a_DT is_VBZ a_DT finite_JJ set_NN of_IN tasks_NNS with_IN ta_NN T_NN a_DT to_TO be_VB accomplished_VBN ._.
S_NN is_VBZ a_DT finite_JJ set_NN of_IN states_NNS with_IN s_NNS S_NN ._.
A_DT state_NN s_NNS is_VBZ a_DT tuple_JJ T_NN a_DT ,_, res1_NN ,_, ..._: ,_, res_VBZ |_CD Resc_NNP |_NNP ,_, which_WDT is_VBZ the_DT characteristic_NN of_IN each_DT unaccomplished_JJ task_NN ta_NN T_NN a_DT in_IN the_DT environment_NN ,_, and_CC the_DT available_JJ consumable_JJ resources_NNS ._.
sta_NN is_VBZ the_DT specific_JJ state_NN of_IN task_NN ta_NN ._.
Also_RB ,_, S_NN contains_VBZ a_DT non_JJ empty_JJ set_NN sg_NN S_NN of_IN goal_NN states_NNS ._.
A_DT goal_NN state_NN is_VBZ a_DT sink_NN state_NN where_WRB an_DT agent_NN stays_VBZ forever_RB ._.
A_DT is_VBZ a_DT finite_JJ set_NN of_IN actions_NNS -LRB-_-LRB- or_CC assignments_NNS -RRB-_-RRB- ._.
The_DT actions_NNS a_DT A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- applicable_JJ in_IN a_DT state_NN are_VBP the_DT combination_NN of_IN all_DT resource_NN assignments_NNS that_WDT may_MD be_VB executed_VBN ,_, according_VBG to_TO the_DT state_NN s_NNS ._.
In_IN particular_JJ ,_, a_DT is_VBZ simply_RB an_DT allocation_NN of_IN resources_NNS to_TO the_DT current_JJ tasks_NNS ,_, and_CC ata_NN is_VBZ the_DT resource_NN allocation_NN to_TO task_NN ta_NN ._.
The_DT possible_JJ actions_NNS are_VBP limited_VBN by_IN Lres_NNS and_CC Gres_NNP ._.
Transition_NN probabilities_NNS Pa_NNP -LRB-_-LRB- s_NNS |_VBP s_NNS -RRB-_-RRB- for_IN s_NNS S_NN and_CC a_DT A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
W_NN =_JJ -LSB-_-LRB- wta_NN -RSB-_-RRB- is_VBZ the_DT relative_JJ weight_NN -LRB-_-LRB- criticality_NN -RRB-_-RRB- of_IN each_DT task_NN ._.
State_NNP rewards_VBZ R_NN =_JJ -LSB-_-LRB- rs_NNS -RSB-_-RRB- :_: taT_NN a_DT rsta_NN sta_NN wta_NN ._.
The_DT relative_JJ reward_NN of_IN the_DT state_NN of_IN a_DT task_NN rsta_NN is_VBZ the_DT product_NN of_IN a_DT real_JJ number_NN sta_NN by_IN the_DT weight_NN factor_NN wta_NN ._.
For_IN our_PRP$ problem_NN ,_, a_DT reward_NN of_IN #_# wta_FW is_VBZ given_VBN when_WRB the_DT state_NN of_IN a_DT task_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- is_VBZ in_IN an_DT achieved_VBN state_NN ,_, and_CC #_# in_IN all_DT other_JJ cases_NNS ._.
A_DT discount_NN -LRB-_-LRB- preference_NN -RRB-_-RRB- factor_NN ,_, which_WDT is_VBZ a_DT real_JJ number_NN between_IN #_# and_CC #_# ._.
A_DT solution_NN of_IN an_DT mdp_NN is_VBZ a_DT policy_NN mapping_NN states_NNS s_VBZ into_IN actions_NNS a_DT A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
In_IN particular_JJ ,_, ta_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- is_VBZ the_DT action_NN -LRB-_-LRB- i_FW ._.
e_LS ._.
resources_NNS to_TO allocate_VB -RRB-_-RRB- that_WDT should_MD be_VB executed_VBN on_IN task_NN ta_NN ,_, considering_VBG the_DT global_JJ state_NN s_NNS ._.
In_IN this_DT case_NN ,_, an_DT optimal_JJ policy_NN is_VBZ one_CD that_WDT maximizes_VBZ the_DT expected_VBN total_JJ reward_NN for_IN accomplishing_VBG all_DT tasks_NNS ._.
The_DT optimal_JJ value_NN of_IN a_DT state_NN ,_, V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ,_, is_VBZ given_VBN by_IN :_: V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ R_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- +_CC max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- s_VBZ S_NN Pa_NNP -LRB-_-LRB- s_NNS |_VBP s_NNS -RRB-_-RRB- V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB the_DT remaining_VBG consumable_JJ resources_NNS in_IN state_NN s_NNS are_VBP Resc_NNP \_CD res_NNS -LRB-_-LRB- a_DT -RRB-_-RRB- ,_, where_WRB res_NNS -LRB-_-LRB- a_DT -RRB-_-RRB- are_VBP the_DT consumable_JJ resources_NNS used_VBN by_IN action_NN a_DT ._.
Indeed_RB ,_, since_IN an_DT action_NN a_DT is_VBZ a_DT resource_NN assignment_NN ,_, Resc_NNP \_NNP res_VBZ -LRB-_-LRB- a_DT -RRB-_-RRB- is_VBZ the_DT new_JJ set_NN of_IN available_JJ resources_NNS after_IN the_DT execution_NN of_IN action_NN a_DT ._.
Furthermore_RB ,_, one_CD may_MD compute_VB the_DT Q-Values_NNS Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- of_IN each_DT state_NN action_NN pair_NN using_VBG the_DT The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_RB following_JJ equation_NN :_: Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- =_JJ R_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- +_CC s_VBZ S_NN Pa_NNP -LRB-_-LRB- s_NNS |_VBP s_NNS -RRB-_-RRB- max_VBP a_DT A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB the_DT optimal_JJ value_NN of_IN a_DT state_NN is_VBZ V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- ._.
The_DT policy_NN is_VBZ subjected_VBN to_TO the_DT local_JJ resource_NN constraints_NNS res_NNS -LRB-_-LRB- -LRB-_-LRB- s_NNS -RRB-_-RRB- -RRB-_-RRB- Lres_VBZ s_NNS S_NN ,_, and_CC res_VBZ Res_NNP ._.
The_DT global_JJ constraint_NN is_VBZ defined_VBN according_VBG to_TO all_DT system_NN trajectories_NNS tra_NN T_NN RA_NN ._.
A_DT system_NN trajectory_NN tra_NN is_VBZ a_DT possible_JJ sequence_NN of_IN state-action_JJ pairs_NNS ,_, until_IN a_DT goal_NN state_NN is_VBZ reached_VBN under_IN the_DT optimal_JJ policy_NN ._.
For_IN example_NN ,_, state_NN s_NNS is_VBZ entered_VBN ,_, which_WDT may_MD transit_NN to_TO s_NNS or_CC to_TO s_NNS ,_, according_VBG to_TO action_NN a_DT ._.
The_DT two_CD possible_JJ system_NN trajectories_NNS are_VBP -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- ,_, -LRB-_-LRB- s_NNS -RRB-_-RRB- and_CC -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- ,_, -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
The_DT global_JJ resource_NN constraint_NN is_VBZ res_NN -LRB-_-LRB- tra_NN -RRB-_-RRB- Gres_VBZ tra_NN T_NN RA_NN ,_, and_CC res_VBZ Resc_NNP where_WRB res_NNS -LRB-_-LRB- tra_NN -RRB-_-RRB- is_VBZ a_DT function_NN which_WDT returns_VBZ the_DT resources_NNS used_VBN by_IN trajectory_NN tra_NN ._.
Since_IN the_DT available_JJ consumable_JJ resources_NNS are_VBP represented_VBN in_IN the_DT state_NN space_NN ,_, this_DT condition_NN is_VBZ verified_VBN by_IN itself_PRP ._.
In_IN other_JJ words_NNS ,_, the_DT model_NN is_VBZ Markovian_JJ as_IN the_DT history_NN has_VBZ not_RB to_TO be_VB considered_VBN in_IN the_DT state_NN space_NN ._.
Furthermore_RB ,_, the_DT time_NN is_VBZ not_RB considered_VBN in_IN the_DT model_NN description_NN ,_, but_CC it_PRP may_MD also_RB include_VB a_DT time_NN horizon_NN by_IN using_VBG a_DT finite_JJ horizon_NN mdp_NN ._.
Since_IN resource_NN allocation_NN in_IN a_DT stochastic_JJ environment_NN is_VBZ NP-Complete_JJ ,_, heuristics_NNS should_MD be_VB employed_VBN ._.
Q-decomposition_NN which_WDT decomposes_VBZ a_DT planning_NN problem_NN to_TO many_JJ agents_NNS to_TO reduce_VB the_DT computational_JJ complexity_NN associated_VBN to_TO the_DT state_NN and_CC /_: or_CC action_NN spaces_NNS is_VBZ now_RB introduced_VBN ._.
2_LS ._.
#_# Q-decomposition_NN for_IN Resource_NNP Allocation_NNP There_EX can_MD be_VB many_JJ types_NNS of_IN resource_NN allocation_NN problems_NNS ._.
Firstly_RB ,_, if_IN the_DT resources_NNS are_VBP already_RB shared_VBN among_IN the_DT agents_NNS ,_, and_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN does_VBZ not_RB influence_VB the_DT state_NN of_IN another_DT agent_NN ,_, the_DT globally_RB optimal_JJ policy_NN can_MD be_VB computed_VBN by_IN planning_VBG separately_RB for_IN each_DT agent_NN ._.
A_DT second_JJ type_NN of_IN resource_NN allocation_NN problem_NN is_VBZ where_WRB the_DT resources_NNS are_VBP already_RB shared_VBN among_IN the_DT agents_NNS ,_, but_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN may_MD influence_VB the_DT reward_NN obtained_VBN by_IN at_IN least_JJS another_DT agent_NN ._.
For_IN instance_NN ,_, a_DT group_NN of_IN agents_NNS which_WDT manages_VBZ the_DT oil_NN consummated_VBN by_IN a_DT country_NN falls_VBZ in_IN this_DT group_NN ._.
These_DT agents_NNS desire_VBP to_TO maximize_VB their_PRP$ specific_JJ reward_NN by_IN consuming_VBG the_DT right_JJ amount_NN of_IN oil_NN ._.
However_RB ,_, all_PDT the_DT agents_NNS are_VBP penalized_VBN when_WRB an_DT agent_NN consumes_VBZ oil_NN because_IN of_IN the_DT pollution_NN it_PRP generates_VBZ ._.
Another_DT example_NN of_IN this_DT type_NN comes_VBZ from_IN our_PRP$ problem_NN of_IN interest_NN ,_, explained_VBN in_IN Section_NN 3_CD ,_, which_WDT is_VBZ a_DT naval_JJ platform_NN which_WDT must_MD counter_VB incoming_JJ missiles_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
tasks_NNS -RRB-_-RRB- by_IN using_VBG its_PRP$ resources_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
weapons_NNS ,_, movements_NNS -RRB-_-RRB- ._.
In_IN some_DT scenarios_NNS ,_, it_PRP may_MD happens_VBZ that_IN the_DT missiles_NNS can_MD be_VB classified_VBN in_IN two_CD types_NNS :_: Those_DT requiring_VBG a_DT set_NN of_IN resources_NNS Res1_NN and_CC those_DT requiring_VBG a_DT set_NN of_IN resources_NNS Res2_NN ._.
This_DT can_MD happen_VB depending_VBG on_IN the_DT type_NN of_IN missiles_NNS ,_, their_PRP$ range_NN ,_, and_CC so_RB on_IN ._.
In_IN this_DT case_NN ,_, two_CD agents_NNS can_MD plan_VB for_IN both_DT set_NN of_IN tasks_NNS to_TO determine_VB the_DT policy_NN ._.
However_RB ,_, there_EX are_VBP interaction_NN between_IN the_DT resource_NN of_IN Res1_NN and_CC Res2_NN ,_, so_RB that_IN certain_JJ combination_NN of_IN resource_NN can_MD not_RB be_VB assigned_VBN ._.
IN_IN particular_JJ ,_, if_IN an_DT agent_NN i_FW allocate_VB resources_NNS Resi_NNS to_TO the_DT first_JJ set_NN of_IN tasks_NNS T_NN ai_VBP ,_, and_CC agent_NN i_FW allocate_VB resources_NNS Resi_NNS to_TO second_JJ set_NN of_IN tasks_NNS T_NN ai_VBP ,_, the_DT resulting_VBG policy_NN may_MD include_VB actions_NNS which_WDT can_MD not_RB be_VB executed_VBN together_RB ._.
To_TO result_VB these_DT conflicts_NNS ,_, we_PRP use_VBP Q-decomposition_JJ proposed_VBN by_IN Russell_NNP and_CC Zimdars_NNP -LSB-_-LRB- #_# -RSB-_-RRB- in_IN the_DT context_NN of_IN reinforcement_NN learning_NN ._.
The_DT primary_JJ assumption_NN underlying_VBG Qdecomposition_NNP is_VBZ that_IN the_DT overall_JJ reward_NN function_NN R_NN can_MD be_VB additively_RB decomposed_VBN into_IN separate_JJ rewards_NNS Ri_NNP for_IN each_DT distinct_JJ agent_NN i_FW Ag_NN ,_, where_WRB |_JJ Ag_NN |_NN is_VBZ the_DT number_NN of_IN agents_NNS ._.
That_DT is_VBZ ,_, R_NN =_JJ iAg_NNP Ri_NNP ._.
It_PRP requires_VBZ each_DT agent_NN to_TO compute_VB a_DT value_NN ,_, from_IN its_PRP$ perspective_NN ,_, for_IN every_DT action_NN ._.
To_TO coordinate_VB with_IN each_DT other_JJ ,_, each_DT agent_NN i_FW reports_VBZ its_PRP$ action_NN values_NNS Qi_NNP -LRB-_-LRB- ai_VBP ,_, si_VBP -RRB-_-RRB- for_IN each_DT state_NN si_FW Si_NNP to_TO an_DT arbitrator_NN at_IN each_DT learning_VBG iteration_NN ._.
The_DT arbitrator_NN then_RB chooses_VBZ an_DT action_NN maximizing_VBG the_DT sum_NN of_IN the_DT agent_NN Q-values_NNS for_IN each_DT global_JJ state_NN s_NNS S_NN ._.
The_DT next_JJ time_NN state_NN s_NNS is_VBZ updated_VBN ,_, an_DT agent_NN i_FW considers_VBZ the_DT value_NN as_IN its_PRP$ respective_JJ contribution_NN ,_, or_CC Q-value_NN ,_, to_TO the_DT global_JJ maximal_JJ Q-value_NN ._.
That_DT is_VBZ ,_, Qi_NNP -LRB-_-LRB- ai_VBP ,_, si_VBP -RRB-_-RRB- is_VBZ the_DT value_NN of_IN a_DT state_NN such_JJ that_IN it_PRP maximizes_VBZ maxaA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- iAg_NNP Qi_NNP -LRB-_-LRB- ai_VBP ,_, si_VBP -RRB-_-RRB- ._.
The_DT fact_NN that_IN the_DT agents_NNS use_VBP a_DT determined_VBN Q-value_NN as_IN the_DT value_NN of_IN a_DT state_NN is_VBZ an_DT extension_NN of_IN the_DT Sarsa_FW on-policy_JJ algorithm_NN -LSB-_-LRB- #_# -RSB-_-RRB- to_TO Q-decomposition_NN ._.
Russell_NNP and_CC Zimdars_NNP called_VBD this_DT approach_NN local_JJ Sarsa_NNP ._.
In_IN this_DT way_NN ,_, an_DT ideal_JJ compromise_NN can_MD be_VB found_VBN for_IN the_DT agents_NNS to_TO reach_VB a_DT global_JJ optimum_NN ._.
Indeed_RB ,_, rather_RB than_IN allowing_VBG each_DT agent_NN to_TO choose_VB the_DT successor_NN action_NN ,_, each_DT agent_NN i_FW uses_VBZ the_DT action_NN ai_VBP executed_VBN by_IN the_DT arbitrator_NN in_IN the_DT successor_NN state_NN si_NNS :_: Qi_NNP -LRB-_-LRB- ai_VBP ,_, si_VBP -RRB-_-RRB- =_JJ Ri_NN -LRB-_-LRB- si_NN -RRB-_-RRB- +_CC siSi_NNP Pai_NNP -LRB-_-LRB- si_FW |_FW si_FW -RRB-_-RRB- Qi_NNP -LRB-_-LRB- ai_VBP ,_, si_VBP -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB the_DT remaining_VBG consumable_JJ resources_NNS in_IN state_NN si_NNS are_VBP Resci_NNP \_CD resi_NNS -LRB-_-LRB- ai_VBP -RRB-_-RRB- for_IN a_DT resource_NN allocation_NN problem_NN ._.
Russell_NNP and_CC Zimdars_NNP -LSB-_-LRB- #_# -RSB-_-RRB- demonstrated_VBD that_IN local_JJ Sarsa_NNP converges_VBZ to_TO the_DT optimum_NN ._.
Also_RB ,_, in_IN some_DT cases_NNS ,_, this_DT form_NN of_IN agent_NN decomposition_NN allows_VBZ the_DT local_JJ Q-functions_NNS to_TO be_VB expressed_VBN by_IN a_DT much_JJ reduced_JJ state_NN and_CC action_NN space_NN ._.
For_IN our_PRP$ resource_NN allocation_NN problem_NN described_VBD briefly_NN in_IN this_DT section_NN ,_, Q-decomposition_NN can_MD be_VB applied_VBN to_TO generate_VB an_DT optimal_JJ solution_NN ._.
Indeed_RB ,_, an_DT optimal_JJ Bellman_NNP backup_NN can_MD be_VB applied_VBN in_IN a_DT state_NN as_IN in_IN Algorithm_NNP #_# ._.
In_IN Line_NNP #_# of_IN the_DT Qdec-backup_JJ function_NN ,_, each_DT agent_NN managing_VBG a_DT task_NN computes_VBZ its_PRP$ respective_JJ Q-value_JJ ._.
Here_RB ,_, Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- determines_VBZ the_DT optimal_JJ Q-value_NN of_IN agent_NN i_FW in_IN state_NN s_NNS ._.
An_DT agent_NN i_FW uses_VBZ as_IN the_DT value_NN of_IN a_DT possible_JJ state_NN transition_NN s_VBZ the_DT Q-value_NN for_IN this_DT agent_NN which_WDT determines_VBZ the_DT maximal_JJ global_JJ Q-value_NN for_IN state_NN s_NNS as_IN in_IN the_DT original_JJ Q-decomposition_JJ approach_NN ._.
In_IN brief_NN ,_, for_IN each_DT visited_VBN states_NNS s_NNS S_NN ,_, each_DT agent_NN computes_VBZ its_PRP$ respective_JJ Q-values_NNS with_IN respect_NN to_TO the_DT global_JJ state_NN s_NNS ._.
So_IN the_DT state_NN space_NN is_VBZ the_DT joint_JJ state_NN space_NN of_IN all_DT agents_NNS ._.
Some_DT of_IN the_DT gain_NN in_IN complexity_NN to_TO use_VB Q-decomposition_JJ resides_VBZ in_IN the_DT siSi_NNP Pai_NNP -LRB-_-LRB- si_FW |_FW s_NNS -RRB-_-RRB- part_NN of_IN the_DT equation_NN ._.
An_DT agent_NN considers_VBZ as_IN a_DT possible_JJ state_NN transition_NN only_RB the_DT possible_JJ states_NNS of_IN the_DT set_NN of_IN tasks_NNS it_PRP manages_VBZ ._.
Since_IN the_DT number_NN of_IN states_NNS is_VBZ exponential_JJ with_IN the_DT number_NN of_IN tasks_NNS ,_, using_VBG Q-decomposition_NN should_MD reduce_VB the_DT planning_NN time_NN significantly_RB ._.
Furthermore_RB ,_, the_DT action_NN space_NN of_IN the_DT agents_NNS takes_VBZ into_IN account_NN only_RB their_PRP$ available_JJ resources_NNS which_WDT is_VBZ much_RB less_RBR complex_JJ than_IN a_DT standard_JJ action_NN space_NN ,_, which_WDT is_VBZ the_DT combination_NN of_IN all_DT possible_JJ resource_NN allocation_NN in_IN a_DT state_NN for_IN all_DT agents_NNS ._.
Then_RB ,_, the_DT arbitrator_NN functionalities_NNS are_VBP in_IN Lines_NNP #_# to_TO ##_CD ._.
The_DT global_JJ Q-value_NN is_VBZ the_DT sum_NN of_IN the_DT Q-values_NNS produced_VBN by_IN each_DT agent_NN managing_NN each_DT task_NN as_IN shown_VBN in_IN Line_NNP ##_NN ,_, considering_VBG the_DT global_JJ action_NN a_DT ._.
In_IN this_DT case_NN ,_, when_WRB an_DT action_NN of_IN an_DT agent_NN i_FW can_MD not_RB be_VB executed_VBN simultaneously_RB with_IN an_DT action_NN of_IN another_DT agent_NN i_FW ,_, the_DT global_JJ action_NN is_VBZ simply_RB discarded_VBN from_IN the_DT action_NN space_NN A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
Line_NNP ##_RB simply_RB allocate_VB the_DT current_JJ value_NN with_IN respect_NN to_TO the_DT highest_JJS global_JJ Q-value_NN ,_, as_IN in_IN a_DT standard_JJ Bellman_NNP backup_NN ._.
Then_RB ,_, the_DT optimal_JJ policy_NN and_CC Q-value_NN of_IN each_DT agent_NN is_VBZ updated_VBN in_IN Lines_NNP ##_NN and_CC ##_NN to_TO the_DT sub-actions_NNS ai_VBP and_CC specific_JJ Q-values_NNS Qi_NN -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- of_IN each_DT agent_NN 1214_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- for_IN action_NN a_DT ._.
Algorithm_NN #_# The_DT Q-decomposition_JJ Bellman_NNP Backup_NNP ._.
1_CD :_: Function_NN Qdec-backup_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- 2_CD :_: V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- #_# 3_CD :_: for_IN all_DT i_FW Ag_NN do_VBP 4_CD :_: for_IN all_DT ai_VBP Ai_NNP -LRB-_-LRB- s_NNS -RRB-_-RRB- do_VBP 5_CD :_: Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- Ri_NNP -LRB-_-LRB- s_NNS -RRB-_-RRB- +_CC si_FW Si_NNP Pai_NNP -LRB-_-LRB- si_FW |_FW s_NNS -RRB-_-RRB- Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- -LCB-_-LRB- where_WRB Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- =_JJ hi_UH -LRB-_-LRB- s_NNS -RRB-_-RRB- when_WRB s_NNS is_VBZ not_RB yet_RB visited_VBN ,_, and_CC s_NNS has_VBZ Resci_NNP \_NNP resi_NN -LRB-_-LRB- ai_VBP -RRB-_-RRB- remaining_VBG consumable_JJ resources_NNS for_IN each_DT agent_NN i_FW -RCB-_-RRB- 6_CD :_: end_VB for_IN 7_CD :_: end_VB for_IN 8_CD :_: for_IN all_PDT a_DT A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- do_VBP 9_CD :_: Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- #_# 10_CD :_: for_IN all_DT i_FW Ag_NN do_VBP 11_CD :_: Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- +_CC Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- 12_CD :_: end_VB for_IN 13_CD :_: if_IN Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- >_JJR V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- then_RB 14_CD :_: V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- Q_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- 15_CD :_: for_IN all_DT i_FW Ag_NN do_VBP 16_CD :_: i_FW -LRB-_-LRB- s_NNS -RRB-_-RRB- ai_VBP 17_CD :_: Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- Qi_NNP -LRB-_-LRB- ai_VBP ,_, s_NNS -RRB-_-RRB- 18_CD :_: end_VB for_IN 19_CD :_: end_VB if_IN 20_CD :_: end_VB for_IN A_DT standard_JJ Bellman_NNP backup_NN has_VBZ a_DT complexity_NN of_IN O_NN -LRB-_-LRB- |_CD A_DT |_NN |_CD SAg_NNP |_CD -RRB-_-RRB- ,_, where_WRB |_JJ SAg_NN |_NN is_VBZ the_DT number_NN of_IN joint_JJ states_NNS for_IN all_DT agents_NNS excluding_VBG the_DT resources_NNS ,_, and_CC |_NN A_DT |_NN is_VBZ the_DT number_NN of_IN joint_JJ actions_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT Q-decomposition_NNP Bellman_NNP backup_NN has_VBZ a_DT complexity_NN of_IN O_NN -LRB-_-LRB- -LRB-_-LRB- |_CD Ag_NN |_CD |_CD Ai_NNP |_CD |_CD Si_NNP -RRB-_-RRB- |_NN -RRB-_-RRB- +_CC -LRB-_-LRB- |_CD A_DT |_NN |_CD Ag_NN |_NN -RRB-_-RRB- -RRB-_-RRB- ,_, where_WRB |_JJ Si_NNP |_CD is_VBZ the_DT number_NN of_IN states_NNS for_IN an_DT agent_NN i_FW ,_, excluding_VBG the_DT resources_NNS and_CC |_CD Ai_NNP |_NNP is_VBZ the_DT number_NN of_IN actions_NNS for_IN an_DT agent_NN i_FW ._.
Since_IN |_CD SAg_NN |_NN is_VBZ combinatorial_JJ with_IN the_DT number_NN of_IN tasks_NNS ,_, so_RB |_JJ Si_NNP |_CD |_CD S_NN |_NN ._.
Also_RB ,_, |_NN A_DT |_NN is_VBZ combinatorial_JJ with_IN the_DT number_NN of_IN resource_NN types_NNS ._.
If_IN the_DT resources_NNS are_VBP already_RB shared_VBN among_IN the_DT agents_NNS ,_, the_DT number_NN of_IN resource_NN type_NN for_IN each_DT agent_NN will_MD usually_RB be_VB lower_JJR than_IN the_DT set_NN of_IN all_DT available_JJ resource_NN types_NNS for_IN all_DT agents_NNS ._.
In_IN these_DT circumstances_NNS ,_, |_CD Ai_NNP |_CD |_CD A_DT |_NN ._.
In_IN a_DT standard_JJ Bellman_NNP backup_NN ,_, |_CD A_DT |_NN is_VBZ multiplied_VBN by_IN |_CD SAg_NN |_NN ,_, which_WDT is_VBZ much_RB more_RBR complex_JJ than_IN multiplying_VBG |_CD A_DT |_NN by_IN |_CD Ag_NN |_NN with_IN the_DT Q-decomposition_NNP Bellman_NNP backup_NN ._.
Thus_RB ,_, the_DT Q-decomposition_NNP Bellman_NNP backup_NN is_VBZ much_RB less_RBR complex_JJ than_IN a_DT standard_JJ Bellman_NNP backup_NN ._.
Furthermore_RB ,_, the_DT communication_NN cost_NN between_IN the_DT agents_NNS and_CC the_DT arbitrator_NN is_VBZ null_JJ since_IN this_DT approach_NN does_VBZ not_RB consider_VB a_DT geographically_RB separated_JJ problem_NN ._.
However_RB ,_, when_WRB the_DT resources_NNS are_VBP available_JJ to_TO all_DT agents_NNS ,_, no_DT Q-decomposition_NN is_VBZ possible_JJ ._.
In_IN this_DT case_NN ,_, Bounded_JJ RealTime_NNP Dynamic_NNP Programming_NNP -LRB-_-LRB- bounded-rtdp_JJ -RRB-_-RRB- permits_VBZ to_TO focuss_VB the_DT search_NN on_IN relevant_JJ states_NNS ,_, and_CC to_TO prune_VB the_DT action_NN space_NN A_NN by_IN using_VBG lower_JJR and_CC higher_JJR bound_VBN on_IN the_DT value_NN of_IN states_NNS ._.
bounded-rtdp_NN is_VBZ now_RB introduced_VBN ._.
2_LS ._.
#_# Bounded-RTDP_NNP Bonet_NNP and_CC Geffner_NNP -LSB-_-LRB- #_# -RSB-_-RRB- proposed_VBD lrtdp_NN as_IN an_DT improvement_NN to_TO rtdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
lrtdp_NN is_VBZ a_DT simple_JJ dynamic_JJ programming_NN algorithm_NN that_WDT involves_VBZ a_DT sequence_NN of_IN trial_NN runs_NNS ,_, each_DT starting_VBG in_IN the_DT initial_JJ state_NN s0_NN and_CC ending_VBG in_IN a_DT goal_NN or_CC a_DT solved_VBN state_NN ._.
Each_DT lrtdp_JJ trial_NN is_VBZ the_DT result_NN of_IN simulating_VBG the_DT policy_NN while_IN updating_VBG the_DT values_NNS V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- using_VBG a_DT Bellman_NNP backup_NN -LRB-_-LRB- Equation_NN 1_CD -RRB-_-RRB- over_IN the_DT states_NNS s_NNS that_WDT are_VBP visited_VBN ._.
h_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- is_VBZ a_DT heuristic_NN which_WDT define_VBP an_DT initial_JJ value_NN for_IN state_NN s_NNS ._.
This_DT heuristic_NN has_VBZ to_TO be_VB admissible_JJ -_: The_DT value_NN given_VBN by_IN the_DT heuristic_NN has_VBZ to_TO overestimate_VB -LRB-_-LRB- or_CC underestimate_VBP -RRB-_-RRB- the_DT optimal_JJ value_NN V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- when_WRB the_DT objective_JJ function_NN is_VBZ maximized_VBN -LRB-_-LRB- or_CC minimized_VBN -RRB-_-RRB- ._.
For_IN example_NN ,_, an_DT admissible_JJ heuristic_NN for_IN a_DT stochastic_JJ shortest_JJS path_NN problem_NN is_VBZ the_DT solution_NN of_IN a_DT deterministic_JJ shortest_JJS path_NN problem_NN ._.
Indeed_RB ,_, since_IN the_DT problem_NN is_VBZ stochastic_JJ ,_, the_DT optimal_JJ value_NN is_VBZ lower_JJR than_IN for_IN the_DT deterministic_JJ version_NN ._.
It_PRP has_VBZ been_VBN proven_VBN that_IN lrtdp_NN ,_, given_VBN an_DT admissible_JJ initial_JJ heuristic_NN on_IN the_DT value_NN of_IN states_NNS can_MD not_RB be_VB trapped_VBN in_IN loops_NNS ,_, and_CC eventually_RB yields_NNS optimal_JJ values_NNS -LSB-_-LRB- #_# -RSB-_-RRB- ._.
The_DT convergence_NN is_VBZ accomplished_VBN by_IN means_NNS of_IN a_DT labeling_NN procedure_NN called_VBN checkSolved_NNP -LRB-_-LRB- s_NNS ,_, -RRB-_-RRB- ._.
This_DT procedure_NN tries_VBZ to_TO label_VB as_IN solved_VBN each_DT traversed_VBN state_NN in_IN the_DT current_JJ trajectory_NN ._.
When_WRB the_DT initial_JJ state_NN is_VBZ labelled_VBN as_IN solved_VBN ,_, the_DT algorithm_NN has_VBZ converged_VBN ._.
In_IN this_DT section_NN ,_, a_DT bounded_VBN version_NN of_IN rtdp_NN -LRB-_-LRB- boundedrtdp_NN -RRB-_-RRB- is_VBZ presented_VBN in_IN Algorithm_NNP #_# to_TO prune_VB the_DT action_NN space_NN of_IN sub-optimal_JJ actions_NNS ._.
This_DT pruning_NN enables_VBZ to_TO speed_VB up_RP the_DT convergence_NN of_IN lrtdp_NN ._.
bounded-rtdp_NN is_VBZ similar_JJ to_TO rtdp_VB except_IN there_EX are_VBP two_CD distinct_JJ initial_JJ heuristics_NNS for_IN unvisited_JJ states_NNS s_VBZ S_NN ;_: hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- and_CC hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
Also_RB ,_, the_DT checkSolved_JJ -LRB-_-LRB- s_NNS ,_, -RRB-_-RRB- procedure_NN can_MD be_VB omitted_VBN because_IN the_DT bounds_NNS can_MD provide_VB the_DT labeling_NN of_IN a_DT state_NN as_IN solved_VBN ._.
On_IN the_DT one_CD hand_NN ,_, hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- defines_VBZ a_DT lower_JJR bound_VBN on_IN the_DT value_NN of_IN s_NNS such_JJ that_IN the_DT optimal_JJ value_NN of_IN s_NNS is_VBZ higher_JJR than_IN hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
For_IN its_PRP$ part_NN ,_, hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- defines_VBZ an_DT upper_JJ bound_VBN on_IN the_DT value_NN of_IN s_NNS such_JJ that_IN the_DT optimal_JJ value_NN of_IN s_NNS is_VBZ lower_JJR than_IN hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
The_DT values_NNS of_IN the_DT bounds_NNS are_VBP computed_VBN in_IN Lines_NNP #_# and_CC 4_CD of_IN the_DT bounded-backup_JJ function_NN ._.
Computing_NNP these_DT two_CD Q-values_NNS is_VBZ made_VBN simultaneously_RB as_IN the_DT state_NN transitions_NNS are_VBP the_DT same_JJ for_IN both_DT Q-values_NNS ._.
Only_RB the_DT values_NNS of_IN the_DT state_NN transitions_NNS change_VBP ._.
Thus_RB ,_, having_VBG to_TO compute_VB two_CD Q-values_NNS instead_RB of_IN one_CD does_VBZ not_RB augment_VB the_DT complexity_NN of_IN the_DT approach_NN ._.
In_IN fact_NN ,_, Smith_NNP and_CC Simmons_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- state_NN that_IN the_DT additional_JJ time_NN to_TO compute_VB a_DT Bellman_NNP backup_NN for_IN two_CD bounds_NNS ,_, instead_RB of_IN one_CD ,_, is_VBZ no_RB more_JJR than_IN ##_CD %_NN ,_, which_WDT is_VBZ also_RB what_WP we_PRP obtained_VBD ._.
In_IN particular_JJ ,_, L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- is_VBZ the_DT lower_JJR bound_VBN of_IN state_NN s_NNS ,_, while_IN U_NNP -LRB-_-LRB- s_NNS -RRB-_-RRB- is_VBZ the_DT upper_JJ bound_VBN of_IN state_NN s_NNS ._.
Similarly_RB ,_, QL_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- is_VBZ the_DT Q-value_NN of_IN the_DT lower_JJR bound_VBN of_IN action_NN a_DT in_IN state_NN s_NNS ,_, while_IN QU_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- is_VBZ the_DT Q-value_NN of_IN the_DT upper_JJ bound_VBN of_IN action_NN a_DT in_IN state_NN s_NNS ._.
Using_VBG these_DT two_CD bounds_NNS allow_VBP significantly_RB reducing_VBG the_DT action_NN space_NN A_NN ._.
Indeed_RB ,_, in_IN Lines_NNP #_# and_CC #_# of_IN the_DT bounded-backup_JJ function_NN ,_, if_IN QU_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- then_RB action_NN a_DT may_MD be_VB pruned_VBN from_IN the_DT action_NN space_NN of_IN s_NNS ._.
In_IN Line_NNP ##_NN of_IN this_DT function_NN ,_, a_DT state_NN can_MD be_VB labeled_VBN as_IN solved_VBN if_IN the_DT difference_NN between_IN the_DT lower_JJR and_CC upper_JJ bounds_NNS is_VBZ lower_JJR than_IN ._.
When_WRB the_DT execution_NN goes_VBZ back_RB to_TO the_DT bounded-rtdp_JJ function_NN ,_, the_DT next_JJ state_NN in_IN Line_NNP ##_NNP has_VBZ a_DT fixed_JJ number_NN of_IN consumable_JJ resources_NNS available_JJ Resc_NNP ,_, determined_VBN in_IN Line_NNP #_# ._.
In_IN brief_NN ,_, pickNextState_NN -LRB-_-LRB- res_NNS -RRB-_-RRB- selects_VBZ a_DT none-solved_JJ state_NN s_VBZ reachable_JJ under_IN the_DT current_JJ policy_NN which_WDT has_VBZ the_DT highest_JJS Bellman_NNP error_NN -LRB-_-LRB- |_CD U_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- |_CD -RRB-_-RRB- ._.
Finally_RB ,_, in_IN Lines_NNP ##_CD to_TO ##_CD ,_, a_DT backup_NN is_VBZ made_VBN in_IN a_DT backward_RB fashion_NN on_IN all_DT visited_VBN state_NN of_IN a_DT trajectory_NN ,_, when_WRB this_DT trajectory_NN has_VBZ been_VBN made_VBN ._.
This_DT strategy_NN has_VBZ been_VBN proven_VBN as_IN efficient_JJ -LSB-_-LRB- ##_CD -RSB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- ._.
As_IN discussed_VBN by_IN Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, this_DT type_NN of_IN algorithm_NN has_VBZ a_DT number_NN of_IN desirable_JJ anytime_RB characteristics_NNS :_: if_IN an_DT action_NN has_VBZ to_TO be_VB picked_VBN in_IN state_NN s_NNS before_IN the_DT algorithm_NN has_VBZ converged_VBN -LRB-_-LRB- while_IN multiple_JJ competitive_JJ actions_NNS remains_VBZ -RRB-_-RRB- ,_, the_DT action_NN with_IN the_DT highest_JJS lower_JJR bound_VBN is_VBZ picked_VBN ._.
Since_IN the_DT upper_JJ bound_VBN for_IN state_NN s_NNS is_VBZ known_VBN ,_, it_PRP may_MD be_VB estimated_VBN The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_CD Algorithm_NNP #_# The_DT bounded-rtdp_JJ algorithm_NN ._.
Adapted_VBN from_IN -LSB-_-LRB- #_# -RSB-_-RRB- and_CC -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
1_CD :_: Function_NN bounded-rtdp_NN -LRB-_-LRB- S_NN -RRB-_-RRB- 2_CD :_: returns_NNS a_DT value_NN function_NN V_NN 3_CD :_: repeat_NN 4_CD :_: s_NNS s0_VBP 5_CD :_: visited_VBN null_JJ 6_CD :_: repeat_NN 7_CD :_: visited_VBN ._.
push_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- 8_CD :_: bounded-backup_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- 9_CD :_: Resc_NNP Resc_NNP \_NNP -LCB-_-LRB- -LRB-_-LRB- s_NNS -RRB-_-RRB- -RCB-_-RRB- 10_CD :_: s_NNS s_NNS ._.
pickNextState_NN -LRB-_-LRB- Resc_NN -RRB-_-RRB- 11_CD :_: until_IN s_NNS is_VBZ a_DT goal_NN 12_CD :_: while_IN visited_VBN =_JJ null_NN do_VBP 13_CD :_: s_NNS visited_VBD ._.
pop_NN -LRB-_-LRB- -RRB-_-RRB- 14_CD :_: bounded-backup_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- 15_CD :_: end_NN while_IN 16_CD :_: until_IN s0_NN is_VBZ solved_VBN or_CC |_CD A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- |_NN =_JJ #_# s_NNS S_NN reachable_JJ from_IN s0_NN 17_CD :_: return_NN V_NN Algorithm_NN #_# The_DT bounded_VBN Bellman_NNP backup_NN ._.
1_CD :_: Function_NN bounded-backup_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- 2_CD :_: for_IN all_PDT a_DT A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- do_VBP 3_CD :_: QU_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- R_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- +_CC s_VBZ S_NN Pa_NNP -LRB-_-LRB- s_NNS |_VBP s_NNS -RRB-_-RRB- U_NNP -LRB-_-LRB- s_NNS -RRB-_-RRB- 4_CD :_: QL_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- R_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- +_CC s_VBZ S_NN Pa_NNP -LRB-_-LRB- s_NNS |_VBP s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- -LCB-_-LRB- where_WRB L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- and_CC U_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- when_WRB s_NNS is_VBZ not_RB yet_RB visited_VBN and_CC s_NNS has_VBZ Resc_NNP \_NNP res_VBZ -LRB-_-LRB- a_DT -RRB-_-RRB- remaining_VBG consumable_JJ resources_NNS -RCB-_-RRB- 5_CD :_: if_IN QU_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- then_RB 6_CD :_: A_DT -LRB-_-LRB- s_NNS -RRB-_-RRB- A_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- \_VBP res_NNS -LRB-_-LRB- a_DT -RRB-_-RRB- 7_CD :_: end_VB if_IN 8_CD :_: end_VB for_IN 9_CD :_: L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- QL_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- 10_CD :_: U_NNP -LRB-_-LRB- s_NNS -RRB-_-RRB- max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- QU_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- 11_CD :_: -LRB-_-LRB- s_NNS -RRB-_-RRB- arg_NN max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- QL_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- 12_CD :_: if_IN |_NN U_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- |_VBP <_JJR then_RB 13_CD :_: s_NNS solved_VBD 14_CD :_: end_VB if_IN how_WRB far_RB the_DT lower_JJR bound_VBN is_VBZ from_IN the_DT optimal_JJ ._.
If_IN the_DT difference_NN between_IN the_DT lower_JJR and_CC upper_JJ bound_VBN is_VBZ too_RB high_JJ ,_, one_PRP can_MD choose_VB to_TO use_VB another_DT greedy_JJ algorithm_NN of_IN one_CD ''_'' s_VBZ choice_NN ,_, which_WDT outputs_VBZ a_DT fast_JJ and_CC near_JJ optimal_JJ solution_NN ._.
Furthermore_RB ,_, if_IN a_DT new_JJ task_NN dynamically_RB arrives_VBZ in_IN the_DT environment_NN ,_, it_PRP can_MD be_VB accommodated_VBN by_IN redefining_VBG the_DT lower_JJR and_CC upper_JJ bounds_NNS which_WDT exist_VBP at_IN the_DT time_NN of_IN its_PRP$ arrival_NN ._.
Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- proved_VBD that_IN an_DT algorithm_NN that_WDT uses_VBZ admissible_JJ lower_JJR and_CC upper_JJ bounds_NNS to_TO prune_VB the_DT action_NN space_NN is_VBZ assured_VBN of_IN converging_VBG to_TO an_DT optimal_JJ solution_NN ._.
The_DT next_JJ sections_NNS describe_VBP two_CD separate_JJ methods_NNS to_TO define_VB hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- and_CC hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
First_NNP of_IN all_DT ,_, the_DT method_NN of_IN Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- is_VBZ briefly_RB described_VBN ._.
Then_RB ,_, our_PRP$ own_JJ method_NN proposes_VBZ tighter_JJR bounds_NNS ,_, thus_RB allowing_VBG a_DT more_RBR effective_JJ pruning_NN of_IN the_DT action_NN space_NN ._.
2_LS ._.
#_# Singh_NNP and_CC Cohn_NNP ''_'' s_VBZ Bounds_NNP Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- defined_VBD lower_JJR and_CC upper_JJ bounds_NNS to_TO prune_VB the_DT action_NN space_NN ._.
Their_PRP$ approach_NN is_VBZ pretty_RB straightforward_JJ ._.
First_NNP of_IN all_DT ,_, a_DT value_NN function_NN is_VBZ computed_VBN for_IN all_DT tasks_NNS to_TO realize_VB ,_, using_VBG a_DT standard_JJ rtdp_NN approach_NN ._.
Then_RB ,_, using_VBG these_DT task-value_JJ functions_NNS ,_, a_DT lower_JJR bound_VBN hL_NN ,_, and_CC upper_JJ bound_VBN hU_NN can_MD be_VB defined_VBN ._.
In_IN particular_JJ ,_, hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ max_NN taT_NNP a_DT Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- ,_, and_CC hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ taT_NN a_DT Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- ._.
For_IN readability_NN ,_, the_DT upper_JJ bound_VBN by_IN Singh_NNP and_CC Cohn_NNP is_VBZ named_VBN SinghU_NNP ,_, and_CC the_DT lower_JJR bound_VBN is_VBZ named_VBN SinghL_NNP ._.
The_DT admissibility_NN of_IN these_DT bounds_NNS has_VBZ been_VBN proven_VBN by_IN Singh_NNP and_CC Cohn_NNP ,_, such_JJ that_IN ,_, the_DT upper_JJ bound_VBN always_RB overestimates_VBZ the_DT optimal_JJ value_NN of_IN each_DT state_NN ,_, while_IN the_DT lower_JJR bound_VBN always_RB underestimates_VBZ the_DT optimal_JJ value_NN of_IN each_DT state_NN ._.
To_TO determine_VB the_DT optimal_JJ policy_NN ,_, Singh_NNP and_CC Cohn_NNP implemented_VBD an_DT algorithm_NN very_RB similar_JJ to_TO bounded-rtdp_NN ,_, which_WDT uses_VBZ the_DT bounds_NNS to_TO initialize_VB L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- and_CC U_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
The_DT only_JJ difference_NN between_IN bounded-rtdp_NN ,_, and_CC the_DT rtdp_NN version_NN of_IN Singh_NNP and_CC Cohn_NNP is_VBZ in_IN the_DT stopping_VBG criteria_NNS ._.
Singh_NNP and_CC Cohn_NNP proposed_VBD that_IN the_DT algorithm_NN terminates_VBZ when_WRB only_RB one_CD competitive_JJ action_NN remains_VBZ for_IN each_DT state_NN ,_, or_CC when_WRB the_DT range_NN of_IN all_DT competitive_JJ actions_NNS for_IN any_DT state_NN are_VBP bounded_VBN by_IN an_DT indifference_NN parameter_NN ._.
bounded-rtdp_JJ labels_NNS states_NNS for_IN which_WDT |_NN U_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- |_VBP <_JJR as_IN solved_VBN and_CC the_DT convergence_NN is_VBZ reached_VBN when_WRB s0_NN is_VBZ solved_VBN or_CC when_WRB only_RB one_CD competitive_JJ action_NN remains_VBZ for_IN each_DT state_NN ._.
This_DT stopping_VBG criteria_NNS is_VBZ more_RBR effective_JJ since_IN it_PRP is_VBZ similar_JJ to_TO the_DT one_CD used_VBN by_IN Smith_NNP and_CC Simmons_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- and_CC McMahan_NNP et_FW al_FW ._.
brtdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN this_DT paper_NN ,_, the_DT bounds_NNS defined_VBN by_IN Singh_NNP and_CC Cohn_NNP and_CC implemented_VBN using_VBG bounded-rtdp_JJ define_VB the_DT Singh-rtdp_JJ approach_NN ._.
The_DT next_JJ sections_NNS propose_VBP to_TO tighten_VB the_DT bounds_NNS of_IN Singh-rtdp_JJ to_TO permit_VB a_DT more_RBR effective_JJ pruning_NN of_IN the_DT action_NN space_NN ._.
2_LS ._.
#_# Reducing_VBG the_DT Upper_NNP Bound_VBN SinghU_NN includes_VBZ actions_NNS which_WDT may_MD not_RB be_VB possible_JJ to_TO execute_VB because_IN of_IN resource_NN constraints_NNS ,_, which_WDT overestimates_VBZ the_DT upper_JJ bound_VBN ._.
To_TO consider_VB only_RB possible_JJ actions_NNS ,_, our_PRP$ upper_JJ bound_VBN ,_, named_VBN maxU_NNP is_VBZ introduced_VBN :_: hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- taT_VBP a_DT Qta_NN -LRB-_-LRB- ata_NN ,_, sta_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB Qta_NN -LRB-_-LRB- ata_NN ,_, sta_NN -RRB-_-RRB- is_VBZ the_DT Q-value_NN of_IN task_NN ta_NN for_IN state_NN sta_NN ,_, and_CC action_NN ata_NN computed_VBD using_VBG a_DT standard_JJ lrtdp_NN approach_NN ._.
Theorem_NNP #_# ._.
#_# ._.
The_DT upper_JJ bound_VBN defined_VBN by_IN Equation_NN #_# is_VBZ admissible_JJ ._.
Proof_NN :_: The_DT local_JJ resource_NN constraints_NNS are_VBP satisfied_JJ because_IN the_DT upper_JJ bound_VBN is_VBZ computed_VBN using_VBG all_DT global_JJ possible_JJ actions_NNS a_DT ._.
However_RB ,_, hU_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- still_RB overestimates_VBZ V_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- because_IN the_DT global_JJ resource_NN constraint_NN is_VBZ not_RB enforced_VBN ._.
Indeed_RB ,_, each_DT task_NN may_MD use_VB all_DT consumable_JJ resources_NNS for_IN its_PRP$ own_JJ purpose_NN ._.
Doing_VBG this_DT produces_VBZ a_DT higher_JJR value_NN for_IN each_DT task_NN ,_, than_IN the_DT one_CD obtained_VBN when_WRB planning_VBG for_IN all_DT tasks_NNS globally_RB with_IN the_DT shared_JJ limited_JJ resources_NNS ._.
Computing_NNP the_DT maxU_NN bound_VBN in_IN a_DT state_NN has_VBZ a_DT complexity_NN of_IN O_NN -LRB-_-LRB- |_CD A_DT |_NN |_CD T_NN a_DT |_NN -RRB-_-RRB- ,_, and_CC O_NN -LRB-_-LRB- |_CD T_NN a_DT |_NN -RRB-_-RRB- for_IN SinghU_NNP ._.
A_DT standard_JJ Bellman_NNP backup_NN has_VBZ a_DT complexity_NN of_IN O_NN -LRB-_-LRB- |_CD A_DT |_NN |_CD S_NN |_NN -RRB-_-RRB- ._.
Since_IN |_NN A_NN |_CD |_CD T_NN a_DT |_NN |_NN A_NN |_CD |_CD S_NN |_NN ,_, the_DT computation_NN time_NN to_TO determine_VB the_DT upper_JJ bound_VBN of_IN a_DT state_NN ,_, which_WDT is_VBZ done_VBN one_CD time_NN for_IN each_DT visited_VBN state_NN ,_, is_VBZ much_RB less_JJR than_IN the_DT computation_NN time_NN required_VBN to_TO compute_VB a_DT standard_JJ Bellman_NNP backup_NN for_IN a_DT state_NN ,_, which_WDT is_VBZ usually_RB done_VBN many_JJ times_NNS for_IN each_DT visited_VBN state_NN ._.
Thus_RB ,_, the_DT computation_NN time_NN of_IN the_DT upper_JJ bound_VBN is_VBZ negligible_JJ ._.
1216_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- 2_CD ._.
#_# Increasing_VBG the_DT Lower_JJR Bound_VBN The_DT idea_NN to_TO increase_VB SinghL_NNP is_VBZ to_TO allocate_VB the_DT resources_NNS a_DT priori_FW among_IN the_DT tasks_NNS ._.
When_WRB each_DT task_NN has_VBZ its_PRP$ own_JJ set_NN of_IN resources_NNS ,_, each_DT task_NN may_MD be_VB solved_VBN independently_RB ._.
The_DT lower_JJR bound_VBN of_IN state_NN s_NNS is_VBZ hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ taT_NN a_DT Lowta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- ,_, where_WRB Lowta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- is_VBZ a_DT value_NN function_NN for_IN each_DT task_NN ta_NN T_NN a_DT ,_, such_JJ that_IN the_DT resources_NNS have_VBP been_VBN allocated_VBN a_DT priori_FW ._.
The_DT allocation_NN a_DT priori_FW of_IN the_DT resources_NNS is_VBZ made_VBN using_VBG marginal_JJ revenue_NN ,_, which_WDT is_VBZ a_DT highly_RB used_VBN concept_NN in_IN microeconomics_NNS -LSB-_-LRB- #_# -RSB-_-RRB- ,_, and_CC has_VBZ recently_RB been_VBN used_VBN for_IN coordination_NN of_IN a_DT Decentralized_JJ mdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN brief_NN ,_, marginal_JJ revenue_NN is_VBZ the_DT extra_JJ revenue_NN that_IN an_DT additional_JJ unit_NN of_IN product_NN will_MD bring_VB to_TO a_DT firm_NN ._.
Thus_RB ,_, for_IN a_DT stochastic_JJ resource_NN allocation_NN problem_NN ,_, the_DT marginal_JJ revenue_NN of_IN a_DT resource_NN is_VBZ the_DT additional_JJ expected_VBN value_NN it_PRP involves_VBZ ._.
The_DT marginal_JJ revenue_NN of_IN a_DT resource_NN res_VBZ for_IN a_DT task_NN ta_NN in_IN a_DT state_NN sta_NN is_VBZ defined_VBN as_IN following_VBG :_: mrta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- =_JJ max_NN ataA_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- Qta_NN -LRB-_-LRB- ata_NN ,_, sta_NN -RRB-_-RRB- max_NN ataA_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- Qta_NN -LRB-_-LRB- ata_NN |_CD res_VBZ /_: ata_NN ,_, sta_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- The_DT concept_NN of_IN marginal_JJ revenue_NN of_IN a_DT resource_NN is_VBZ used_VBN in_IN Algorithm_NNP #_# to_TO allocate_VB the_DT resources_NNS a_DT priori_FW among_IN the_DT tasks_NNS which_WDT enables_VBZ to_TO define_VB the_DT lower_JJR bound_VBN value_NN of_IN a_DT state_NN ._.
In_IN Line_NNP #_# of_IN the_DT algorithm_NN ,_, a_DT value_NN function_NN is_VBZ computed_VBN for_IN all_DT tasks_NNS in_IN the_DT environment_NN using_VBG a_DT standard_JJ lrtdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- approach_NN ._.
These_DT value_NN functions_NNS ,_, which_WDT are_VBP also_RB used_VBN for_IN the_DT upper_JJ bound_VBN ,_, are_VBP computed_VBN considering_VBG that_IN each_DT task_NN may_MD use_VB all_DT available_JJ resources_NNS ._.
The_DT Line_NNP #_# initializes_VBZ the_DT valueta_NN variable_NN ._.
This_DT variable_NN is_VBZ the_DT estimated_VBN value_NN of_IN each_DT task_NN ta_NN T_NN a_DT ._.
In_IN the_DT beginning_NN of_IN the_DT algorithm_NN ,_, no_DT resources_NNS are_VBP allocated_VBN to_TO a_DT specific_JJ task_NN ,_, thus_RB the_DT valueta_NN variable_NN is_VBZ initialized_VBN to_TO #_# for_IN all_DT ta_NN T_NN a_DT ._.
Then_RB ,_, in_IN Line_NNP #_# ,_, a_DT resource_NN type_NN res_NNS -LRB-_-LRB- consumable_JJ or_CC non-consumable_JJ -RRB-_-RRB- is_VBZ selected_VBN to_TO be_VB allocated_VBN ._.
Here_RB ,_, a_DT domain_NN expert_NN may_MD separate_VB all_DT available_JJ resources_NNS in_IN many_JJ types_NNS or_CC parts_NNS to_TO be_VB allocated_VBN ._.
The_DT resources_NNS are_VBP allocated_VBN in_IN the_DT order_NN of_IN its_PRP$ specialization_NN ._.
In_IN other_JJ words_NNS ,_, the_DT more_JJR a_DT resource_NN is_VBZ efficient_JJ on_IN a_DT small_JJ group_NN of_IN tasks_NNS ,_, the_DT more_RBR it_PRP is_VBZ allocated_VBN early_RB ._.
Allocating_VBG the_DT resources_NNS in_IN this_DT order_NN improves_VBZ the_DT quality_NN of_IN the_DT resulting_VBG lower_JJR bound_VBN ._.
The_DT Line_NNP ##_NN computes_VBZ the_DT marginal_JJ revenue_NN of_IN a_DT consumable_JJ resource_NN res_NNS for_IN each_DT task_NN ta_NN T_NN a_DT ._.
For_IN a_DT non-consumable_JJ resource_NN ,_, since_IN the_DT resource_NN is_VBZ not_RB considered_VBN in_IN the_DT state_NN space_NN ,_, all_DT other_JJ reachable_JJ states_NNS from_IN sta_NN consider_VBP that_IN the_DT resource_NN res_VBZ is_VBZ still_RB usable_JJ ._.
The_DT approach_NN here_RB is_VBZ to_TO sum_VB the_DT difference_NN between_IN the_DT real_JJ value_NN of_IN a_DT state_NN to_TO the_DT maximal_JJ Q-value_NN of_IN this_DT state_NN if_IN resource_NN res_NNS can_MD not_RB be_VB used_VBN for_IN all_DT states_NNS in_IN a_DT trajectory_NN given_VBN by_IN the_DT policy_NN of_IN task_NN ta_NN ._.
This_DT heuristic_NN proved_VBD to_TO obtain_VB good_JJ results_NNS ,_, but_CC other_JJ ones_NNS may_MD be_VB tried_VBN ,_, for_IN example_NN Monte-Carlo_NNP simulation_NN ._.
In_IN Line_NNP ##_NN ,_, the_DT marginal_JJ revenue_NN is_VBZ updated_VBN in_IN function_NN of_IN the_DT resources_NNS already_RB allocated_VBN to_TO each_DT task_NN ._.
R_NN -LRB-_-LRB- sgta_NN -RRB-_-RRB- is_VBZ the_DT reward_NN to_TO realize_VB task_NN ta_NN ._.
Thus_RB ,_, Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- valueta_NN R_NN -LRB-_-LRB- sgta_NN -RRB-_-RRB- is_VBZ the_DT residual_JJ expected_VBN value_NN that_WDT remains_VBZ to_TO be_VB achieved_VBN ,_, knowing_VBG current_JJ allocation_NN to_TO task_NN ta_NN ,_, and_CC normalized_VBN by_IN the_DT reward_NN of_IN realizing_VBG the_DT tasks_NNS ._.
The_DT marginal_JJ revenue_NN is_VBZ multiplied_VBN by_IN this_DT term_NN to_TO indicate_VB that_IN ,_, the_DT more_JJR a_DT task_NN has_VBZ a_DT high_JJ residual_JJ value_NN ,_, the_DT more_RBR its_PRP$ marginal_JJ revenue_NN is_VBZ going_VBG to_TO be_VB high_JJ ._.
Then_RB ,_, a_DT task_NN ta_NN is_VBZ selected_VBN in_IN Line_NNP ##_NN with_IN the_DT highest_JJS marginal_JJ revenue_NN ,_, adjusted_VBN with_IN residual_JJ value_NN ._.
In_IN Line_NNP ##_NN ,_, the_DT resource_NN type_NN res_NNS is_VBZ allocated_VBN to_TO the_DT group_NN of_IN resources_NNS Resta_NNP of_IN task_NN ta_NN ._.
Afterwards_RB ,_, Line_NNP ##_CD recomAlgorithm_NN #_# The_DT marginal_JJ revenue_NN lower_JJR bound_VBD algorithm_NN ._.
1_CD :_: Function_NN revenue-bound_JJ -LRB-_-LRB- S_NN -RRB-_-RRB- 2_CD :_: returns_NNS a_DT lower_JJR bound_VBN LowT_NN a_DT 3_CD :_: for_IN all_DT ta_NN T_NN a_DT do_VBP 4_CD :_: Vta_NN lrtdp_NN -LRB-_-LRB- Sta_NN -RRB-_-RRB- 5_CD :_: valueta_NN #_# 6_CD :_: end_VB for_IN 7_CD :_: s_NNS s0_VBP 8_CD :_: repeat_NN 9_CD :_: res_NNS Select_NNP a_DT resource_NN type_NN res_VBZ Res_NNP 10_CD :_: for_IN all_DT ta_NN T_NN a_DT do_VBP 11_CD :_: if_IN res_NNS is_VBZ consumable_JJ then_RB 12_CD :_: mrta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- Vta_NN -LRB-_-LRB- sta_NN -LRB-_-LRB- Res_NNP \_CD res_NNS -RRB-_-RRB- -RRB-_-RRB- 13_CD :_: else_RB 14_CD :_: mrta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- #_# 15_CD :_: repeat_NN 16_CD :_: mrta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- mrta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- +_CC Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- max_NN -LRB-_-LRB- ataA_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- |_CD res_NNS /_: ata_NN -RRB-_-RRB- Qta_NN -LRB-_-LRB- ata_NN ,_, sta_NN -RRB-_-RRB- 17_CD :_: sta_NN sta_NN ._.
pickNextState_NN -LRB-_-LRB- Resc_NN -RRB-_-RRB- 18_CD :_: until_IN sta_NN is_VBZ a_DT goal_NN 19_CD :_: s_NNS s0_VBP 20_CD :_: end_VB if_IN 21_CD :_: mrrvta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- mrta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- valueta_NN R_NN -LRB-_-LRB- sgta_NN -RRB-_-RRB- 22_CD :_: end_VB for_IN 23_CD :_: ta_NN Task_NNP ta_NN T_NN a_DT which_WDT maximize_VBP mrrvta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- 24_CD :_: Resta_NNP Resta_NNP -LCB-_-LRB- res_NNS -RCB-_-RRB- 25_CD :_: temp_NN 26_CD :_: if_IN res_NNS is_VBZ consumable_JJ then_RB 27_CD :_: temp_NN res_VBZ 28_CD :_: end_VB if_IN 29_CD :_: valueta_NN valueta_NN +_CC -LRB-_-LRB- -LRB-_-LRB- Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- valueta_NN -RRB-_-RRB- max_NN ataA_NN -LRB-_-LRB- sta_NN ,_, res_NNS -RRB-_-RRB- Qta_NN -LRB-_-LRB- ata_NN ,_, sta_NN -LRB-_-LRB- temp_NN -RRB-_-RRB- -RRB-_-RRB- Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- -RRB-_-RRB- 30_CD :_: until_IN all_DT resource_NN types_NNS res_VBZ Res_NNP are_VBP assigned_VBN 31_CD :_: for_IN all_DT ta_NN T_NN a_DT do_VBP 32_CD :_: Lowta_NN lrtdp_NN -LRB-_-LRB- Sta_NN ,_, Resta_NNP -RRB-_-RRB- 33_CD :_: end_VB for_IN 34_CD :_: return_NN LowT_NN a_DT putes_NN valueta_NN ._.
The_DT first_JJ part_NN of_IN the_DT equation_NN to_TO compute_VB valueta_NN represents_VBZ the_DT expected_VBN residual_JJ value_NN for_IN task_NN ta_NN ._.
This_DT term_NN is_VBZ multiplied_VBN by_IN max_NN ataA_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- Qta_NN -LRB-_-LRB- ata_NN ,_, sta_NN -LRB-_-LRB- res_NNS -RRB-_-RRB- -RRB-_-RRB- Vta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- ,_, which_WDT is_VBZ the_DT ratio_NN of_IN the_DT efficiency_NN of_IN resource_NN type_NN res_NNS ._.
In_IN other_JJ words_NNS ,_, valueta_NN is_VBZ assigned_VBN to_TO valueta_NN +_CC -LRB-_-LRB- the_DT residual_JJ value_NN the_DT value_NN ratio_NN of_IN resource_NN type_NN res_NNS -RRB-_-RRB- ._.
For_IN a_DT consumable_JJ resource_NN ,_, the_DT Q-value_JJ consider_VBP only_RB resource_NN res_VBZ in_IN the_DT state_NN space_NN ,_, while_IN for_IN a_DT non-consumable_JJ resource_NN ,_, no_DT resources_NNS are_VBP available_JJ ._.
All_DT resource_NN types_NNS are_VBP allocated_VBN in_IN this_DT manner_NN until_IN Res_NNP is_VBZ empty_JJ ._.
All_DT consumable_JJ and_CC non-consumable_JJ resource_NN types_NNS are_VBP allocated_VBN to_TO each_DT task_NN ._.
When_WRB all_DT resources_NNS are_VBP allocated_VBN ,_, the_DT lower_JJR bound_VBN components_NNS Lowta_NNP of_IN each_DT task_NN are_VBP computed_VBN in_IN Line_NNP ##_NN ._.
When_WRB the_DT global_JJ solution_NN is_VBZ computed_VBN ,_, the_DT lower_JJR bound_VBN is_VBZ as_IN follow_VB :_: hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ max_NN -LRB-_-LRB- SinghL_NN ,_, max_NN aA_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- taT_VBP a_DT Lowta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- We_PRP use_VBP the_DT maximum_NN of_IN the_DT SinghL_NN bound_VBD and_CC the_DT sum_NN of_IN the_DT lower_JJR bound_VBN components_NNS Lowta_NNP ,_, thus_RB marginalrevenue_JJ SinghL_NN ._.
In_IN particular_JJ ,_, the_DT SinghL_NN bound_VBD may_MD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_CD be_VB higher_JJR when_WRB a_DT little_JJ number_NN of_IN tasks_NNS remain_VBP ._.
As_IN the_DT components_NNS Lowta_NNP are_VBP computed_VBN considering_VBG s0_NN ;_: for_IN example_NN ,_, if_IN in_IN a_DT subsequent_JJ state_NN only_RB one_CD task_NN remains_VBZ ,_, the_DT bound_VBN of_IN SinghL_NNP will_MD be_VB higher_JJR than_IN any_DT of_IN the_DT Lowta_NNP components_NNS ._.
The_DT main_JJ difference_NN of_IN complexity_NN between_IN SinghL_NNP and_CC revenue-bound_JJ is_VBZ in_IN Line_NNP ##_CD where_WRB a_DT value_NN for_IN each_DT task_NN has_VBZ to_TO be_VB computed_VBN with_IN the_DT shared_JJ resource_NN ._.
However_RB ,_, since_IN the_DT resource_NN are_VBP shared_VBN ,_, the_DT state_NN space_NN and_CC action_NN space_NN is_VBZ greatly_RB reduced_VBN for_IN each_DT task_NN ,_, reducing_VBG greatly_RB the_DT calculus_NN compared_VBN to_TO the_DT value_NN functions_VBZ computed_VBN in_IN Line_NNP 4_CD which_WDT is_VBZ done_VBN for_IN both_DT SinghL_NN and_CC revenue-bound_JJ ._.
Theorem_NNP #_# ._.
#_# ._.
The_DT lower_JJR bound_VBN of_IN Equation_NN #_# is_VBZ admissible_JJ ._.
Proof_NN :_: Lowta_NN -LRB-_-LRB- sta_NN -RRB-_-RRB- is_VBZ computed_VBN with_IN the_DT resource_NN being_VBG shared_VBN ._.
Summing_VBG the_DT Lowta_NNP -LRB-_-LRB- sta_NN -RRB-_-RRB- value_NN functions_NNS for_IN each_DT ta_NN T_NN a_DT does_VBZ not_RB violates_VBZ the_DT local_JJ and_CC global_JJ resource_NN constraints_NNS ._.
Indeed_RB ,_, as_IN the_DT resources_NNS are_VBP shared_VBN ,_, the_DT tasks_NNS can_MD not_RB overuse_NN them_PRP ._.
Thus_RB ,_, hL_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- is_VBZ a_DT realizable_JJ policy_NN ,_, and_CC an_DT admissible_JJ lower_JJR bound_VBN ._.
3_LS ._.
DISCUSSION_NN AND_CC EXPERIMENTS_NNS The_DT domain_NN of_IN the_DT experiments_NNS is_VBZ a_DT naval_JJ platform_NN which_WDT must_MD counter_VB incoming_JJ missiles_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
tasks_NNS -RRB-_-RRB- by_IN using_VBG its_PRP$ resources_NNS -LRB-_-LRB- i_FW ._.
e_LS ._.
weapons_NNS ,_, movements_NNS -RRB-_-RRB- ._.
For_IN the_DT experiments_NNS ,_, 100_CD randomly_RB resource_NN allocation_NN problems_NNS were_VBD generated_VBN for_IN each_DT approach_NN ,_, and_CC possible_JJ number_NN of_IN tasks_NNS ._.
In_IN our_PRP$ problem_NN ,_, |_CD Sta_NN |_NN =_JJ #_# ,_, thus_RB each_DT task_NN can_MD be_VB in_IN four_CD distinct_JJ states_NNS ._.
There_EX are_VBP two_CD types_NNS of_IN states_NNS ;_: firstly_RB ,_, states_NNS where_WRB actions_NNS modify_VBP the_DT transition_NN probabilities_NNS ;_: and_CC then_RB ,_, there_EX are_VBP goal_NN states_NNS ._.
The_DT state_NN transitions_NNS are_VBP all_DT stochastic_JJ because_IN when_WRB a_DT missile_NN is_VBZ in_IN a_DT given_VBN state_NN ,_, it_PRP may_MD always_RB transit_NN in_IN many_JJ possible_JJ states_NNS ._.
In_IN particular_JJ ,_, each_DT resource_NN type_NN has_VBZ a_DT probability_NN to_TO counter_VB a_DT missile_NN between_IN ##_CD %_NN and_CC ##_CD %_NN depending_VBG on_IN the_DT state_NN of_IN the_DT task_NN ._.
When_WRB a_DT missile_NN is_VBZ not_RB countered_VBN ,_, it_PRP transits_VBZ to_TO another_DT state_NN ,_, which_WDT may_MD be_VB preferred_VBN or_CC not_RB to_TO the_DT current_JJ state_NN ,_, where_WRB the_DT most_RBS preferred_JJ state_NN for_IN a_DT task_NN is_VBZ when_WRB it_PRP is_VBZ countered_VBN ._.
The_DT effectiveness_NN of_IN each_DT resource_NN is_VBZ modified_VBN randomly_RB by_IN ##_CD %_NN at_IN the_DT start_NN of_IN a_DT scenario_NN ._.
There_EX are_VBP also_RB local_JJ and_CC global_JJ resource_NN constraints_NNS on_IN the_DT amount_NN that_WDT may_MD be_VB used_VBN ._.
For_IN the_DT local_JJ constraints_NNS ,_, at_IN most_JJS #_# resource_NN of_IN each_DT type_NN can_MD be_VB allocated_VBN to_TO execute_VB tasks_NNS in_IN a_DT specific_JJ state_NN ._.
This_DT constraint_NN is_VBZ also_RB present_JJ on_IN a_DT real_JJ naval_JJ platform_NN because_IN of_IN sensor_NN and_CC launcher_NN constraints_NNS and_CC engagement_NN policies_NNS ._.
Furthermore_RB ,_, for_IN consumable_JJ resources_NNS ,_, the_DT total_JJ amount_NN of_IN available_JJ consumable_JJ resource_NN is_VBZ between_IN #_# and_CC #_# for_IN each_DT type_NN ._.
The_DT global_JJ constraint_NN is_VBZ generated_VBN randomly_RB at_IN the_DT start_NN of_IN a_DT scenario_NN for_IN each_DT consumable_JJ resource_NN type_NN ._.
The_DT number_NN of_IN resource_NN type_NN has_VBZ been_VBN fixed_VBN to_TO #_# ,_, where_WRB there_EX are_VBP #_# consumable_JJ resource_NN types_NNS and_CC #_# non-consumable_JJ resources_NNS types_NNS ._.
For_IN this_DT problem_NN a_DT standard_JJ lrtdp_NN approach_NN has_VBZ been_VBN implemented_VBN ._.
A_DT simple_JJ heuristic_NN has_VBZ been_VBN used_VBN where_WRB the_DT value_NN of_IN an_DT unvisited_JJ state_NN is_VBZ assigned_VBN as_IN the_DT value_NN of_IN a_DT goal_NN state_NN such_JJ that_IN all_DT tasks_NNS are_VBP achieved_VBN ._.
This_DT way_NN ,_, the_DT value_NN of_IN each_DT unvisited_JJ state_NN is_VBZ assured_VBN to_TO overestimate_VB its_PRP$ real_JJ value_NN since_IN the_DT value_NN of_IN achieving_VBG a_DT task_NN ta_NN is_VBZ the_DT highest_JJS the_DT planner_NN may_MD get_VB for_IN ta_NN ._.
Since_IN this_DT heuristic_NN is_VBZ pretty_RB straightforward_JJ ,_, the_DT advantages_NNS of_IN using_VBG better_JJR heuristics_NNS are_VBP more_RBR evident_JJ ._.
Nevertheless_RB ,_, even_RB if_IN the_DT lrtdp_NN approach_NN uses_VBZ a_DT simple_JJ heuristic_NN ,_, still_RB a_DT huge_JJ part_NN of_IN the_DT state_NN space_NN is_VBZ not_RB visited_VBN when_WRB computing_VBG the_DT optimal_JJ policy_NN ._.
The_DT approaches_NNS described_VBN in_IN this_DT paper_NN are_VBP compared_VBN in_IN Figures_NNS #_# and_CC #_# ._.
Lets_VBZ summarize_VB these_DT approaches_NNS here_RB :_: Qdec-lrtdp_NN :_: The_DT backups_NNS are_VBP computed_VBN using_VBG the_DT Qdec-backup_JJ function_NN -LRB-_-LRB- Algorithm_NN #_# -RRB-_-RRB- ,_, but_CC in_IN a_DT lrtdp_NN context_NN ._.
In_IN particular_JJ the_DT updates_NNS made_VBN in_IN the_DT checkSolved_JJ function_NN are_VBP also_RB made_VBN using_VBG the_DT the_DT Qdecbackup_NNP function_NN ._.
lrtdp-up_NN :_: The_DT upper_JJ bound_VBN of_IN maxU_NN is_VBZ used_VBN for_IN lrtdp_NN ._.
Singh-rtdp_JJ :_: The_DT SinghL_NN and_CC SinghU_NN bounds_NNS are_VBP used_VBN for_IN bounded-rtdp_NN ._.
mr-rtdp_NN :_: The_DT revenue-bound_JJ and_CC maxU_NN bounds_NNS are_VBP used_VBN for_IN bounded-rtdp_NN ._.
To_TO implement_VB Qdec-lrtdp_NN ,_, we_PRP divided_VBD the_DT set_NN of_IN tasks_NNS in_IN two_CD equal_JJ parts_NNS ._.
The_DT set_NN of_IN task_NN T_NN ai_VBP ,_, managed_VBN by_IN agent_NN i_FW ,_, can_MD be_VB accomplished_VBN with_IN the_DT set_NN of_IN resources_NNS Resi_NNP ,_, while_IN the_DT second_JJ set_NN of_IN task_NN T_NN ai_VBP ,_, managed_VBN by_IN agent_NN Agi_NN ,_, can_MD be_VB accomplished_VBN with_IN the_DT set_NN of_IN resources_NNS Resi_NNP ._.
Resi_NNP had_VBD one_CD consumable_JJ resource_NN type_NN and_CC one_CD non-consumable_JJ resource_NN type_NN ,_, while_IN Resi_NNP had_VBD two_CD consumable_JJ resource_NN types_NNS and_CC one_CD non-consumable_JJ resource_NN type_NN ._.
When_WRB the_DT number_NN of_IN tasks_NNS is_VBZ odd_JJ ,_, one_CD more_JJR task_NN was_VBD assigned_VBN to_TO T_NN ai_VBP ._.
There_EX are_VBP constraint_NN between_IN the_DT group_NN of_IN resource_NN Resi_NN and_CC Resi_NN such_JJ that_IN some_DT assignments_NNS are_VBP not_RB possible_JJ ._.
These_DT constraints_NNS are_VBP managed_VBN by_IN the_DT arbitrator_NN as_IN described_VBN in_IN Section_NN #_# ._.
#_# ._.
Q-decomposition_NN permits_VBZ to_TO diminish_VB the_DT planning_NN time_NN significantly_RB in_IN our_PRP$ problem_NN settings_NNS ,_, and_CC seems_VBZ a_DT very_RB efficient_JJ approach_NN when_WRB a_DT group_NN of_IN agents_NNS have_VBP to_TO allocate_VB resources_NNS which_WDT are_VBP only_RB available_JJ to_TO themselves_PRP ,_, but_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN may_MD influence_VB the_DT reward_NN obtained_VBN by_IN at_IN least_JJS another_DT agent_NN ._.
To_TO compute_VB the_DT lower_JJR bound_VBN of_IN revenue-bound_JJ ,_, all_DT available_JJ resources_NNS have_VBP to_TO be_VB separated_VBN in_IN many_JJ types_NNS or_CC parts_NNS to_TO be_VB allocated_VBN ._.
For_IN our_PRP$ problem_NN ,_, we_PRP allocated_VBD each_DT resource_NN of_IN each_DT type_NN in_IN the_DT order_NN of_IN of_IN its_PRP$ specialization_NN like_IN we_PRP said_VBD when_WRB describing_VBG the_DT revenue-bound_JJ function_NN ._.
In_IN terms_NNS of_IN experiments_NNS ,_, notice_NN that_IN the_DT lrtdp_NN lrtdp-up_NN and_CC approaches_NNS for_IN resource_NN allocation_NN ,_, which_WDT doe_VBP not_RB prune_VB the_DT action_NN space_NN ,_, are_VBP much_RB more_RBR complex_JJ ._.
For_IN instance_NN ,_, it_PRP took_VBD an_DT average_NN of_IN ####_CD seconds_NNS to_TO plan_VB for_IN the_DT lrtdp-up_JJ approach_NN with_IN six_CD tasks_NNS ._.
The_DT Singh-rtdp_JJ approach_NN diminished_VBD the_DT planning_NN time_NN by_IN using_VBG a_DT lower_JJR and_CC upper_JJ bound_VBN to_TO prune_VB the_DT action_NN space_NN ._.
mr-rtdp_NN further_RBR reduce_VB the_DT planning_NN time_NN by_IN providing_VBG very_RB tight_JJ initial_JJ bounds_NNS ._.
In_IN particular_JJ ,_, Singh-rtdp_JJ needed_VBN ###_NN seconds_NNS in_IN average_JJ to_TO solve_VB problem_NN with_IN six_CD tasks_NNS and_CC mr-rtdp_NN required_VBD ##_CD seconds_NNS ._.
Indeed_RB ,_, the_DT time_NN reduction_NN is_VBZ quite_RB significant_JJ compared_VBN to_TO lrtdp-up_JJ ,_, which_WDT demonstrates_VBZ the_DT efficiency_NN of_IN using_VBG bounds_NNS to_TO prune_VB the_DT action_NN space_NN ._.
Furthermore_RB ,_, we_PRP implemented_VBD mr-rtdp_JJ with_IN the_DT SinghU_NNP bound_VBD ,_, and_CC this_DT was_VBD slightly_RB less_RBR efficient_JJ than_IN with_IN the_DT maxU_NN bound_VBD ._.
We_PRP also_RB implemented_VBD mr-rtdp_JJ with_IN the_DT SinghL_NN bound_VBD ,_, and_CC this_DT was_VBD slightly_RB more_RBR efficient_JJ than_IN Singh-rtdp_JJ ._.
From_IN these_DT results_NNS ,_, we_PRP conclude_VBP that_IN the_DT difference_NN of_IN efficiency_NN between_IN mr-rtdp_JJ and_CC Singh-rtdp_JJ is_VBZ more_RBR attributable_JJ to_TO the_DT marginal-revenue_NN lower_JJR bound_VBD that_IN to_TO the_DT maxU_NN upper_JJ bound_VBN ._.
Indeed_RB ,_, when_WRB the_DT number_NN of_IN task_NN to_TO execute_VB is_VBZ high_JJ ,_, the_DT lower_JJR bounds_NNS by_IN Singh-rtdp_JJ takes_VBZ the_DT values_NNS of_IN a_DT single_JJ task_NN ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT lower_JJR bound_VBN of_IN mr-rtdp_JJ takes_VBZ into_IN account_NN the_DT value_NN of_IN all_DT 1218_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- 0_CD ._.
##_NN 0_CD ._.
#_# 1_CD 10_CD 100_CD 1000_CD 10000_CD 100000_CD 1_CD #_# #_# #_# #_# #_# #_# #_# #_# ##_CD ##_CD ##_CD ##_CD Timeinseconds_NNPS Number_NNP of_IN tasks_NNS LRTDP_NNP QDEC-LRTDP_NNP Figure_NNP #_# :_: Efficiency_NN of_IN Q-decomposition_JJ LRTDP_NN and_CC LRTDP_NN ._.
0_CD ._.
##_NN 0_CD ._.
#_# 1_CD 10_CD 100_CD 1000_CD 10000_CD 1_CD #_# #_# #_# #_# #_# #_# #_# Timeinseconds_NNPS Number_NNP of_IN tasks_NNS LRTDP_VBP LRTDP-up_JJ Singh-RTDP_JJ MR-RTDP_NN Figure_NN #_# :_: Efficiency_NN of_IN MR-RTDP_NN compared_VBN to_TO SINGH-RTDP_NN ._.
task_NN by_IN using_VBG a_DT heuristic_NN to_TO distribute_VB the_DT resources_NNS ._.
Indeed_RB ,_, an_DT optimal_JJ allocation_NN is_VBZ one_CD where_WRB the_DT resources_NNS are_VBP distributed_VBN in_IN the_DT best_JJS way_NN to_TO all_DT tasks_NNS ,_, and_CC our_PRP$ lower_JJR bound_VBD heuristically_RB does_VBZ that_IN ._.
4_LS ._.
CONCLUSION_NN The_DT experiments_NNS have_VBP shown_VBN that_IN Q-decomposition_NN seems_VBZ a_DT very_RB efficient_JJ approach_NN when_WRB a_DT group_NN of_IN agents_NNS have_VBP to_TO allocate_VB resources_NNS which_WDT are_VBP only_RB available_JJ to_TO themselves_PRP ,_, but_CC the_DT actions_NNS made_VBN by_IN an_DT agent_NN may_MD influence_VB the_DT reward_NN obtained_VBN by_IN at_IN least_JJS another_DT agent_NN ._.
On_IN the_DT other_JJ hand_NN ,_, when_WRB the_DT available_JJ resource_NN are_VBP shared_VBN ,_, no_DT Q-decomposition_NN is_VBZ possible_JJ and_CC we_PRP proposed_VBD tight_JJ bounds_NNS for_IN heuristic_NN search_NN ._.
In_IN this_DT case_NN ,_, the_DT planning_NN time_NN of_IN bounded-rtdp_NN ,_, which_WDT prunes_VBZ the_DT action_NN space_NN ,_, is_VBZ significantly_RB lower_JJR than_IN for_IN lrtdp_NN ._.
Furthermore_RB ,_, The_DT marginal_JJ revenue_NN bound_VBD proposed_VBN in_IN this_DT paper_NN compares_VBZ favorably_RB to_TO the_DT Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- approach_NN ._.
boundedrtdp_NN with_IN our_PRP$ proposed_VBN bounds_NNS may_MD apply_VB to_TO a_DT wide_JJ range_NN of_IN stochastic_JJ environments_NNS ._.
The_DT only_JJ condition_NN for_IN the_DT use_NN our_PRP$ bounds_NNS is_VBZ that_IN each_DT task_NN possesses_VBZ consumable_JJ and_CC /_: or_CC non-consumable_JJ limited_JJ resources_NNS ._.
An_DT interesting_JJ research_NN avenue_NN would_MD be_VB to_TO experiment_NN our_PRP$ bounds_NNS with_IN other_JJ heuristic_NN search_NN algorithms_NNS ._.
For_IN instance_NN ,_, frtdp_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, and_CC brtdp_NN -LSB-_-LRB- #_# -RSB-_-RRB- are_VBP both_DT efficient_JJ heuristic_NN search_NN algorithms_NNS ._.
In_IN particular_JJ ,_, both_DT these_DT approaches_NNS proposed_VBD an_DT efficient_JJ state_NN trajectory_NN updates_NNS ,_, when_WRB given_VBN upper_JJ and_CC lower_JJR bounds_NNS ._.
Our_PRP$ tight_JJ bounds_NNS would_MD enable_VB ,_, for_IN both_DT frtdp_NN and_CC brtdp_NN ,_, to_TO reduce_VB the_DT number_NN of_IN backup_NN to_TO perform_VB before_IN convergence_NN ._.
Finally_RB ,_, the_DT bounded-rtdp_JJ function_NN prunes_VBZ the_DT action_NN space_NN when_WRB QU_NNP -LRB-_-LRB- a_DT ,_, s_NNS -RRB-_-RRB- L_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ,_, as_IN Singh_NNP and_CC Cohn_NNP -LSB-_-LRB- ##_CD -RSB-_-RRB- suggested_VBD ._.
frtdp_NN and_CC brtdp_NN could_MD also_RB prune_VB the_DT action_NN space_NN in_IN these_DT circumstances_NNS to_TO further_RBR reduce_VB their_PRP$ planning_NN time_NN ._.
5_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
Barto_NNP ,_, S_NN ._.
Bradtke_NNP ,_, and_CC S_NN ._.
Singh_NNP ._.
Learning_VBG to_TO act_VB using_VBG real-time_JJ dynamic_JJ programming_NN ._.
Artificial_JJ Intelligence_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 81-138_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
Beynier_NNP and_CC A_NNP ._.
I_PRP ._.
Mouaddib_NNP ._.
An_DT iterative_JJ algorithm_NN for_IN solving_VBG constrained_VBN decentralized_JJ markov_NN decision_NN processes_NNS ._.
In_IN Proceeding_VBG of_IN the_DT Twenty-First_NNP National_NNP Conference_NNP on_IN Artificial_NNP Intelligence_NNP -LRB-_-LRB- AAAI-06_NN -RRB-_-RRB- ,_, ####_NN ._.
-LSB-_-LRB- #_# -RSB-_-RRB- B_NN ._.
Bonet_NNP and_CC H_NNP ._.
Geffner_NNP ._.
Faster_JJR heuristic_NN search_NN algorithms_NNS for_IN planning_VBG with_IN uncertainty_NN and_CC full_JJ feedback_NN ._.
In_IN Proceedings_NNP of_IN the_DT Eighteenth_NNP International_NNP Joint_NNP Conference_NNP on_IN Artificial_NNP Intelligence_NNP -LRB-_-LRB- IJCAI-03_NN -RRB-_-RRB- ,_, August_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- B_NN ._.
Bonet_NNP and_CC H_NNP ._.
Geffner_NNP ._.
Labeled_VBN lrtdp_JJ approach_NN :_: Improving_NN the_DT convergence_NN of_IN real-time_JJ dynamic_JJ programming_NN ._.
In_IN Proceeding_VBG of_IN the_DT Thirteenth_NNP International_NNP Conference_NNP on_IN Automated_NNP Planning_NNP &_CC Scheduling_NNP -LRB-_-LRB- ICAPS-03_NN -RRB-_-RRB- ,_, pages_NNS 12-21_CD ,_, Trento_NNP ,_, Italy_NNP ,_, 2003_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- E_NN ._.
A_DT ._.
Hansen_NNP and_CC S_NN ._.
Zilberstein_NN ._.
lao_NN :_: A_DT heuristic_NN search_NN algorithm_NN that_WDT finds_VBZ solutions_NNS with_IN loops_NNS ._.
Artificial_JJ Intelligence_NNP ,_, ###_CD -LRB-_-LRB- 1-2_CD -RRB-_-RRB- :_: 35-62_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- H_NN ._.
B_NN ._.
McMahan_NNP ,_, M_NN ._.
Likhachev_NNP ,_, and_CC G_NN ._.
J_NN ._.
Gordon_NNP ._.
Bounded_VBN real-time_JJ dynamic_JJ programming_NN :_: rtdp_NN with_IN monotone_NN upper_JJ bounds_NNS and_CC performance_NN guarantees_NNS ._.
In_IN ICML_NN ''_'' ##_NN :_: Proceedings_NNP of_IN the_DT Twenty-Second_NNP International_NNP Conference_NNP on_IN Machine_NN learning_NN ,_, pages_NNS 569-576_CD ,_, New_NNP York_NNP ,_, NY_NNP ,_, USA_NNP ,_, ####_CD ._.
ACM_NNP Press_NNP ._.
-LSB-_-LRB- #_# -RSB-_-RRB- R_NN ._.
S_NN ._.
Pindyck_NNP and_CC D_NNP ._.
L_NN ._.
Rubinfeld_NNP ._.
Microeconomics_NNS ._.
Prentice_NNP Hall_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- G_NN ._.
A_DT ._.
Rummery_NNP and_CC M_NN ._.
Niranjan_NNP ._.
On-line_JJ Q-learning_NN using_VBG connectionist_NN systems_NNS ._.
Technical_NNP report_NN CUED_VBD /_: FINFENG_NNP /_: TR_NN ###_CD ,_, Cambridge_NNP University_NNP Engineering_NNP Department_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- S_NN ._.
J_NN ._.
Russell_NNP and_CC A_NNP ._.
Zimdars_NNP ._.
Q-decomposition_NN for_IN reinforcement_NN learning_VBG agents_NNS ._.
In_IN ICML_NN ,_, pages_NNS 656-663_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Singh_NNP and_CC D_NNP ._.
Cohn_NNP ._.
How_WRB to_TO dynamically_RB merge_VB markov_NN decision_NN processes_NNS ._.
In_IN Advances_NNS in_IN Neural_NNP Information_NNP Processing_NNP Systems_NNPS ,_, volume_NN ##_NN ,_, pages_NNS 1057-1063_CD ,_, Cambridge_NNP ,_, MA_NNP ,_, USA_NNP ,_, ####_CD ._.
MIT_NNP Press_NNP ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- T_NN ._.
Smith_NNP and_CC R_NN ._.
Simmons_NNP ._.
Focused_VBN real-time_JJ dynamic_JJ programming_NN for_IN mdps_NNS :_: Squeezing_VBG more_JJR out_IN of_IN a_DT heuristic_NN ._.
In_IN Proceedings_NNP of_IN the_DT Twenty-First_NNP National_NNP Conference_NNP on_IN Artificial_NNP Intelligence_NNP -LRB-_-LRB- AAAI_NNP -RRB-_-RRB- ,_, Boston_NNP ,_, USA_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- W_NN ._.
Zhang_NNP ._.
Modeling_NN and_CC solving_VBG a_DT resource_NN allocation_NN problem_NN with_IN soft_JJ constraint_NN techniques_NNS ._.
Technical_NNP report_NN :_: wucs-2002-13_NN ,_, Washington_NNP University_NNP ,_, Saint-Louis_NNP ,_, Missouri_NNP ,_, ####_CD ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_CD
