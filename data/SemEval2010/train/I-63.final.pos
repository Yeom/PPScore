Combinatorial_JJ Resource_NNP Scheduling_NN for_IN Multiagent_JJ MDPs_NNS Dmitri_NNP A_NNP ._.
Dolgov_NNP ,_, Michael_NNP R_NNP ._.
James_NNP ,_, and_CC Michael_NNP E_NNP ._.
Samples_NNS AI_NN and_CC Robotics_NNP Group_NNP Technical_NNP Research_NNP ,_, Toyota_NNP Technical_NNP Center_NNP USA_NNP -LCB-_-LRB- ddolgov_NN ,_, michael_NN ._.
r_NN ._.
james_NNS ,_, michael_NN ._.
samples_NNS -RCB-_-RRB- @_SYM gmail_NN ._.
com_NN ABSTRACT_NN Optimal_JJ resource_NN scheduling_NN in_IN multiagent_JJ systems_NNS is_VBZ a_DT computationally_RB challenging_JJ task_NN ,_, particularly_RB when_WRB the_DT values_NNS of_IN resources_NNS are_VBP not_RB additive_JJ ._.
We_PRP consider_VBP the_DT combinatorial_JJ problem_NN of_IN scheduling_VBG the_DT usage_NN of_IN multiple_JJ resources_NNS among_IN agents_NNS that_WDT operate_VBP in_IN stochastic_JJ environments_NNS ,_, modeled_VBN as_IN Markov_NNP decision_NN processes_NNS -LRB-_-LRB- MDPs_NNS -RRB-_-RRB- ._.
In_IN recent_JJ years_NNS ,_, efficient_JJ resource-allocation_JJ algorithms_NNS have_VBP been_VBN developed_VBN for_IN agents_NNS with_IN resource_NN values_NNS induced_VBN by_IN MDPs_NNS ._.
However_RB ,_, this_DT prior_JJ work_NN has_VBZ focused_VBN on_IN static_JJ resource-allocation_JJ problems_NNS where_WRB resources_NNS are_VBP distributed_VBN once_RB and_CC then_RB utilized_VBN in_IN infinite-horizon_NN MDPs_NNS ._.
We_PRP extend_VBP those_DT existing_VBG models_NNS to_TO the_DT problem_NN of_IN combinatorial_JJ resource_NN scheduling_NN ,_, where_WRB agents_NNS persist_VBP only_RB for_IN finite_JJ periods_NNS between_IN their_PRP$ -LRB-_-LRB- predefined_VBN -RRB-_-RRB- arrival_NN and_CC departure_NN times_NNS ,_, requiring_VBG resources_NNS only_RB for_IN those_DT time_NN periods_NNS ._.
We_PRP provide_VBP a_DT computationally_RB efficient_JJ procedure_NN for_IN computing_VBG globally_RB optimal_JJ resource_NN assignments_NNS to_TO agents_NNS over_IN time_NN ._.
We_PRP illustrate_VBP and_CC empirically_RB analyze_VBP the_DT method_NN in_IN the_DT context_NN of_IN a_DT stochastic_JJ jobscheduling_NN domain_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNS I_PRP ._.
#_# ._.
#_# -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Problem_NNP Solving_VBG ,_, Control_NNP Methods_NNS ,_, and_CC Search_VB ;_: I_PRP ._.
#_# ._.
##_NN -LSB-_-LRB- Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Distributed_VBN Artificial_NNP Intelligence-Multiagent_NNP systems_NNS General_NNP Terms_NNS Algorithms_NNS ,_, Performance_NNP ,_, Design_NN 1_CD ._.
INTRODUCTION_NN The_DT tasks_NNS of_IN optimal_JJ resource_NN allocation_NN and_CC scheduling_NN are_VBP ubiquitous_JJ in_IN multiagent_JJ systems_NNS ,_, but_CC solving_VBG such_JJ optimization_NN problems_NNS can_MD be_VB computationally_RB difficult_JJ ,_, due_JJ to_TO a_DT number_NN of_IN factors_NNS ._.
In_IN particular_JJ ,_, when_WRB the_DT value_NN of_IN a_DT set_NN of_IN resources_NNS to_TO an_DT agent_NN is_VBZ not_RB additive_JJ -LRB-_-LRB- as_RB is_VBZ often_RB the_DT case_NN with_IN resources_NNS that_WDT are_VBP substitutes_NNS or_CC complements_VBZ -RRB-_-RRB- ,_, the_DT utility_NN function_NN might_MD have_VB to_TO be_VB defined_VBN on_IN an_DT exponentially_RB large_JJ space_NN of_IN resource_NN bundles_NNS ,_, which_WDT very_RB quickly_RB becomes_VBZ computationally_RB intractable_JJ ._.
Further_RB ,_, even_RB when_WRB each_DT agent_NN has_VBZ a_DT utility_NN function_NN that_WDT is_VBZ nonzero_NN only_RB on_IN a_DT small_JJ subset_NN of_IN the_DT possible_JJ resource_NN bundles_NNS ,_, obtaining_VBG optimal_JJ allocation_NN is_VBZ still_RB computationally_RB prohibitive_JJ ,_, as_IN the_DT problem_NN becomes_VBZ NP-complete_JJ -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
Such_JJ computational_JJ issues_NNS have_VBP recently_RB spawned_VBN several_JJ threads_NNS of_IN work_NN in_IN using_VBG compact_JJ models_NNS of_IN agents_NNS ''_'' preferences_NNS ._.
One_CD idea_NN is_VBZ to_TO use_VB any_DT structure_NN present_JJ in_IN utility_NN functions_NNS to_TO represent_VB them_PRP compactly_RB ,_, via_IN ,_, for_IN example_NN ,_, logical_JJ formulas_NNS -LSB-_-LRB- ##_CD ,_, ##_CD ,_, #_# ,_, #_# -RSB-_-RRB- ._.
An_DT alternative_NN is_VBZ to_TO directly_RB model_VB the_DT mechanisms_NNS that_WDT define_VBP the_DT agents_NNS ''_'' utility_NN functions_NNS and_CC perform_VB resource_NN allocation_NN directly_RB with_IN these_DT models_NNS -LSB-_-LRB- #_# -RSB-_-RRB- ._.
A_DT way_NN of_IN accomplishing_VBG this_DT is_VBZ to_TO model_VB the_DT processes_NNS by_IN which_WDT an_DT agent_NN might_MD utilize_VB the_DT resources_NNS and_CC define_VB the_DT utility_NN function_NN as_IN the_DT payoff_NN of_IN these_DT processes_NNS ._.
In_IN particular_JJ ,_, if_IN an_DT agent_NN uses_VBZ resources_NNS to_TO act_VB in_IN a_DT stochastic_JJ environment_NN ,_, its_PRP$ utility_NN function_NN can_MD be_VB naturally_RB modeled_VBN with_IN a_DT Markov_NNP decision_NN process_NN ,_, whose_WP$ action_NN set_NN is_VBZ parameterized_VBN by_IN the_DT available_JJ resources_NNS ._.
This_DT representation_NN can_MD then_RB be_VB used_VBN to_TO construct_VB very_RB efficient_JJ resource-allocation_JJ algorithms_NNS that_WDT lead_VBP to_TO an_DT exponential_JJ speedup_NN over_IN a_DT straightforward_JJ optimization_NN problem_NN with_IN flat_JJ representations_NNS of_IN combinatorial_JJ preferences_NNS -LSB-_-LRB- #_# ,_, #_# ,_, #_# -RSB-_-RRB- ._.
However_RB ,_, this_DT existing_VBG work_NN on_IN resource_NN allocation_NN with_IN preferences_NNS induced_VBN by_IN resource-parameterized_JJ MDPs_NNS makes_VBZ an_DT assumption_NN that_IN the_DT resources_NNS are_VBP only_RB allocated_VBN once_RB and_CC are_VBP then_RB utilized_VBN by_IN the_DT agents_NNS independently_RB within_IN their_PRP$ infinite-horizon_NN MDPs_NNS ._.
This_DT assumption_NN that_IN no_DT reallocation_NN of_IN resources_NNS is_VBZ possible_JJ can_MD be_VB limiting_VBG in_IN domains_NNS where_WRB agents_NNS arrive_VBP and_CC depart_VBP dynamically_RB ._.
In_IN this_DT paper_NN ,_, we_PRP extend_VBP the_DT work_NN on_IN resource_NN allocation_NN under_IN MDP-induced_JJ preferences_NNS to_TO discrete-time_JJ scheduling_NN problems_NNS ,_, where_WRB agents_NNS are_VBP present_JJ in_IN the_DT system_NN for_IN finite_JJ time_NN intervals_NNS and_CC can_MD only_RB use_VB resources_NNS within_IN these_DT intervals_NNS ._.
In_IN particular_JJ ,_, agents_NNS arrive_VBP and_CC depart_VBP at_IN arbitrary_JJ -LRB-_-LRB- predefined_JJ -RRB-_-RRB- times_NNS and_CC within_IN these_DT intervals_NNS use_VBP resources_NNS to_TO execute_VB tasks_NNS in_IN finite-horizon_JJ MDPs_NNS ._.
We_PRP address_VBP the_DT problem_NN of_IN globally_RB optimal_JJ resource_NN scheduling_NN ,_, where_WRB the_DT objective_NN is_VBZ to_TO find_VB an_DT allocation_NN of_IN resources_NNS to_TO the_DT agents_NNS across_IN time_NN that_IN maximizes_VBZ the_DT sum_NN of_IN the_DT expected_VBN rewards_NNS that_IN they_PRP obtain_VBP ._.
In_IN this_DT context_NN ,_, our_PRP$ main_JJ contribution_NN is_VBZ a_DT mixed-integerprogramming_JJ formulation_NN of_IN the_DT scheduling_NN problem_NN that_WDT chooses_VBZ globally_RB optimal_JJ resource_NN assignments_NNS ,_, starting_VBG times_NNS ,_, and_CC execution_NN horizons_NNS for_IN all_DT agents_NNS -LRB-_-LRB- within_IN their_PRP$ arrival1220_NN 978-81-904262-7-5_CD -LRB-_-LRB- RPS_NN -RRB-_-RRB- c_NN ####_CD IFAAMAS_NNP departure_NN intervals_NNS -RRB-_-RRB- ._.
We_PRP analyze_VBP and_CC empirically_RB compare_VB two_CD flavors_NNS of_IN the_DT scheduling_NN problem_NN :_: one_CD ,_, where_WRB agents_NNS have_VBP static_JJ resource_NN assignments_NNS within_IN their_PRP$ finite-horizon_JJ MDPs_NNS ,_, and_CC another_DT ,_, where_WRB resources_NNS can_MD be_VB dynamically_RB reallocated_VBN between_IN agents_NNS at_IN every_DT time_NN step_NN ._.
In_IN the_DT rest_NN of_IN the_DT paper_NN ,_, we_PRP first_RB lay_VBD down_RP the_DT necessary_JJ groundwork_NN in_IN Section_NN #_# and_CC then_RB introduce_VB our_PRP$ model_NN and_CC formal_JJ problem_NN statement_NN in_IN Section_NN #_# ._.
In_IN Section_NN #_# ._.
#_# ,_, we_PRP describe_VBP our_PRP$ main_JJ result_NN ,_, the_DT optimization_NN program_NN for_IN globally_RB optimal_JJ resource_NN scheduling_NN ._.
Following_VBG the_DT discussion_NN of_IN our_PRP$ experimental_JJ results_NNS on_IN a_DT job-scheduling_JJ problem_NN in_IN Section_NN #_# ,_, we_PRP conclude_VBP in_IN Section_NN #_# with_IN a_DT discussion_NN of_IN possible_JJ extensions_NNS and_CC generalizations_NNS of_IN our_PRP$ method_NN ._.
2_LS ._.
BACKGROUND_NN Similarly_RB to_TO the_DT model_NN used_VBN in_IN previous_JJ work_NN on_IN resourceallocation_NN with_IN MDP-induced_JJ preferences_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ,_, we_PRP define_VBP the_DT value_NN of_IN a_DT set_NN of_IN resources_NNS to_TO an_DT agent_NN as_IN the_DT value_NN of_IN the_DT best_JJS MDP_NN policy_NN that_WDT is_VBZ realizable_JJ ,_, given_VBN those_DT resources_NNS ._.
However_RB ,_, since_IN the_DT focus_NN of_IN our_PRP$ work_NN is_VBZ on_IN scheduling_NN problems_NNS ,_, and_CC a_DT large_JJ part_NN of_IN the_DT optimization_NN problem_NN is_VBZ to_TO decide_VB how_WRB resources_NNS are_VBP allocated_VBN in_IN time_NN among_IN agents_NNS with_IN finite_JJ arrival_NN and_CC departure_NN times_NNS ,_, we_PRP model_VBP the_DT agents_NNS ''_'' planning_VBG problems_NNS as_IN finite-horizon_JJ MDPs_NNS ,_, in_IN contrast_NN to_TO previous_JJ work_NN that_WDT used_VBD infinite-horizon_JJ discounted_JJ MDPs_NNS ._.
In_IN the_DT rest_NN of_IN this_DT section_NN ,_, we_PRP first_RB introduce_VB some_DT necessary_JJ background_NN on_IN finite-horizon_JJ MDPs_NNS and_CC present_VB a_DT linear-programming_JJ formulation_NN that_WDT serves_VBZ as_IN the_DT basis_NN for_IN our_PRP$ solution_NN algorithm_NN developed_VBN in_IN Section_NN #_# ._.
We_PRP also_RB outline_VBP the_DT standard_JJ methods_NNS for_IN combinatorial_JJ resource_NN scheduling_NN with_IN flat_JJ resource_NN values_NNS ,_, which_WDT serve_VBP as_IN a_DT comparison_NN benchmark_NN for_IN the_DT new_JJ model_NN developed_VBN here_RB ._.
2_LS ._.
#_# Markov_NNP Decision_NNP Processes_NNPS A_DT stationary_JJ ,_, finite-domain_JJ ,_, discrete-time_JJ MDP_NN -LRB-_-LRB- see_VB ,_, for_IN example_NN ,_, -LSB-_-LRB- ##_CD -RSB-_-RRB- for_IN a_DT thorough_JJ and_CC detailed_JJ development_NN -RRB-_-RRB- can_MD be_VB described_VBN as_IN S_NN ,_, A_NN ,_, p_NN ,_, r_NN ,_, where_WRB :_: S_NN is_VBZ a_DT finite_JJ set_NN of_IN system_NN states_NNS ;_: A_NN is_VBZ a_DT finite_JJ set_NN of_IN actions_NNS that_WDT are_VBP available_JJ to_TO the_DT agent_NN ;_: p_NN is_VBZ a_DT stationary_JJ stochastic_JJ transition_NN function_NN ,_, where_WRB p_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- is_VBZ the_DT probability_NN of_IN transitioning_VBG to_TO state_NN upon_IN executing_VBG action_NN a_DT in_IN state_NN s_NNS ;_: r_NN is_VBZ a_DT stationary_JJ reward_NN function_NN ,_, where_WRB r_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- specifies_VBZ the_DT reward_NN obtained_VBN upon_IN executing_VBG action_NN a_DT in_IN state_NN s_NNS ._.
Given_VBN such_JJ an_DT MDP_NN ,_, a_DT decision_NN problem_NN under_IN a_DT finite_JJ horizon_NN T_NN is_VBZ to_TO choose_VB an_DT optimal_JJ action_NN at_IN every_DT time_NN step_NN to_TO maximize_VB the_DT expected_VBN value_NN of_IN the_DT total_JJ reward_NN accrued_VBN during_IN the_DT agent_NN ''_'' s_NNS -LRB-_-LRB- finite_JJ -RRB-_-RRB- lifetime_NN ._.
The_DT agent_NN ''_'' s_NNS optimal_JJ policy_NN is_VBZ then_RB a_DT function_NN of_IN current_JJ state_NN s_NNS and_CC the_DT time_NN until_IN the_DT horizon_NN ._.
An_DT optimal_JJ policy_NN for_IN such_JJ a_DT problem_NN is_VBZ to_TO act_VB greedily_RB with_IN respect_NN to_TO the_DT optimal_JJ value_NN function_NN ,_, defined_VBN recursively_RB by_IN the_DT following_VBG system_NN of_IN finite-time_JJ Bellman_NNP equations_NNS -LSB-_-LRB- #_# -RSB-_-RRB- :_: v_LS -LRB-_-LRB- s_NNS ,_, t_NN -RRB-_-RRB- =_JJ max_NN a_DT r_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- +_CC X_NN p_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- v_LS -LRB-_-LRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ,_, s_VBZ S_NN ,_, t_NN -LSB-_-LRB- #_# ,_, T_NN #_# -RSB-_-RRB- ;_: v_LS -LRB-_-LRB- s_NNS ,_, T_NN -RRB-_-RRB- =_JJ #_# ,_, s_VBZ S_NN ;_: where_WRB v_LS -LRB-_-LRB- s_NNS ,_, t_NN -RRB-_-RRB- is_VBZ the_DT optimal_JJ value_NN of_IN being_VBG in_IN state_NN s_NNS at_IN time_NN t_NN -LSB-_-LRB- #_# ,_, T_NN -RSB-_-RRB- ._.
This_DT optimal_JJ value_NN function_NN can_MD be_VB easily_RB computed_VBN using_VBG dynamic_JJ programming_NN ,_, leading_VBG to_TO the_DT following_VBG optimal_JJ policy_NN ,_, where_WRB -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- is_VBZ the_DT probability_NN of_IN executing_VBG action_NN a_DT in_IN state_NN s_NNS at_IN time_NN t_NN :_: -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- =_JJ -LRB-_-LRB- 1_CD ,_, a_DT =_JJ argmaxa_NN r_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- +_CC P_NN p_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- v_LS -LRB-_-LRB- ,_, t_NN +_CC #_# -RRB-_-RRB- ,_, 0_CD ,_, otherwise_RB ._.
The_DT above_JJ is_VBZ the_DT most_RBS common_JJ way_NN of_IN computing_VBG the_DT optimal_JJ value_NN function_NN -LRB-_-LRB- and_CC therefore_RB an_DT optimal_JJ policy_NN -RRB-_-RRB- for_IN a_DT finite-horizon_JJ MDP_NN ._.
However_RB ,_, we_PRP can_MD also_RB formulate_VB the_DT problem_NN as_IN the_DT following_VBG linear_JJ program_NN -LRB-_-LRB- similarly_RB to_TO the_DT dual_JJ LP_NN for_IN infinite-horizon_NN discounted_VBD MDPs_NNS -LSB-_-LRB- ##_NNS ,_, #_# ,_, #_# -RSB-_-RRB- -RRB-_-RRB- :_: max_NN X_NN s_VBZ X_NN a_DT r_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- X_NN t_NN x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- subject_NN to_TO :_: X_NN a_DT x_NN -LRB-_-LRB- ,_, a_DT ,_, t_NN +_CC #_# -RRB-_-RRB- =_JJ X_NN s_NNS ,_, a_DT p_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- ,_, t_NN -LSB-_-LRB- #_# ,_, T_NN #_# -RSB-_-RRB- ;_: X_NN a_DT x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, #_# -RRB-_-RRB- =_JJ -LRB-_-LRB- s_NNS -RRB-_-RRB- ,_, s_VBZ S_NN ;_: -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB -LRB-_-LRB- s_NNS -RRB-_-RRB- is_VBZ the_DT initial_JJ distribution_NN over_IN the_DT state_NN space_NN ,_, and_CC x_NN is_VBZ the_DT -LRB-_-LRB- non-stationary_JJ -RRB-_-RRB- occupation_NN measure_NN -LRB-_-LRB- x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- is_VBZ the_DT total_NN expected_VBN number_NN of_IN times_NNS action_NN a_DT is_VBZ executed_VBN in_IN state_NN s_NNS at_IN time_NN t_NN -RRB-_-RRB- ._.
An_DT optimal_JJ -LRB-_-LRB- non-stationary_JJ -RRB-_-RRB- policy_NN is_VBZ obtained_VBN from_IN the_DT occupation_NN measure_NN as_IN follows_VBZ :_: -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- =_JJ x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- /_: X_NN a_DT x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- s_VBZ S_NN ,_, t_NN -LSB-_-LRB- #_# ,_, T_NN -RSB-_-RRB- ._.
-LRB-_-LRB- #_# -RRB-_-RRB- Note_VBP that_IN the_DT standard_JJ unconstrained_JJ finite-horizon_NN MDP_NN ,_, as_IN described_VBN above_IN ,_, always_RB has_VBZ a_DT uniformly-optimal_JJ solution_NN -LRB-_-LRB- optimal_JJ for_IN any_DT initial_JJ distribution_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- -RRB-_-RRB- ._.
Therefore_RB ,_, an_DT optimal_JJ policy_NN can_MD be_VB obtained_VBN by_IN using_VBG an_DT arbitrary_JJ constant_JJ -LRB-_-LRB- s_NNS -RRB-_-RRB- >_JJR #_# -LRB-_-LRB- in_IN particular_JJ ,_, -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ #_# will_MD result_VB in_IN x_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- =_JJ -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- -RRB-_-RRB- ._.
However_RB ,_, for_IN MDPs_NNS with_IN resource_NN constraints_NNS -LRB-_-LRB- as_IN defined_VBN below_IN in_IN Section_NN #_# -RRB-_-RRB- ,_, uniformly-optimal_JJ policies_NNS do_VBP not_RB in_IN general_JJ exist_VBP ._.
In_IN such_JJ cases_NNS ,_, becomes_VBZ a_DT part_NN of_IN the_DT problem_NN input_NN ,_, and_CC a_DT resulting_VBG policy_NN is_VBZ only_RB optimal_JJ for_IN that_DT particular_JJ ._.
This_DT result_NN is_VBZ well_RB known_VBN for_IN infinite-horizon_NN MDPs_NNS with_IN various_JJ types_NNS of_IN constraints_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ,_, and_CC it_PRP also_RB holds_VBZ for_IN our_PRP$ finite-horizon_JJ model_NN ,_, which_WDT can_MD be_VB easily_RB established_VBN via_IN a_DT line_NN of_IN reasoning_NN completely_RB analogous_JJ to_TO the_DT arguments_NNS in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
2_LS ._.
#_# Combinatorial_NNP Resource_NNP Scheduling_NNP A_NNP straightforward_JJ approach_NN to_TO resource_NN scheduling_NN for_IN a_DT set_NN of_IN agents_NNS M_NN ,_, whose_WP$ values_NNS for_IN the_DT resources_NNS are_VBP induced_VBN by_IN stochastic_JJ planning_NN problems_NNS -LRB-_-LRB- in_IN our_PRP$ case_NN ,_, finite-horizon_JJ MDPs_NNS -RRB-_-RRB- would_MD be_VB to_TO have_VB each_DT agent_NN enumerate_VB all_DT possible_JJ resource_NN assignments_NNS over_IN time_NN and_CC ,_, for_IN each_DT one_CD ,_, compute_VB its_PRP$ value_NN by_IN solving_VBG the_DT corresponding_JJ MDP_NN ._.
Then_RB ,_, each_DT agent_NN would_MD provide_VB valuations_NNS for_IN each_DT possible_JJ resource_NN bundle_NN over_IN time_NN to_TO a_DT centralized_JJ coordinator_NN ,_, who_WP would_MD compute_VB the_DT optimal_JJ resource_NN assignments_NNS across_IN time_NN based_VBN on_IN these_DT valuations_NNS ._.
When_WRB resources_NNS can_MD be_VB allocated_VBN at_IN different_JJ times_NNS to_TO different_JJ agents_NNS ,_, each_DT agent_NN must_MD submit_VB valuations_NNS for_IN every_DT combination_NN of_IN possible_JJ time_NN horizons_NNS ._.
Let_VB each_DT agent_NN m_NN M_NN execute_VB its_PRP$ MDP_NN within_IN the_DT arrival-departure_JJ time_NN interval_JJ -LSB-_-LRB- a_DT m_NN ,_, d_NN m_NN -RSB-_-RRB- ._.
Hence_RB ,_, agent_NN m_NN will_MD execute_VB an_DT MDP_NN with_IN time_NN horizon_NN no_RB greater_JJR than_IN Tm_NN =_JJ d_NN ma_NN m_NN +_CC #_# ._.
Let_VB b_NN be_VB the_DT global_JJ time_NN horizon_NN for_IN the_DT problem_NN ,_, before_IN which_WDT all_DT of_IN the_DT agents_NNS ''_'' MDPs_NNS must_MD finish_VB ._.
We_PRP assume_VBP d_NN m_NN <_JJR b_NN ,_, m_NN M_NN ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_NN For_IN the_DT scheduling_NN problem_NN where_WRB agents_NNS have_VBP static_JJ resource_NN requirements_NNS within_IN their_PRP$ finite-horizon_JJ MDPs_NNS ,_, the_DT agents_NNS provide_VBP a_DT valuation_NN for_IN each_DT resource_NN bundle_NN for_IN each_DT possible_JJ time_NN horizon_NN -LRB-_-LRB- from_IN -LSB-_-LRB- #_# ,_, Tm_NN -RSB-_-RRB- -RRB-_-RRB- that_IN they_PRP may_MD use_VB ._.
Let_VB be_VB the_DT set_NN of_IN resources_NNS to_TO be_VB allocated_VBN among_IN the_DT agents_NNS ._.
An_DT agent_NN will_MD get_VB at_IN most_JJS one_CD resource_NN bundle_NN for_IN one_CD of_IN the_DT time_NN horizons_NNS ._.
Let_VB the_DT variable_JJ m_NN enumerate_VB all_DT possible_JJ pairs_NNS of_IN resource_NN bundles_NNS and_CC time_NN horizons_NNS for_IN agent_NN m_NN ,_, so_RB there_EX are_VBP #_# |_CD |_CD Tm_NN values_NNS for_IN -LRB-_-LRB- the_DT space_NN of_IN bundles_NNS is_VBZ exponential_JJ in_IN the_DT number_NN of_IN resource_NN types_NNS |_VBP |_NN -RRB-_-RRB- ._.
The_DT agent_NN m_NN must_MD provide_VB a_DT value_NN v_LS m_NN for_IN each_DT ,_, and_CC the_DT coordinator_NN will_MD allocate_VB at_IN most_JJS one_CD -LRB-_-LRB- resource_NN ,_, time_NN horizon_NN -RRB-_-RRB- pair_NN to_TO each_DT agent_NN ._.
This_DT allocation_NN is_VBZ expressed_VBN as_IN an_DT indicator_NN variable_JJ z_SYM m_NN -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- that_WDT shows_VBZ whether_IN is_VBZ assigned_VBN to_TO agent_NN m_NN ._.
For_IN time_NN and_CC resource_NN ,_, the_DT function_NN nm_NN -LRB-_-LRB- ,_, ,_, -RRB-_-RRB- -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- indicates_VBZ whether_IN the_DT bundle_NN in_IN uses_NNS resource_NN at_IN time_NN -LRB-_-LRB- we_PRP make_VBP the_DT assumption_NN that_IN agents_NNS have_VBP binary_JJ resource_NN requirements_NNS -RRB-_-RRB- ._.
This_DT allocation_NN problem_NN is_VBZ NP-complete_JJ ,_, even_RB when_WRB considering_VBG only_RB a_DT single_JJ time_NN step_NN ,_, and_CC its_PRP$ difficulty_NN increases_VBZ significantly_RB with_IN multiple_JJ time_NN steps_NNS because_IN of_IN the_DT increasing_VBG number_NN of_IN values_NNS of_IN ._.
The_DT problem_NN of_IN finding_VBG an_DT optimal_JJ allocation_NN that_IN satisfies_VBZ the_DT global_JJ constraint_NN that_IN the_DT amount_NN of_IN each_DT resource_NN allocated_VBN to_TO all_DT agents_NNS does_VBZ not_RB exceed_VB the_DT available_JJ amount_NN b_NN -LRB-_-LRB- -RRB-_-RRB- can_MD be_VB expressed_VBN as_IN the_DT following_VBG integer_NN program_NN :_: max_NN X_NN mM_NN X_NN m_NN z_SYM mv_FW m_NN subject_NN to_TO :_: X_NN m_NN z_SYM m_NN #_# ,_, m_NN M_NN ;_: X_NN mM_NN X_NN m_NN z_SYM mnm_FW -LRB-_-LRB- ,_, ,_, -RRB-_-RRB- b_NN -LRB-_-LRB- -RRB-_-RRB- ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- ,_, ;_: -LRB-_-LRB- #_# -RRB-_-RRB- The_DT first_JJ constraint_NN in_IN equation_NN #_# says_VBZ that_IN no_DT agent_NN can_MD receive_VB more_JJR than_IN one_CD bundle_NN ,_, and_CC the_DT second_JJ constraint_NN ensures_VBZ that_IN the_DT total_JJ assignment_NN of_IN resource_NN does_VBZ not_RB ,_, at_IN any_DT time_NN ,_, exceed_VBP the_DT resource_NN bound_VBD ._.
For_IN the_DT scheduling_NN problem_NN where_WRB the_DT agents_NNS are_VBP able_JJ to_TO dynamically_RB reallocate_VB resources_NNS ,_, each_DT agent_NN must_MD specify_VB a_DT value_NN for_IN every_DT combination_NN of_IN bundles_NNS and_CC time_NN steps_NNS within_IN its_PRP$ time_NN horizon_NN ._.
Let_VB the_DT variable_JJ m_NN in_IN this_DT case_NN enumerate_VB all_DT possible_JJ resource_NN bundles_NNS for_IN which_WDT at_IN most_JJS one_CD bundle_NN may_MD be_VB assigned_VBN to_TO agent_NN m_NN at_IN each_DT time_NN step_NN ._.
Therefore_RB ,_, in_IN this_DT case_NN there_EX are_VBP P_NN t_NN -LSB-_-LRB- #_# ,_, Tm_NN -RSB-_-RRB- -LRB-_-LRB- #_# |_CD |_NN -RRB-_-RRB- t_NN #_# |_CD |_CD Tm_NN possibilities_NNS of_IN resource_NN bundles_NNS assigned_VBN to_TO different_JJ time_NN slots_NNS ,_, for_IN the_DT Tm_NN different_JJ time_NN horizons_NNS ._.
The_DT same_JJ set_NN of_IN equations_NNS -LRB-_-LRB- #_# -RRB-_-RRB- can_MD be_VB used_VBN to_TO solve_VB this_DT dynamic_JJ scheduling_NN problem_NN ,_, but_CC the_DT integer_NN program_NN is_VBZ different_JJ because_IN of_IN the_DT difference_NN in_IN how_WRB is_VBZ defined_VBN ._.
In_IN this_DT case_NN ,_, the_DT number_NN of_IN values_NNS is_VBZ exponential_JJ in_IN each_DT agent_NN ''_'' s_NNS planning_VBG horizon_NN Tm_NN ,_, resulting_VBG in_IN a_DT much_JJ larger_JJR program_NN ._.
This_DT straightforward_JJ approach_NN to_TO solving_VBG both_DT of_IN these_DT scheduling_NN problems_NNS requires_VBZ an_DT enumeration_NN and_CC solution_NN of_IN either_CC #_# |_CD |_CD Tm_NN -LRB-_-LRB- static_JJ allocation_NN -RRB-_-RRB- or_CC P_NN t_NN -LSB-_-LRB- #_# ,_, Tm_NN -RSB-_-RRB- #_# |_CD |_CD t_NN -LRB-_-LRB- dynamic_JJ reallocation_NN -RRB-_-RRB- MDPs_NNS for_IN each_DT agent_NN ,_, which_WDT very_RB quickly_RB becomes_VBZ intractable_JJ with_IN the_DT growth_NN of_IN the_DT number_NN of_IN resources_NNS |_CD |_CD or_CC the_DT time_NN horizon_NN Tm_NN ._.
3_LS ._.
MODEL_NN AND_CC PROBLEM_NN STATEMENT_JJ We_PRP now_RB formally_RB introduce_VB our_PRP$ model_NN of_IN the_DT resourcescheduling_NN problem_NN ._.
The_DT problem_NN input_NN consists_VBZ of_IN the_DT following_VBG components_NNS :_: M_NN ,_, ,_, b_NN ,_, a_DT m_NN ,_, d_NN m_NN ,_, b_NN are_VBP as_IN defined_VBN above_IN in_IN Section_NN #_# ._.
#_# ._.
-LCB-_-LRB- m_NN -RCB-_-RRB- =_JJ -LCB-_-LRB- S_NN ,_, A_NN ,_, pm_NN ,_, rm_NN ,_, m_NN -RCB-_-RRB- are_VBP the_DT MDPs_NNS of_IN all_DT agents_NNS m_NN M_NN ._.
Without_IN loss_NN of_IN generality_NN ,_, we_PRP assume_VBP that_IN state_NN and_CC action_NN spaces_NNS of_IN all_DT agents_NNS are_VBP the_DT same_JJ ,_, but_CC each_DT has_VBZ its_PRP$ own_JJ transition_NN function_NN pm_NN ,_, reward_NN function_NN rm_NN ,_, and_CC initial_JJ conditions_NNS m_NN ._.
m_NN :_: A_DT -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- is_VBZ the_DT mapping_NN of_IN actions_NNS to_TO resources_NNS for_IN agent_NN m_NN ._.
m_NN -LRB-_-LRB- a_DT ,_, -RRB-_-RRB- indicates_VBZ whether_IN action_NN a_DT of_IN agent_NN m_NN needs_VBZ resource_NN ._.
An_DT agent_NN m_NN that_WDT receives_VBZ a_DT set_NN of_IN resources_NNS that_WDT does_VBZ not_RB include_VB resource_NN can_MD not_RB execute_VB in_IN its_PRP$ MDP_NN policy_NN any_DT action_NN a_DT for_IN which_WDT m_NN -LRB-_-LRB- a_DT ,_, -RRB-_-RRB- =_JJ #_# ._.
We_PRP assume_VBP all_DT resource_NN requirements_NNS are_VBP binary_JJ ;_: as_IN discussed_VBN below_IN in_IN Section_NN #_# ,_, this_DT assumption_NN is_VBZ not_RB limiting_VBG ._.
Given_VBN the_DT above_JJ input_NN ,_, the_DT optimization_NN problem_NN we_PRP consider_VBP is_VBZ to_TO find_VB the_DT globally_RB optimal-maximizing_VBG the_DT sum_NN of_IN expected_VBN rewards-mapping_NN of_IN resources_NNS to_TO agents_NNS for_IN all_DT time_NN steps_NNS :_: :_: M_NN -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ._.
A_DT solution_NN is_VBZ feasible_JJ if_IN the_DT corresponding_JJ assignment_NN of_IN resources_NNS to_TO the_DT agents_NNS does_VBZ not_RB violate_VB the_DT global_JJ resource_NN constraint_NN :_: X_NN m_NN m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- b_NN -LRB-_-LRB- -RRB-_-RRB- ,_, ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- ._.
-LRB-_-LRB- #_# -RRB-_-RRB- We_PRP consider_VBP two_CD flavors_NNS of_IN the_DT resource-scheduling_JJ problem_NN ._.
The_DT first_JJ formulation_NN restricts_VBZ resource_NN assignments_NNS to_TO the_DT space_NN where_WRB the_DT allocation_NN of_IN resources_NNS to_TO each_DT agent_NN is_VBZ static_JJ during_IN the_DT agent_NN ''_'' s_NNS lifetime_NN ._.
The_DT second_JJ formulation_NN allows_VBZ reassignment_NN of_IN resources_NNS between_IN agents_NNS at_IN every_DT time_NN step_NN within_IN their_PRP$ lifetimes_NNS ._.
Figure_NNP #_# depicts_VBZ a_DT resource-scheduling_JJ problem_NN with_IN three_CD agents_NNS M_NN =_JJ -LCB-_-LRB- m1_NN ,_, m2_NN ,_, m3_NN -RCB-_-RRB- ,_, three_CD resources_NNS =_JJ -LCB-_-LRB- #_# ,_, #_# ,_, #_# -RCB-_-RRB- ,_, and_CC a_DT global_JJ problem_NN horizon_NN of_IN b_NN =_JJ ##_CD ._.
The_DT agents_NNS ''_'' arrival_NN and_CC departure_NN times_NNS are_VBP shown_VBN as_IN gray_JJ boxes_NNS and_CC are_VBP -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ,_, -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ,_, and_CC -LCB-_-LRB- #_# ,_, ##_CD -RCB-_-RRB- ,_, respectively_RB ._.
A_DT solution_NN to_TO this_DT problem_NN is_VBZ shown_VBN via_IN horizontal_JJ bars_NNS within_IN each_DT agents_NNS ''_'' box_NN ,_, where_WRB the_DT bars_NNS correspond_VBP to_TO the_DT allocation_NN of_IN the_DT three_CD resource_NN types_NNS ._.
Figure_NNP 1a_NN shows_VBZ a_DT solution_NN to_TO a_DT static_JJ scheduling_NN problem_NN ._.
According_VBG to_TO the_DT shown_VBN solution_NN ,_, agent_NN m1_NN begins_VBZ the_DT execution_NN of_IN its_PRP$ MDP_NN at_IN time_NN =_JJ #_# and_CC has_VBZ a_DT lock_NN on_IN all_DT three_CD resources_NNS until_IN it_PRP finishes_VBZ execution_NN at_IN time_NN =_JJ #_# ._.
Note_VB that_DT agent_NN m1_NN relinquishes_VBZ its_PRP$ hold_NN on_IN the_DT resources_NNS before_IN its_PRP$ announced_VBN departure_NN time_NN of_IN d_NN m1_NN =_JJ #_# ,_, ostensibly_RB because_IN other_JJ agents_NNS can_MD utilize_VB the_DT resources_NNS more_RBR effectively_RB ._.
Thus_RB ,_, at_IN time_NN =_JJ #_# ,_, resources_NNS #_# and_CC #_# are_VBP allocated_VBN to_TO agent_NN m2_NN ,_, who_WP then_RB uses_VBZ them_PRP to_TO execute_VB its_PRP$ MDP_NN -LRB-_-LRB- using_VBG only_RB actions_NNS supported_VBN by_IN resources_NNS #_# and_CC #_# -RRB-_-RRB- until_IN time_NN =_JJ #_# ._.
Agent_NNP m3_NN holds_VBZ resource_NN #_# during_IN the_DT interval_NN -LSB-_-LRB- #_# ,_, ##_NN -RSB-_-RRB- ._.
Figure_NNP 1b_NN shows_VBZ a_DT possible_JJ solution_NN to_TO the_DT dynamic_JJ version_NN of_IN the_DT same_JJ problem_NN ._.
There_RB ,_, resources_NNS can_MD be_VB reallocated_VBN between_IN agents_NNS at_IN every_DT time_NN step_NN ._.
For_IN example_NN ,_, agent_NN m1_NN gives_VBZ up_RP its_PRP$ use_NN of_IN resource_NN #_# at_IN time_NN =_JJ #_# ,_, although_IN it_PRP continues_VBZ the_DT execution_NN of_IN its_PRP$ MDP_NN until_IN time_NN =_JJ #_# ._.
Notice_NNP that_IN an_DT agent_NN is_VBZ not_RB allowed_VBN to_TO stop_VB and_CC restart_VB its_PRP$ MDP_NN ,_, so_RB agent_NN m1_NN is_VBZ only_RB able_JJ to_TO continue_VB executing_VBG in_IN the_DT interval_NN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- if_IN it_PRP has_VBZ actions_NNS that_WDT do_VBP not_RB require_VB any_DT resources_NNS -LRB-_-LRB- m_NN -LRB-_-LRB- a_DT ,_, -RRB-_-RRB- =_JJ #_# -RRB-_-RRB- ._.
Clearly_RB ,_, the_DT model_NN and_CC problem_NN statement_NN described_VBN above_IN make_VB a_DT number_NN of_IN assumptions_NNS about_IN the_DT problem_NN and_CC the_DT desired_VBN solution_NN properties_NNS ._.
We_PRP discuss_VBP some_DT of_IN those_DT assumptions_NNS and_CC their_PRP$ implications_NNS in_IN Section_NN #_# ._.
1222_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- -LRB-_-LRB- a_DT -RRB-_-RRB- -LRB-_-LRB- b_NN -RRB-_-RRB- Figure_NN #_# :_: Illustration_NN of_IN a_DT solution_NN to_TO a_DT resource-scheduling_JJ problem_NN with_IN three_CD agents_NNS and_CC three_CD resources_NNS :_: a_DT -RRB-_-RRB- static_JJ resource_NN assignments_NNS -LRB-_-LRB- resource_NN assignments_NNS are_VBP constant_JJ within_IN agents_NNS ''_'' lifetimes_NNS ;_: b_LS -RRB-_-RRB- dynamic_JJ assignment_NN -LRB-_-LRB- resource_NN assignments_NNS are_VBP allowed_VBN to_TO change_VB at_IN every_DT time_NN step_NN -RRB-_-RRB- ._.
4_LS ._.
RESOURCE_NN SCHEDULING_NN Our_PRP$ resource-scheduling_JJ algorithm_NN proceeds_NNS in_IN two_CD stages_NNS ._.
First_RB ,_, we_PRP perform_VBP a_DT preprocessing_JJ step_NN that_WDT augments_VBZ the_DT agent_NN MDPs_NNS ;_: this_DT process_NN is_VBZ described_VBN in_IN Section_NN #_# ._.
#_# ._.
Second_RB ,_, using_VBG these_DT augmented_JJ MDPs_NN we_PRP construct_VBP a_DT global_JJ optimization_NN problem_NN ,_, which_WDT is_VBZ described_VBN in_IN Section_NN #_# ._.
#_# ._.
4_LS ._.
#_# Augmenting_NNP Agents_NNPS ''_'' MDPs_NNS In_IN the_DT model_NN described_VBN in_IN the_DT previous_JJ section_NN ,_, we_PRP assume_VBP that_IN if_IN an_DT agent_NN does_VBZ not_RB possess_VB the_DT necessary_JJ resources_NNS to_TO perform_VB actions_NNS in_IN its_PRP$ MDP_NN ,_, its_PRP$ execution_NN is_VBZ halted_VBN and_CC the_DT agent_NN leaves_VBZ the_DT system_NN ._.
In_IN other_JJ words_NNS ,_, the_DT MDPs_NNS can_MD not_RB be_VB paused_VBN and_CC resumed_VBN ._.
For_IN example_NN ,_, in_IN the_DT problem_NN shown_VBN in_IN Figure_NNP 1a_NN ,_, agent_NN m1_NN releases_VBZ all_DT resources_NNS after_IN time_NN =_JJ #_# ,_, at_IN which_WDT point_VBP the_DT execution_NN of_IN its_PRP$ MDP_NN is_VBZ halted_VBN ._.
Similarly_RB ,_, agents_NNS m2_NN and_CC m3_NN only_RB execute_VB their_PRP$ MDPs_NNS in_IN the_DT intervals_NNS -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- and_CC -LSB-_-LRB- #_# ,_, ##_NN -RSB-_-RRB- ,_, respectively_RB ._.
Therefore_RB ,_, an_DT important_JJ part_NN of_IN the_DT global_JJ decision-making_JJ problem_NN is_VBZ to_TO decide_VB the_DT window_NN of_IN time_NN during_IN which_WDT each_DT of_IN the_DT agents_NNS is_VBZ active_JJ -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, executing_VBG its_PRP$ MDP_NN -RRB-_-RRB- ._.
To_TO accomplish_VB this_DT ,_, we_PRP augment_VBP each_DT agent_NN ''_'' s_NNS MDP_NN with_IN two_CD new_JJ states_NNS -LRB-_-LRB- start_NN and_CC finish_NN states_NNS sb_VBP ,_, sf_VBP ,_, respectively_RB -RRB-_-RRB- and_CC a_DT new_JJ start_NN /_: stop_VB action_NN a_DT ,_, as_IN illustrated_VBN in_IN Figure_NNP #_# ._.
The_DT idea_NN is_VBZ that_IN an_DT agent_NN stays_VBZ in_IN the_DT start_NN state_NN sb_NN until_IN it_PRP is_VBZ ready_JJ to_TO execute_VB its_PRP$ MDP_NN ,_, at_IN which_WDT point_NN it_PRP performs_VBZ the_DT start_NN /_: stop_VB action_NN a_DT and_CC transitions_NNS into_IN the_DT state_NN space_NN of_IN the_DT original_JJ MDP_NN with_IN the_DT transition_NN probability_NN that_WDT corresponds_VBZ to_TO the_DT original_JJ initial_JJ distribution_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
For_IN example_NN ,_, in_IN Figure_NNP 1a_NN ,_, for_IN agent_NN m2_NN this_DT would_MD happen_VB at_IN time_NN =_JJ #_# ._.
Once_RB the_DT agent_NN gets_VBZ to_TO the_DT end_NN of_IN its_PRP$ activity_NN window_NN -LRB-_-LRB- time_NN =_JJ #_# for_IN agent_NN m2_NN in_IN Figure_NNP 1a_NN -RRB-_-RRB- ,_, it_PRP performs_VBZ the_DT start_NN /_: stop_VB action_NN ,_, which_WDT takes_VBZ it_PRP into_IN the_DT sink_NN finish_NN state_NN sf_NN at_IN time_NN =_JJ #_# ._.
More_RBR precisely_RB ,_, given_VBN an_DT MDP_NN S_NN ,_, A_NN ,_, pm_NN ,_, rm_NN ,_, m_NN ,_, we_PRP define_VBP an_DT augmented_JJ MDP_NN S_NN ,_, A_NN ,_, pm_NN ,_, rm_NN ,_, m_NN as_IN follows_VBZ :_: S_NN =_JJ S_NN sb_NN sf_NN ;_: A_NN =_JJ A_NN a_DT ;_: p_NN -LRB-_-LRB- s_NNS |_VBP sb_NN ,_, a_DT -RRB-_-RRB- =_JJ -LRB-_-LRB- s_NNS -RRB-_-RRB- ,_, s_VBZ S_NN ;_: p_NN -LRB-_-LRB- sb_NN |_CD sb_NN ,_, a_DT -RRB-_-RRB- =_JJ #_# ._.
#_# ,_, a_DT A_NN ;_: p_NN -LRB-_-LRB- sf_NN |_CD s_NNS ,_, a_DT -RRB-_-RRB- =_JJ #_# ._.
#_# ,_, s_VBZ S_NN ;_: p_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- =_JJ p_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- ,_, s_NNS ,_, S_NN ,_, a_DT A_NN ;_: r_NN -LRB-_-LRB- sb_NN ,_, a_DT -RRB-_-RRB- =_JJ r_NN -LRB-_-LRB- sf_NN ,_, a_DT -RRB-_-RRB- =_JJ #_# ,_, a_DT A_NN ;_: r_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- =_JJ r_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- ,_, s_VBZ S_NN ,_, a_DT A_NN ;_: -LRB-_-LRB- sb_NN -RRB-_-RRB- =_JJ #_# ;_: -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ #_# ,_, s_VBZ S_NN ;_: where_WRB all_DT non-specified_JJ transition_NN probabilities_NNS are_VBP assumed_VBN to_TO be_VB zero_CD ._.
Further_RB ,_, in_IN order_NN to_TO account_VB for_IN the_DT new_JJ starting_VBG state_NN ,_, we_PRP begin_VBP the_DT MDP_NN one_CD time-step_NN earlier_RBR ,_, setting_VBG a_DT m_NN a_DT m_NN #_# ._.
This_DT will_MD not_RB affect_VB the_DT resource_NN allocation_NN due_JJ to_TO the_DT resource_NN constraints_NNS only_RB being_VBG enforced_VBN for_IN the_DT original_JJ MDP_NN states_NNS ,_, as_RB will_MD be_VB discussed_VBN in_IN the_DT next_JJ section_NN ._.
For_IN example_NN ,_, the_DT augmented_JJ MDPs_NNS shown_VBN in_IN Figure_NNP 2b_NN -LRB-_-LRB- which_WDT starts_VBZ in_IN state_NN sb_NN at_IN time_NN =_JJ #_# -RRB-_-RRB- would_MD be_VB constructed_VBN from_IN an_DT MDP_NN with_IN original_JJ arrival_NN time_NN =_JJ #_# ._.
Figure_NNP 2b_NN also_RB shows_VBZ a_DT sample_NN trajectory_NN through_IN the_DT state_NN space_NN :_: the_DT agent_NN starts_VBZ in_IN state_NN sb_NN ,_, transitions_NNS into_IN the_DT state_NN space_NN S_NN of_IN the_DT original_JJ MDP_NN ,_, and_CC finally_RB exists_VBZ into_IN the_DT sink_NN state_NN sf_NN ._.
Note_VB that_IN if_IN we_PRP wanted_VBD to_TO model_VB a_DT problem_NN where_WRB agents_NNS could_MD pause_VB their_PRP$ MDPs_NNS at_IN arbitrary_JJ time_NN steps_NNS -LRB-_-LRB- which_WDT might_MD be_VB useful_JJ for_IN domains_NNS where_WRB dynamic_JJ reallocation_NN is_VBZ possible_JJ -RRB-_-RRB- ,_, we_PRP could_MD easily_RB accomplish_VB this_DT by_IN including_VBG an_DT extra_JJ action_NN that_WDT transitions_VBZ from_IN each_DT state_NN to_TO itself_PRP with_IN zero_CD reward_NN ._.
4_LS ._.
#_# MILP_NN for_IN Resource_NNP Scheduling_NNP Given_VBN a_DT set_NN of_IN augmented_JJ MDPs_NNS ,_, as_IN defined_VBN above_IN ,_, the_DT goal_NN of_IN this_DT section_NN is_VBZ to_TO formulate_VB a_DT global_JJ optimization_NN program_NN that_WDT solves_VBZ the_DT resource-scheduling_JJ problem_NN ._.
In_IN this_DT section_NN and_CC below_IN ,_, all_DT MDPs_NNS are_VBP assumed_VBN to_TO be_VB the_DT augmented_JJ MDPs_NNS as_IN defined_VBN in_IN Section_NN #_# ._.
#_# ._.
Our_PRP$ approach_NN is_VBZ similar_JJ to_TO the_DT idea_NN used_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- :_: we_PRP begin_VBP with_IN the_DT linear-program_JJ formulation_NN of_IN agents_NNS ''_'' MDPs_NNS -LRB-_-LRB- #_# -RRB-_-RRB- and_CC augment_VBP it_PRP with_IN constraints_NNS that_WDT ensure_VBP that_IN the_DT corresponding_JJ resource_NN allocation_NN across_IN agents_NNS and_CC time_NN is_VBZ valid_JJ ._.
The_DT resulting_VBG optimization_NN problem_NN then_RB simultaneously_RB solves_VBZ the_DT agents_NNS ''_'' MDPs_NNS and_CC resource-scheduling_JJ problems_NNS ._.
In_IN the_DT rest_NN of_IN this_DT section_NN ,_, we_PRP incrementally_RB develop_VBP a_DT mixed_JJ integer_NN program_NN -LRB-_-LRB- MILP_NN -RRB-_-RRB- that_WDT achieves_VBZ this_DT ._.
In_IN the_DT absence_NN of_IN resource_NN constraints_NNS ,_, the_DT agents_NNS ''_'' finitehorizon_NN MDPs_NNS are_VBP completely_RB independent_JJ ,_, and_CC the_DT globally_RB optimal_JJ solution_NN can_MD be_VB trivially_RB obtained_VBN via_IN the_DT following_VBG LP_NN ,_, which_WDT is_VBZ simply_RB an_DT aggregation_NN of_IN single-agent_JJ finitehorizon_NN LPs_NNS :_: max_NN X_NN m_NN X_NN s_VBZ X_NN a_DT rm_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- X_NN t_NN xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- subject_NN to_TO :_: X_NN a_DT xm_NN -LRB-_-LRB- ,_, a_DT ,_, t_NN +_CC #_# -RRB-_-RRB- =_JJ X_NN s_NNS ,_, a_DT pm_NN -LRB-_-LRB- |_CD s_NNS ,_, a_DT -RRB-_-RRB- xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- ,_, m_NN M_NN ,_, S_NN ,_, t_NN -LSB-_-LRB- #_# ,_, Tm_NN #_# -RSB-_-RRB- ;_: X_NN a_DT xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, #_# -RRB-_-RRB- =_JJ m_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ,_, m_NN M_NN ,_, s_VBZ S_NN ;_: -LRB-_-LRB- ##_CD -RRB-_-RRB- where_WRB xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- is_VBZ the_DT occupation_NN measure_NN of_IN agent_NN m_NN ,_, and_CC The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_CD -LRB-_-LRB- a_DT -RRB-_-RRB- -LRB-_-LRB- b_NN -RRB-_-RRB- Figure_NN #_# :_: Illustration_NN of_IN augmenting_VBG an_DT MDP_NN to_TO allow_VB for_IN variable_JJ starting_VBG and_CC stopping_VBG times_NNS :_: a_DT -RRB-_-RRB- -LRB-_-LRB- left_VBN -RRB-_-RRB- the_DT original_JJ two-state_JJ MDP_NN with_IN a_DT single_JJ action_NN ;_: -LRB-_-LRB- right_NN -RRB-_-RRB- the_DT augmented_JJ MDP_NN with_IN new_JJ states_NNS sb_NN and_CC sf_NN and_CC the_DT new_JJ action_NN a_DT -LRB-_-LRB- note_NN that_IN the_DT origianl_NN transitions_NNS are_VBP not_RB changed_VBN in_IN the_DT augmentation_NN process_NN -RRB-_-RRB- ;_: b_LS -RRB-_-RRB- the_DT augmented_JJ MDP_NN displayed_VBD as_IN a_DT trajectory_NN through_IN time_NN -LRB-_-LRB- grey_JJ lines_NNS indicate_VBP all_DT transitions_NNS ,_, while_IN black_JJ lines_NNS indicate_VBP a_DT given_VBN trajectory_NN ._.
Objective_NN Function_NN -LRB-_-LRB- sum_NN of_IN expected_VBN rewards_NNS over_IN all_DT agents_NNS -RRB-_-RRB- max_NN X_NN m_NN X_NN s_VBZ X_NN a_DT rm_NN -LRB-_-LRB- s_NNS ,_, a_DT -RRB-_-RRB- X_NN t_NN xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- Meaning_VBG Implication_NN Linear_NNP Constraints_NNP Tie_NNP x_NN to_TO ._.
Agent_NNP is_VBZ only_RB active_JJ when_WRB occupation_NN measure_NN is_VBZ nonzero_NN in_IN original_JJ MDP_NN states_NNS ._.
m_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ #_# =_JJ xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, a_DT m_NN +_CC #_# -RRB-_-RRB- =_JJ #_# s_NNS /_: -LCB-_-LRB- sb_NN ,_, sf_NN -RCB-_-RRB- ,_, a_DT A_NN X_NN s_VBZ /_: -LCB-_-LRB- sb_NN ,_, sf_NN -RCB-_-RRB- X_NN a_DT xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- m_NN -LRB-_-LRB- a_DT m_NN +_CC t_NN #_# -RRB-_-RRB- m_NN M_NN ,_, t_NN -LSB-_-LRB- #_# ,_, Tm_NN -RSB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- Agent_NNP can_MD only_RB be_VB active_JJ in_IN -LRB-_-LRB- a_DT m_NN ,_, d_NN m_NN -RRB-_-RRB- m_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ #_# m_NN M_NN ,_, /_: -LRB-_-LRB- a_DT m_NN ,_, d_NN m_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- Can_MD not_RB use_VB resources_NNS when_WRB not_RB active_JJ m_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ #_# =_JJ m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- =_JJ #_# -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- ,_, m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- m_NN -LRB-_-LRB- -RRB-_-RRB- m_NN M_NN ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- ,_, -LRB-_-LRB- #_# -RRB-_-RRB- Tie_NN x_NN to_TO -LRB-_-LRB- nonzero_NN x_NN forces_NNS corresponding_VBG to_TO be_VB nonzero_NN ._. -RRB-_-RRB-
m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- =_JJ #_# ,_, m_NN -LRB-_-LRB- a_DT ,_, -RRB-_-RRB- =_JJ #_# =_JJ xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, a_DT m_NN +_CC #_# -RRB-_-RRB- =_JJ #_# s_NNS /_: -LCB-_-LRB- sb_NN ,_, sf_NN -RCB-_-RRB- 1_CD /_: |_CD A_DT |_NN X_NN a_DT m_NN -LRB-_-LRB- a_DT ,_, -RRB-_-RRB- X_NN s_VBZ /_: -LCB-_-LRB- sb_NN ,_, sf_NN -RCB-_-RRB- xm_NN -LRB-_-LRB- s_NNS ,_, a_DT ,_, t_NN -RRB-_-RRB- m_NN -LRB-_-LRB- t_NN +_CC a_DT m_NN #_# ,_, -RRB-_-RRB- m_NN M_NN ,_, ,_, t_NN -LSB-_-LRB- #_# ,_, Tm_NN -RSB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- Resource_NNP bounds_NNS X_NN m_NN m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- b_NN -LRB-_-LRB- -RRB-_-RRB- ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- -LRB-_-LRB- ##_CD -RRB-_-RRB- Agent_NNP can_MD not_RB change_VB resources_NNS while_IN active_JJ ._.
Only_RB enabled_VBN for_IN scheduling_NN with_IN static_JJ assignments_NNS ._.
m_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ #_# and_CC m_NN -LRB-_-LRB- +_CC #_# -RRB-_-RRB- =_JJ #_# =_JJ m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- =_JJ m_NN -LRB-_-LRB- +_CC #_# ,_, -RRB-_-RRB- m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- Z_NN -LRB-_-LRB- #_# m_NN -LRB-_-LRB- +_CC #_# -RRB-_-RRB- -RRB-_-RRB- m_NN -LRB-_-LRB- +_CC #_# ,_, -RRB-_-RRB- +_CC Z_NN -LRB-_-LRB- #_# m_NN -LRB-_-LRB- -RRB-_-RRB- -RRB-_-RRB- m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- +_CC Z_NN -LRB-_-LRB- #_# m_NN -LRB-_-LRB- +_CC #_# -RRB-_-RRB- -RRB-_-RRB- m_NN -LRB-_-LRB- +_CC #_# ,_, -RRB-_-RRB- Z_NN -LRB-_-LRB- #_# m_NN -LRB-_-LRB- -RRB-_-RRB- -RRB-_-RRB- m_NN M_NN ,_, ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- -LRB-_-LRB- ##_CD -RRB-_-RRB- Table_NNP #_# :_: MILP_NN for_IN globally_RB optimal_JJ resource_NN scheduling_NN ._.
Tm_NN =_JJ d_NN m_NN a_DT m_NN +_CC #_# is_VBZ the_DT time_NN horizon_NN for_IN the_DT agent_NN ''_'' s_NNS MDP_NN ._.
Using_VBG this_DT LP_NN as_IN a_DT basis_NN ,_, we_PRP augment_VBP it_PRP with_IN constraints_NNS that_WDT ensure_VBP that_IN the_DT resource_NN usage_NN implied_VBN by_IN the_DT agents_NNS ''_'' occupation_NN measures_NNS -LCB-_-LRB- xm_NN -RCB-_-RRB- does_VBZ not_RB violate_VB the_DT global_JJ resource_NN requirements_NNS b_NN at_IN any_DT time_NN step_NN -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- ._.
To_TO formulate_VB these_DT resource_NN constraints_NNS ,_, we_PRP use_VBP the_DT following_VBG binary_JJ variables_NNS :_: m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- =_JJ -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ,_, m_NN M_NN ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- ,_, ,_, which_WDT serve_VBP as_IN indicator_NN variables_NNS that_WDT define_VBP whether_IN agent_NN m_NN possesses_VBZ resource_NN at_IN time_NN ._.
These_DT are_VBP analogous_JJ to_TO the_DT static_JJ indicator_NN variables_NNS used_VBN in_IN the_DT one-shot_JJ static_JJ resource-allocation_NN problem_NN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
m_NN =_SYM -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ,_, m_NN M_NN ,_, -LSB-_-LRB- #_# ,_, b_NN -RSB-_-RRB- are_VBP indicator_NN variables_NNS that_WDT specify_VBP whether_IN agent_NN m_NN is_VBZ active_JJ -LRB-_-LRB- i_FW ._.
e_LS ._.
,_, executing_VBG its_PRP$ MDP_NN -RRB-_-RRB- at_IN time_NN ._.
The_DT meaning_NN of_IN resource-usage_JJ variables_NNS is_VBZ illustrated_VBN in_IN Figure_NNP #_# :_: m_NN -LRB-_-LRB- ,_, -RRB-_-RRB- =_JJ #_# only_RB if_IN resource_NN is_VBZ allocated_VBN to_TO agent_NN m_NN at_IN time_NN ._.
The_DT meaning_NN of_IN the_DT activity_NN indicators_NNS is_VBZ illustrated_VBN in_IN Figure_NNP 2b_NN :_: when_WRB agent_NN m_NN is_VBZ in_IN either_CC the_DT start_NN state_NN sb_NN or_CC the_DT finish_NN state_NN sf_NN ,_, the_DT corresponding_JJ m_NN =_JJ #_# ,_, but_CC once_RB the_DT agent_NN becomes_VBZ active_JJ and_CC enters_VBZ one_CD of_IN the_DT other_JJ states_NNS ,_, we_PRP set_VBD m_NN =_JJ #_# ._.
This_DT meaning_NN of_IN can_MD be_VB enforced_VBN with_IN a_DT linear_JJ constraint_NN that_WDT synchronizes_VBZ the_DT values_NNS of_IN the_DT agents_NNS ''_'' occupation_NN measures_NNS xm_NN and_CC the_DT activity_NN 1224_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- indicators_NNS ,_, as_IN shown_VBN in_IN -LRB-_-LRB- #_# -RRB-_-RRB- in_IN Table_NNP #_# ._.
Another_DT constraint_NN we_PRP have_VBP to_TO add-because_VB the_DT activity_NN indicators_NNS are_VBP defined_VBN on_IN the_DT global_JJ timeline_NN -_: is_VBZ to_TO enforce_VB the_DT fact_NN that_IN the_DT agent_NN is_VBZ inactive_JJ outside_IN of_IN its_PRP$ arrivaldeparture_NN window_NN ._.
This_DT is_VBZ accomplished_VBN by_IN constraint_NN -LRB-_-LRB- #_# -RRB-_-RRB- in_IN Table_NNP #_# ._.
Furthermore_RB ,_, agents_NNS should_MD not_RB be_VB using_VBG resources_NNS while_IN they_PRP are_VBP inactive_JJ ._.
This_DT constraint_NN can_MD also_RB be_VB enforced_VBN via_IN a_DT linear_JJ inequality_NN on_IN and_CC ,_, as_IN shown_VBN in_IN -LRB-_-LRB- #_# -RRB-_-RRB- ._.
Constraint_NN -LRB-_-LRB- #_# -RRB-_-RRB- sets_VBZ the_DT value_NN of_IN to_TO match_VB the_DT policy_NN defined_VBN by_IN the_DT occupation_NN measure_NN xm_NN ._.
In_IN a_DT similar_JJ fashion_NN ,_, we_PRP have_VBP to_TO make_VB sure_JJ that_IN the_DT resource-usage_JJ variables_NNS are_VBP also_RB synchronized_VBN with_IN the_DT occupation_NN measure_NN xm_NN ._.
This_DT is_VBZ done_VBN via_IN constraint_NN -LRB-_-LRB- #_# -RRB-_-RRB- in_IN Table_NNP #_# ,_, which_WDT is_VBZ nearly_RB identical_JJ to_TO the_DT analogous_JJ constraint_NN from_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
After_IN implementing_VBG the_DT above_JJ constraint_NN ,_, which_WDT enforces_VBZ the_DT meaning_NN of_IN ,_, we_PRP add_VBP a_DT constraint_NN that_WDT ensures_VBZ that_IN the_DT agents_NNS ''_'' resource_NN usage_NN never_RB exceeds_VBZ the_DT amounts_NNS of_IN available_JJ resources_NNS ._.
This_DT condition_NN is_VBZ also_RB trivially_RB expressed_VBN as_IN a_DT linear_JJ inequality_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- in_IN Table_NNP #_# ._.
Finally_RB ,_, for_IN the_DT problem_NN formulation_NN where_WRB resource_NN assignments_NNS are_VBP static_JJ during_IN a_DT lifetime_NN of_IN an_DT agent_NN ,_, we_PRP add_VBP a_DT constraint_NN that_WDT ensures_VBZ that_IN the_DT resource-usage_JJ variables_NNS do_VBP not_RB change_VB their_PRP$ value_NN while_IN the_DT agent_NN is_VBZ active_JJ -LRB-_-LRB- =_JJ #_# -RRB-_-RRB- ._.
This_DT is_VBZ accomplished_VBN via_IN the_DT linear_JJ constraint_NN -LRB-_-LRB- ##_NN -RRB-_-RRB- ,_, where_WRB Z_NN #_# is_VBZ a_DT constant_JJ that_DT is_VBZ used_VBN to_TO turn_VB off_RP the_DT constraints_NNS when_WRB m_NN -LRB-_-LRB- -RRB-_-RRB- =_JJ #_# or_CC m_NN -LRB-_-LRB- +_CC #_# -RRB-_-RRB- =_JJ #_# ._.
This_DT constraint_NN is_VBZ not_RB used_VBN for_IN the_DT dynamic_JJ problem_NN formulation_NN ,_, where_WRB resources_NNS can_MD be_VB reallocated_VBN between_IN agents_NNS at_IN every_DT time_NN step_NN ._.
To_TO summarize_VB ,_, Table_NNP #_# together_RB with_IN the_DT conservationof-flow_JJ constraints_NNS from_IN -LRB-_-LRB- ##_NN -RRB-_-RRB- defines_VBZ the_DT MILP_NN that_WDT simultaneously_RB computes_VBZ an_DT optimal_JJ resource_NN assignment_NN for_IN all_DT agents_NNS across_IN time_NN as_RB well_RB as_IN optimal_JJ finite-horizon_JJ MDP_NN policies_NNS that_WDT are_VBP valid_JJ under_IN that_DT resource_NN assignment_NN ._.
As_IN a_DT rough_JJ measure_NN of_IN the_DT complexity_NN of_IN this_DT MILP_NN ,_, let_VB us_PRP consider_VB the_DT number_NN of_IN optimization_NN variables_NNS and_CC constraints_NNS ._.
Let_VB TM_NN =_JJ P_NN Tm_NN =_JJ P_NN m_NN -LRB-_-LRB- a_DT m_NN d_NN m_NN +_CC #_# -RRB-_-RRB- be_VB the_DT sum_NN of_IN the_DT lengths_NNS of_IN the_DT arrival-departure_JJ windows_NNS across_IN all_DT agents_NNS ._.
Then_RB ,_, the_DT number_NN of_IN optimization_NN variables_NNS is_VBZ :_: TM_NN +_CC b_NN |_CD M_NN |_CD |_CD |_NN +_CC b_NN |_CD M_NN |_NN ,_, TM_NN of_IN which_WDT are_VBP continuous_JJ -LRB-_-LRB- xm_NN -RRB-_-RRB- ,_, and_CC b_NN |_CD M_NN |_CD |_CD |_NN +_CC b_NN |_CD M_NN |_NNS are_VBP binary_JJ -LRB-_-LRB- and_CC -RRB-_-RRB- ._.
However_RB ,_, notice_NN that_IN all_DT but_CC TM_NNP |_CD M_NN |_NN of_IN the_DT are_VBP set_VBN to_TO zero_CD by_IN constraint_NN -LRB-_-LRB- #_# -RRB-_-RRB- ,_, which_WDT also_RB immediately_RB forces_VBZ all_DT but_CC TM_NNP |_CD M_NN |_CD |_CD |_NN of_IN the_DT to_TO be_VB zero_CD via_IN the_DT constraints_NNS -LRB-_-LRB- #_# -RRB-_-RRB- ._.
The_DT number_NN of_IN constraints_NNS -LRB-_-LRB- not_RB including_VBG the_DT degenerate_JJ constraints_NNS in_IN -LRB-_-LRB- #_# -RRB-_-RRB- -RRB-_-RRB- in_IN the_DT MILP_NN is_VBZ :_: TM_NN +_CC TM_NN |_CD |_NN +_CC b_NN |_CD |_NN +_CC b_NN |_CD M_NN |_CD |_CD |_NN ._.
Despite_IN the_DT fact_NN that_IN the_DT complexity_NN of_IN the_DT MILP_NN is_VBZ ,_, in_IN the_DT worst_JJS case_NN ,_, exponential1_NN in_IN the_DT number_NN of_IN binary_JJ variables_NNS ,_, the_DT complexity_NN of_IN this_DT MILP_NN is_VBZ significantly_RB -LRB-_-LRB- exponentially_RB -RRB-_-RRB- lower_JJR than_IN that_DT of_IN the_DT MILP_NN with_IN flat_JJ utility_NN functions_NNS ,_, described_VBN in_IN Section_NN #_# ._.
#_# ._.
This_DT result_NN echos_VBZ the_DT efficiency_NN gains_NNS reported_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- for_IN single-shot_JJ resource-allocation_NN problems_NNS ,_, but_CC is_VBZ much_RB more_RBR pronounced_JJ ,_, because_IN of_IN the_DT explosion_NN of_IN the_DT flat_JJ utility_NN representation_NN due_JJ to_TO the_DT temporal_JJ aspect_NN of_IN the_DT problem_NN -LRB-_-LRB- recall_NN the_DT prohibitive_JJ complexity_NN of_IN the_DT combinatorial_JJ optimization_NN in_IN Section_NN #_# ._.
#_# -RRB-_-RRB- ._.
We_PRP empirically_RB analyze_VBP the_DT performance_NN of_IN this_DT method_NN in_IN Section_NN #_# ._.
1_LS Strictly_RB speaking_VBG ,_, solving_VBG MILPs_NNS to_TO optimality_NN is_VBZ NPcomplete_NN in_IN the_DT number_NN of_IN integer_NN variables_NNS ._.
5_CD ._.
EXPERIMENTAL_JJ RESULTS_NNS Although_IN the_DT complexity_NN of_IN solving_VBG MILPs_NNS is_VBZ in_IN the_DT worst_JJS case_NN exponential_NN in_IN the_DT number_NN of_IN integer_NN variables_NNS ,_, there_EX are_VBP many_JJ efficient_JJ methods_NNS for_IN solving_VBG MILPs_NNS that_WDT allow_VBP our_PRP$ algorithm_NN to_TO scale_VB well_RB for_IN parameters_NNS common_JJ to_TO resource_NN allocation_NN and_CC scheduling_NN problems_NNS ._.
In_IN particular_JJ ,_, this_DT section_NN introduces_VBZ a_DT problem_NN domain-the_JJ repairshop_NN problem-used_JJ to_TO empirically_RB evaluate_VB our_PRP$ algorithm_NN ''_'' s_NNS scalability_NN in_IN terms_NNS of_IN the_DT number_NN of_IN agents_NNS |_CD M_NN |_NN ,_, the_DT number_NN of_IN shared_JJ resources_NNS |_VBP |_RB ,_, and_CC the_DT varied_JJ lengths_NNS of_IN global_JJ time_NN b_NN during_IN which_WDT agents_NNS may_MD enter_VB and_CC exit_VB the_DT system_NN ._.
The_DT repairshop_NN problem_NN is_VBZ a_DT simple_JJ parameterized_JJ MDP_NN adopting_VBG the_DT metaphor_NN of_IN a_DT vehicular_JJ repair_NN shop_NN ._.
Agents_NNS in_IN the_DT repair_NN shop_NN are_VBP mechanics_NNS with_IN a_DT number_NN of_IN independent_JJ tasks_NNS that_WDT yield_VBP reward_NN only_RB when_WRB completed_VBN ._.
In_IN our_PRP$ MDP_NN model_NN of_IN this_DT system_NN ,_, actions_NNS taken_VBN to_TO advance_VB through_IN the_DT state_NN space_NN are_VBP only_RB allowed_VBN if_IN the_DT agent_NN holds_VBZ certain_JJ resources_NNS that_WDT are_VBP publicly_RB available_JJ to_TO the_DT shop_NN ._.
These_DT resources_NNS are_VBP in_IN finite_JJ supply_NN ,_, and_CC optimal_JJ policies_NNS for_IN the_DT shop_NN will_MD determine_VB when_WRB each_DT agent_NN may_MD hold_VB the_DT limited_JJ resources_NNS to_TO take_VB actions_NNS and_CC earn_VBP individual_JJ rewards_NNS ._.
Each_DT task_NN to_TO be_VB completed_VBN is_VBZ associated_VBN with_IN a_DT single_JJ action_NN ,_, although_IN the_DT agent_NN is_VBZ required_VBN to_TO repeat_VB the_DT action_NN numerous_JJ times_NNS before_IN completing_VBG the_DT task_NN and_CC earning_VBG a_DT reward_NN ._.
This_DT model_NN was_VBD parameterized_VBN in_IN terms_NNS of_IN the_DT number_NN of_IN agents_NNS in_IN the_DT system_NN ,_, the_DT number_NN of_IN different_JJ types_NNS of_IN resources_NNS that_WDT could_MD be_VB linked_VBN to_TO necessary_JJ actions_NNS ,_, a_DT global_JJ time_NN during_IN which_WDT agents_NNS are_VBP allowed_VBN to_TO arrive_VB and_CC depart_VB ,_, and_CC a_DT maximum_NN length_NN for_IN the_DT number_NN of_IN time_NN steps_NNS an_DT agent_NN may_MD remain_VB in_IN the_DT system_NN ._.
All_DT datapoints_NNS in_IN our_PRP$ experiments_NNS were_VBD obtained_VBN with_IN ##_NN evaluations_NNS using_VBG CPLEX_NNP to_TO solve_VB the_DT MILPs_NNS on_IN a_DT Pentium4_NN computer_NN with_IN 2Gb_NN of_IN RAM_NNP ._.
Trials_NNS were_VBD conducted_VBN on_IN both_CC the_DT static_JJ and_CC the_DT dynamic_JJ version_NN of_IN the_DT resourcescheduling_NN problem_NN ,_, as_IN defined_VBN earlier_RBR ._.
Figure_NNP #_# shows_VBZ the_DT runtime_NN and_CC policy_NN value_NN for_IN independent_JJ modifications_NNS to_TO the_DT parameter_NN set_NN ._.
The_DT top_JJ row_NN shows_VBZ how_WRB the_DT solution_NN time_NN for_IN the_DT MILP_NN scales_NNS as_IN we_PRP increase_VBP the_DT number_NN of_IN agents_NNS |_CD M_NN |_NN ,_, the_DT global_JJ time_NN horizon_NN b_NN ,_, and_CC the_DT number_NN of_IN resources_NNS |_CD |_CD ._.
Increasing_VBG the_DT number_NN of_IN agents_NNS leads_VBZ to_TO exponential_JJ complexity_NN scaling_NN ,_, which_WDT is_VBZ to_TO be_VB expected_VBN for_IN an_DT NP-complete_JJ problem_NN ._.
However_RB ,_, increasing_VBG the_DT global_JJ time_NN limit_NN b_NN or_CC the_DT total_JJ number_NN of_IN resource_NN types_NNS |_VBP |_SYM -_: while_IN holding_VBG the_DT number_NN of_IN agents_NNS constantdoes_VBZ not_RB lead_VB to_TO decreased_VBN performance_NN ._.
This_DT occurs_VBZ because_IN the_DT problems_NNS get_VBP easier_JJR as_IN they_PRP become_VBP under-constrained_JJ ,_, which_WDT is_VBZ also_RB a_DT common_JJ phenomenon_NN for_IN NP-complete_JJ problems_NNS ._.
We_PRP also_RB observe_VBP that_IN the_DT solution_NN to_TO the_DT dynamic_JJ version_NN of_IN the_DT problem_NN can_MD often_RB be_VB computed_VBN much_JJ faster_JJR than_IN the_DT static_JJ version_NN ._.
The_DT bottom_JJ row_NN of_IN Figure_NNP #_# shows_VBZ the_DT joint_JJ policy_NN value_NN of_IN the_DT policies_NNS that_WDT correspond_VBP to_TO the_DT computed_JJ optimal_JJ resource-allocation_NN schedules_NNS ._.
We_PRP can_MD observe_VB that_IN the_DT dynamic_JJ version_NN yields_NNS higher_JJR reward_NN -LRB-_-LRB- as_IN expected_VBN ,_, since_IN the_DT reward_NN for_IN the_DT dynamic_JJ version_NN is_VBZ always_RB no_RB less_JJR than_IN the_DT reward_NN of_IN the_DT static_JJ version_NN -RRB-_-RRB- ._.
We_PRP should_MD point_VB out_RP that_IN these_DT graphs_NNS should_MD not_RB be_VB viewed_VBN as_IN a_DT measure_NN of_IN performance_NN of_IN two_CD different_JJ algorithms_NNS -LRB-_-LRB- both_CC algorithms_NNS produce_VBP optimal_JJ solutions_NNS but_CC to_TO different_JJ problems_NNS -RRB-_-RRB- ,_, but_CC rather_RB as_IN observations_NNS about_IN how_WRB the_DT quality_NN of_IN optimal_JJ solutions_NNS change_VBP as_IN more_JJR flexibility_NN is_VBZ allowed_VBN in_IN the_DT reallocation_NN of_IN resources_NNS ._.
Figure_NNP #_# shows_VBZ runtime_NN and_CC policy_NN value_NN for_IN trials_NNS in_IN which_WDT common_JJ input_NN variables_NNS are_VBP scaled_VBN together_RB ._.
This_DT allows_VBZ The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_CD 2_CD #_# #_# #_# ##_CD 10_CD 3_CD 10_CD 2_CD 10_CD 1_CD 10_CD 0_CD 10_CD 1_CD 10_CD 2_CD 10_CD 3_CD 10_CD 4_CD Number_NN of_IN Agents_NNPS |_CD M_NN |_CD CPUTime_NN ,_, sec_NN |_CD |_NN =_JJ #_# ,_, =_JJ ##_CD static_JJ dynamic_JJ 50_CD ###_CD ###_CD ###_CD 10_CD 2_CD 10_CD 1_CD 10_CD 0_CD 10_CD 1_CD 10_CD 2_CD 10_CD 3_CD Global_JJ Time_NNP Boundary_NNP CPUTime_NNP ,_, sec_NN |_CD M_NN |_NN =_JJ #_# ,_, |_CD |_NN =_JJ #_# static_JJ dynamic_JJ 10_CD ##_CD ##_CD ##_CD ##_CD 10_CD 2_CD 10_CD 1_CD 10_CD 0_CD 10_CD 1_CD 10_CD 2_CD Number_NN of_IN Resources_NNP |_CD |_CD CPUTime_NN ,_, sec_NN |_CD M_NN |_NN =_JJ #_# ,_, =_JJ ##_CD static_JJ dynamic_JJ 2_CD #_# #_# #_# ##_CD 200_CD 400_CD 600_CD 800_CD 1000_CD 1200_CD 1400_CD 1600_CD Number_NNP of_IN Agents_NNPS |_CD M_NN |_CD Value_NNP |_CD |_NN =_JJ #_# ,_, =_JJ ##_CD static_JJ dynamic_JJ 50_CD ###_CD ###_CD ###_CD 400_CD 500_CD 600_CD 700_CD 800_CD 900_CD 1000_CD 1100_CD 1200_CD 1300_CD 1400_CD Global_JJ Time_NNP Boundary_NNP Value_NNP |_CD M_NN |_NN =_JJ #_# ,_, |_CD |_NN =_JJ #_# static_JJ dynamic_JJ 10_CD ##_CD ##_CD ##_CD ##_NN 500_CD 600_CD 700_CD 800_CD 900_CD 1000_CD 1100_CD 1200_CD 1300_CD 1400_CD Number_NNP of_IN Resources_NNPS |_VBP |_CD Value_NNP |_CD M_NN |_NN =_JJ #_# ,_, =_JJ ##_CD static_JJ dynamic_JJ Figure_NN #_# :_: Evaluation_NN of_IN our_PRP$ MILP_NN for_IN variable_JJ numbers_NNS of_IN agents_NNS -LRB-_-LRB- column_NN #_# -RRB-_-RRB- ,_, lengths_NNS of_IN global-time_JJ window_NN -LRB-_-LRB- column_NN 2_CD -RRB-_-RRB- ,_, and_CC numbers_NNS of_IN resource_NN types_NNS -LRB-_-LRB- column_NN #_# -RRB-_-RRB- ._.
Top_JJ row_NN shows_VBZ CPU_NNP time_NN ,_, and_CC bottom_JJ row_NN shows_VBZ the_DT joint_JJ reward_NN of_IN agents_NNS ''_'' MDP_NN policies_NNS ._.
Error_NN bars_NNS show_VBP the_DT 1st_JJ and_CC 3rd_JJ quartiles_NNS -LRB-_-LRB- ##_CD %_NN and_CC ##_CD %_NN -RRB-_-RRB- ._.
2_CD #_# #_# #_# ##_CD 10_CD 3_CD 10_CD 2_CD 10_CD 1_CD 10_CD 0_CD 10_CD 1_CD 10_CD 2_CD 10_CD 3_CD Number_NN of_IN Agents_NNPS |_CD M_NN |_CD CPUTime_NN ,_, sec_NN =_JJ ##_CD |_CD M_NN |_CD static_JJ dynamic_JJ 2_CD #_# #_# #_# ##_CD 10_CD 3_CD 10_CD 2_CD 10_CD 1_CD 10_CD 0_CD 10_CD 1_CD 10_CD 2_CD 10_CD 3_CD 10_CD 4_CD Number_NN of_IN Agents_NNPS |_CD M_NN |_CD CPUTime_NN ,_, sec_NN |_CD |_NN =_JJ #_# |_CD M_NN |_CD static_JJ dynamic_JJ 2_CD #_# #_# #_# ##_CD 10_CD 3_CD 10_CD 2_CD 10_CD 1_CD 10_CD 0_CD 10_CD 1_CD 10_CD 2_CD 10_CD 3_CD 10_CD 4_CD Number_NN of_IN Agents_NNPS |_CD M_NN |_CD CPUTime_NN ,_, sec_NN |_CD |_NN =_JJ #_# |_CD M_NN |_CD static_JJ dynamic_JJ 2_CD #_# #_# #_# ##_CD 200_CD 400_CD 600_CD 800_CD 1000_CD 1200_CD 1400_CD 1600_CD 1800_CD 2000_CD 2200_CD Number_NNP of_IN Agents_NNPS |_CD M_NN |_CD Value_NN =_JJ ##_CD |_CD M_NN |_CD static_JJ dynamic_JJ 2_CD #_# #_# #_# ##_CD 200_CD 400_CD 600_CD 800_CD 1000_CD 1200_CD 1400_CD 1600_CD 1800_CD 2000_CD Number_NNP of_IN Agents_NNPS |_CD M_NN |_CD Value_NNP |_CD |_NN =_JJ #_# |_CD M_NN |_CD static_JJ dynamic_JJ 2_CD #_# #_# #_# ##_CD 0_CD 500_CD 1000_CD 1500_CD 2000_CD 2500_CD Number_NNP of_IN Agents_NNPS |_CD M_NN |_CD Value_NNP |_CD |_NN =_JJ #_# |_CD M_NN |_CD static_JJ dynamic_JJ Figure_NN #_# :_: Evaluation_NN of_IN our_PRP$ MILP_NN using_VBG correlated_VBD input_NN variables_NNS ._.
The_DT left_JJ column_NN tracks_VBZ the_DT performance_NN and_CC CPU_NNP time_NN as_IN the_DT number_NN of_IN agents_NNS and_CC global-time_JJ window_NN increase_NN together_RB -LRB-_-LRB- b_NN =_JJ ##_CD |_CD M_NN |_NN -RRB-_-RRB- ._.
The_DT middle_JJ and_CC the_DT right_JJ column_NN track_VBP the_DT performance_NN and_CC CPU_NNP time_NN as_IN the_DT number_NN of_IN resources_NNS and_CC the_DT number_NN of_IN agents_NNS increase_VBP together_RB as_IN |_CD |_NN =_JJ #_# |_CD M_NN |_NN and_CC |_NN |_NN =_JJ #_# |_CD M_NN |_NN ,_, respectively_RB ._.
Error_NN bars_NNS show_VBP the_DT 1st_JJ and_CC 3rd_JJ quartiles_NNS -LRB-_-LRB- ##_CD %_NN and_CC ##_CD %_NN -RRB-_-RRB- ._.
1226_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- us_PRP to_TO explore_VB domains_NNS where_WRB the_DT total_JJ number_NN of_IN agents_NNS scales_NNS proportionally_RB to_TO the_DT total_JJ number_NN of_IN resource_NN types_NNS or_CC the_DT global_JJ time_NN horizon_NN ,_, while_IN keeping_VBG constant_JJ the_DT average_JJ agent_NN density_NN -LRB-_-LRB- per_IN unit_NN of_IN global_JJ time_NN -RRB-_-RRB- or_CC the_DT average_JJ number_NN of_IN resources_NNS per_IN agent_NN -LRB-_-LRB- which_WDT commonly_RB occurs_VBZ in_IN real-life_JJ applications_NNS -RRB-_-RRB- ._.
Overall_RB ,_, we_PRP believe_VBP that_IN these_DT experimental_JJ results_NNS indicate_VBP that_IN our_PRP$ MILP_NN formulation_NN can_MD be_VB used_VBN to_TO effectively_RB solve_VB resource-scheduling_JJ problems_NNS of_IN nontrivial_JJ size_NN ._.
6_CD ._.
DISCUSSION_NN AND_CC CONCLUSIONS_NNS Throughout_IN the_DT paper_NN ,_, we_PRP have_VBP made_VBN a_DT number_NN of_IN assumptions_NNS in_IN our_PRP$ model_NN and_CC solution_NN algorithm_NN ;_: we_PRP discuss_VBP their_PRP$ implications_NNS below_IN ._.
Continual_JJ execution_NN ._.
We_PRP assume_VBP that_IN once_RB an_DT agent_NN stops_VBZ executing_VBG its_PRP$ MDP_NN -LRB-_-LRB- transitions_NNS into_IN state_NN sf_NN -RRB-_-RRB- ,_, it_PRP exits_NNS the_DT system_NN and_CC can_MD not_RB return_VB ._.
It_PRP is_VBZ easy_JJ to_TO relax_VB this_DT assumption_NN for_IN domains_NNS where_WRB agents_NNS ''_'' MDPs_NNS can_MD be_VB paused_VBN and_CC restarted_VBN ._.
All_DT that_WDT is_VBZ required_VBN is_VBZ to_TO include_VB an_DT additional_JJ pause_NN action_NN which_WDT transitions_NNS from_IN a_DT given_VBN state_NN back_RB to_TO itself_PRP ,_, and_CC has_VBZ zero_CD reward_NN ._.
Indifference_NN to_TO start_VB time_NN ._.
We_PRP used_VBD a_DT reward_NN model_NN where_WRB agents_NNS ''_'' rewards_NNS depend_VBP only_RB on_IN the_DT time_NN horizon_NN of_IN their_PRP$ MDPs_NNS and_CC not_RB the_DT global_JJ start_NN time_NN ._.
This_DT is_VBZ a_DT consequence_NN of_IN our_PRP$ MDP-augmentation_NN procedure_NN from_IN Section_NN #_# ._.
#_# ._.
It_PRP is_VBZ easy_JJ to_TO extend_VB the_DT model_NN so_IN that_IN the_DT agents_NNS incur_VBP an_DT explicit_JJ penalty_NN for_IN idling_VBG by_IN assigning_VBG a_DT non-zero_JJ negative_JJ reward_NN to_TO the_DT start_NN state_NN sb_NN ._.
Binary_JJ resource_NN requirements_NNS ._.
For_IN simplicity_NN ,_, we_PRP have_VBP assumed_VBN that_IN resource_NN costs_NNS are_VBP binary_JJ :_: m_NN -LRB-_-LRB- a_DT ,_, -RRB-_-RRB- =_JJ -LCB-_-LRB- #_# ,_, #_# -RCB-_-RRB- ,_, but_CC our_PRP$ results_NNS generalize_VBP in_IN a_DT straightforward_JJ manner_NN to_TO non-binary_JJ resource_NN mappings_NNS ,_, analogously_RB to_TO the_DT procedure_NN used_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Cooperative_NNP agents_NNS ._.
The_DT optimization_NN procedure_NN discussed_VBN in_IN this_DT paper_NN was_VBD developed_VBN in_IN the_DT context_NN of_IN cooperative_JJ agents_NNS ,_, but_CC it_PRP can_MD also_RB be_VB used_VBN to_TO design_VB a_DT mechanism_NN for_IN scheduling_NN resources_NNS among_IN self-interested_JJ agents_NNS ._.
This_DT optimization_NN procedure_NN can_MD be_VB embedded_VBN in_IN a_DT VickreyClarke-Groves_JJ auction_NN ,_, completely_RB analogously_RB to_TO the_DT way_NN it_PRP was_VBD done_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
In_IN fact_NN ,_, all_PDT the_DT results_NNS of_IN -LSB-_-LRB- #_# -RSB-_-RRB- about_IN the_DT properties_NNS of_IN the_DT auction_NN and_CC information_NN privacy_NN directly_RB carry_VBP over_RP to_TO the_DT scheduling_NN domain_NN discussed_VBN in_IN this_DT paper_NN ,_, requiring_VBG only_RB slight_JJ modifications_NNS to_TO deal_VB with_IN finitehorizon_NN MDPs_NNS ._.
Known_VBN ,_, deterministic_JJ arrival_NN and_CC departure_NN times_NNS ._.
Finally_RB ,_, we_PRP have_VBP assumed_VBN that_IN agents_NNS ''_'' arrival_NN and_CC departure_NN times_NNS -LRB-_-LRB- a_DT m_NN and_CC d_NN m_NN -RRB-_-RRB- are_VBP deterministic_JJ and_CC known_VBN a_DT priori_FW ._.
This_DT assumption_NN is_VBZ fundamental_JJ to_TO our_PRP$ solution_NN method_NN ._.
While_IN there_EX are_VBP many_JJ domains_NNS where_WRB this_DT assumption_NN is_VBZ valid_JJ ,_, in_IN many_JJ cases_NNS agents_NNS arrive_VBP and_CC depart_VBP dynamically_RB and_CC their_PRP$ arrival_NN and_CC departure_NN times_NNS can_MD only_RB be_VB predicted_VBN probabilistically_RB ,_, leading_VBG to_TO online_JJ resource-allocation_NN problems_NNS ._.
In_IN particular_JJ ,_, in_IN the_DT case_NN of_IN self-interested_JJ agents_NNS ,_, this_DT becomes_VBZ an_DT interesting_JJ version_NN of_IN an_DT online-mechanism-design_JJ problem_NN -LSB-_-LRB- ##_CD ,_, ##_CD -RSB-_-RRB- ._.
In_IN summary_NN ,_, we_PRP have_VBP presented_VBN an_DT MILP_NN formulation_NN for_IN the_DT combinatorial_JJ resource-scheduling_JJ problem_NN where_WRB agents_NNS ''_'' values_NNS for_IN possible_JJ resource_NN assignments_NNS are_VBP defined_VBN by_IN finitehorizon_NN MDPs_NNS ._.
This_DT result_NN extends_VBZ previous_JJ work_NN -LRB-_-LRB- -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- -RRB-_-RRB- on_IN static_JJ one-shot_JJ resource_NN allocation_NN under_IN MDP-induced_JJ preferences_NNS to_TO resource-scheduling_JJ problems_NNS with_IN a_DT temporal_JJ aspect_NN ._.
As_IN such_JJ ,_, this_DT work_NN takes_VBZ a_DT step_NN in_IN the_DT direction_NN of_IN designing_VBG an_DT online_JJ mechanism_NN for_IN agents_NNS with_IN combinatorial_JJ resource_NN preferences_NNS induced_VBN by_IN stochastic_JJ planning_NN problems_NNS ._.
Relaxing_VBG the_DT assumption_NN about_IN deterministic_JJ arrival_NN and_CC departure_NN times_NNS of_IN the_DT agents_NNS is_VBZ a_DT focus_NN of_IN our_PRP$ future_JJ work_NN ._.
We_PRP would_MD like_VB to_TO thank_VB the_DT anonymous_JJ reviewers_NNS for_IN their_PRP$ insightful_JJ comments_NNS and_CC suggestions_NNS ._.
7_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- E_NN ._.
Altman_NNP and_CC A_NNP ._.
Shwartz_NNP ._.
Adaptive_JJ control_NN of_IN constrained_VBN Markov_NNP chains_NNS :_: Criteria_NNS and_CC policies_NNS ._.
Annals_NNS of_IN Operations_NNP Research_NNP ,_, special_JJ issue_NN on_IN Markov_NNP Decision_NNP Processes_NNP ,_, ##_CD :_: 101-134_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- R_NN ._.
Bellman_NNP ._.
Dynamic_NNP Programming_NNP ._.
Princeton_NNP University_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- C_NN ._.
Boutilier_NNP ._.
Solving_VBG concisely_RB expressed_VBN combinatorial_JJ auction_NN problems_NNS ._.
In_IN Proc_NNP ._.
of_IN AAAI-02_NN ,_, pages_NNS 359-366_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- C_NN ._.
Boutilier_NNP and_CC H_NNP ._.
H_NN ._.
Hoos_NNP ._.
Bidding_NN languages_NNS for_IN combinatorial_JJ auctions_NNS ._.
In_IN Proc_NNP ._.
of_IN IJCAI-01_NN ,_, pages_NNS 1211-1217_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Dolgov_NNP ._.
Integrated_NNP Resource_NNP Allocation_NNP and_CC Planning_NNP in_IN Stochastic_NNP Multiagent_NNP Environments_NNS ._.
PhD_NN thesis_NN ,_, Computer_NNP Science_NNP Department_NNP ,_, University_NNP of_IN Michigan_NNP ,_, February_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
A_DT ._.
Dolgov_NNP and_CC E_NNP ._.
H_NN ._.
Durfee_NNP ._.
Optimal_JJ resource_NN allocation_NN and_CC policy_NN formulation_NN in_IN loosely-coupled_JJ Markov_NNP decision_NN processes_NNS ._.
In_IN Proc_NNP ._.
of_IN ICAPS-04_NN ,_, pages_NNS 315-324_CD ,_, June_NNP ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
A_DT ._.
Dolgov_NNP and_CC E_NNP ._.
H_NN ._.
Durfee_NNP ._.
Computationally_RB efficient_JJ combinatorial_JJ auctions_NNS for_IN resource_NN allocation_NN in_IN weakly-coupled_JJ MDPs_NNS ._.
In_IN Proc_NNP ._.
of_IN AAMAS-05_NN ,_, New_NNP York_NNP ,_, NY_NNP ,_, USA_NNP ,_, ####_CD ._.
ACM_NNP Press_NNP ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
A_DT ._.
Dolgov_NNP and_CC E_NNP ._.
H_NN ._.
Durfee_NNP ._.
Resource_NNP allocation_NN among_IN agents_NNS with_IN preferences_NNS induced_VBN by_IN factored_JJ MDPs_NNS ._.
In_IN Proc_NNP ._.
of_IN AAMAS-06_NN ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- K_NNP ._.
Larson_NNP and_CC T_NN ._.
Sandholm_NNP ._.
Mechanism_NN design_NN and_CC deliberative_JJ agents_NNS ._.
In_IN Proc_NNP ._.
of_IN AAMAS-05_NN ,_, pages_NNS 650-656_CD ,_, New_NNP York_NNP ,_, NY_NNP ,_, USA_NNP ,_, ####_CD ._.
ACM_NNP Press_NNP ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- N_NN ._.
Nisan_NNP ._.
Bidding_NN and_CC allocation_NN in_IN combinatorial_JJ auctions_NNS ._.
In_IN Electronic_JJ Commerce_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
C_NN ._.
Parkes_NN and_CC S_NN ._.
Singh_NNP ._.
An_DT MDP-based_JJ approach_NN to_TO Online_NNP Mechanism_NN Design_NN ._.
In_IN Proc_NNP ._.
of_IN the_DT Seventeenths_NNP Annual_JJ Conference_NN on_IN Neural_NNP Information_NNP Processing_NNP Systems_NNP -LRB-_-LRB- NIPS-03_NN -RRB-_-RRB- ,_, ####_NN ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
C_NN ._.
Parkes_NNP ,_, S_NN ._.
Singh_NNP ,_, and_CC D_NN ._.
Yanovsky_NNP ._.
Approximately_RB efficient_JJ online_JJ mechanism_NN design_NN ._.
In_IN Proc_NNP ._.
of_IN the_DT Eighteenths_NNP Annual_JJ Conference_NN on_IN Neural_NNP Information_NNP Processing_NNP Systems_NNP -LRB-_-LRB- NIPS-04_NN -RRB-_-RRB- ,_, ####_NN ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- M_NN ._.
L_NN ._.
Puterman_NNP ._.
Markov_NNP Decision_NNP Processes_NNP ._.
John_NNP Wiley_NNP &_CC Sons_NNP ,_, New_NNP York_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- M_NN ._.
H_NN ._.
Rothkopf_NNP ,_, A_NNP ._.
Pekec_NNP ,_, and_CC R_NN ._.
M_NN ._.
Harstad_NNP ._.
Computationally_RB manageable_JJ combinational_JJ auctions_NNS ._.
Management_NNP Science_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 1131-1147_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- T_NN ._.
Sandholm_NNP ._.
An_DT algorithm_NN for_IN optimal_JJ winner_NN determination_NN in_IN combinatorial_JJ auctions_NNS ._.
In_IN Proc_NNP ._.
of_IN IJCAI-99_NN ,_, pages_NNS 542-547_CD ,_, San_NNP Francisco_NNP ,_, CA_NNP ,_, USA_NNP ,_, 1999_CD ._.
Morgan_NNP Kaufmann_NNP Publishers_NNPS Inc_NNP ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ####_CD
