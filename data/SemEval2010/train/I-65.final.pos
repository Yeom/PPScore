Graphical_NNP Models_NNS for_IN Online_NNP Solutions_NNPS to_TO Interactive_JJ POMDPs_NNS Prashant_NNP Doshi_NNP Dept_NNP ._.
of_IN Computer_NNP Science_NNP University_NNP of_IN Georgia_NNP Athens_NNP ,_, GA_NNP #####_NNP ,_, USA_NNP pdoshi_NNS @_IN cs_NNS ._.
uga_NN ._.
edu_NN Yifeng_NNP Zeng_NNP Dept_NNP ._.
of_IN Computer_NNP Science_NNP Aalborg_NNP University_NNP DK-9220_NN Aalborg_NNP ,_, Denmark_NNP yfzeng_NN @_IN cs_NNS ._.
aau_NN ._.
edu_NN Qiongyu_NNP Chen_NNP Dept_NNP ._.
of_IN Computer_NNP Science_NNP National_NNP Univ_NNP ._.
of_IN Singapore_NNP 117543_CD ,_, Singapore_NNP chenqy_NN @_IN comp_NN ._.
nus_NN ._.
edu_NN ._.
sg_NN ABSTRACT_NN We_PRP develop_VBP a_DT new_JJ graphical_JJ representation_NN for_IN interactive_JJ partially_RB observable_JJ Markov_NNP decision_NN processes_NNS -LRB-_-LRB- I-POMDPs_NNS -RRB-_-RRB- that_WDT is_VBZ significantly_RB more_RBR transparent_JJ and_CC semantically_RB clear_JJ than_IN the_DT previous_JJ representation_NN ._.
These_DT graphical_JJ models_NNS called_VBN interactive_JJ dynamic_JJ influence_NN diagrams_NNS -LRB-_-LRB- I-DIDs_NNS -RRB-_-RRB- seek_VBP to_TO explicitly_RB model_VB the_DT structure_NN that_WDT is_VBZ often_RB present_JJ in_IN real-world_JJ problems_NNS by_IN decomposing_VBG the_DT situation_NN into_IN chance_NN and_CC decision_NN variables_NNS ,_, and_CC the_DT dependencies_NNS between_IN the_DT variables_NNS ._.
I-DIDs_JJ generalize_VBP DIDs_NNS ,_, which_WDT may_MD be_VB viewed_VBN as_IN graphical_JJ representations_NNS of_IN POMDPs_NNS ,_, to_TO multiagent_JJ settings_NNS in_IN the_DT same_JJ way_NN that_IN I-POMDPs_NNS generalize_VBP POMDPs_NNS ._.
I-DIDs_NNS may_MD be_VB used_VBN to_TO compute_VB the_DT policy_NN of_IN an_DT agent_NN online_NN as_IN the_DT agent_NN acts_VBZ and_CC observes_VBZ in_IN a_DT setting_NN that_WDT is_VBZ populated_VBN by_IN other_JJ interacting_VBG agents_NNS ._.
Using_VBG several_JJ examples_NNS ,_, we_PRP show_VBP how_WRB I-DIDs_NNS may_MD be_VB applied_VBN and_CC demonstrate_VBP their_PRP$ usefulness_NN ._.
Categories_NNS and_CC Subject_NNP Descriptors_NNS I_PRP ._.
#_# ._.
##_NN -LSB-_-LRB- Distributed_VBN Artificial_NNP Intelligence_NNP -RSB-_-RRB- :_: Multiagent_NNP Systems_NNP General_NNP Terms_NNS Theory_NNP 1_CD ._.
INTRODUCTION_NNP Interactive_JJ partially_RB observable_JJ Markov_NNP decision_NN processes_NNS -LRB-_-LRB- IPOMDPs_NNS -RRB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- provide_VBP a_DT framework_NN for_IN sequential_JJ decision-making_NN in_IN partially_RB observable_JJ multiagent_JJ environments_NNS ._.
They_PRP generalize_VBP POMDPs_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- to_TO multiagent_JJ settings_NNS by_IN including_VBG the_DT other_JJ agents_NNS ''_'' computable_JJ models_NNS in_IN the_DT state_NN space_NN along_IN with_IN the_DT states_NNS of_IN the_DT physical_JJ environment_NN ._.
The_DT models_NNS encompass_VBP all_DT information_NN influencing_VBG the_DT agents_NNS ''_'' behaviors_NNS ,_, including_VBG their_PRP$ preferences_NNS ,_, capabilities_NNS ,_, and_CC beliefs_NNS ,_, and_CC are_VBP thus_RB analogous_JJ to_TO types_NNS in_IN Bayesian_JJ games_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
I-POMDPs_JJ adopt_VB a_DT subjective_JJ approach_NN to_TO understanding_VBG strategic_JJ behavior_NN ,_, rooted_VBN in_IN a_DT decision-theoretic_JJ framework_NN that_WDT takes_VBZ a_DT decision-maker_NN ''_'' s_NNS perspective_NN in_IN the_DT interaction_NN ._.
In_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ,_, Polich_NNP and_CC Gmytrasiewicz_NNP introduced_VBD interactive_JJ dynamic_JJ influence_NN diagrams_NNS -LRB-_-LRB- I-DIDs_NNS -RRB-_-RRB- as_IN the_DT computational_JJ representations_NNS of_IN I-POMDPs_NNS ._.
I-DIDs_JJ generalize_VBP DIDs_NNS -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, which_WDT may_MD be_VB viewed_VBN as_IN computational_JJ counterparts_NNS of_IN POMDPs_NNS ,_, to_TO multiagents_NNS settings_NNS in_IN the_DT same_JJ way_NN that_IN I-POMDPs_NNS generalize_VBP POMDPs_NNS ._.
I-DIDs_JJ contribute_VBP to_TO a_DT growing_VBG line_NN of_IN work_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- that_WDT includes_VBZ multi-agent_JJ influence_NN diagrams_NNS -LRB-_-LRB- MAIDs_NNS -RRB-_-RRB- -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, and_CC more_RBR recently_RB ,_, networks_NNS of_IN influence_NN diagrams_NNS -LRB-_-LRB- NIDs_NNS -RRB-_-RRB- -LSB-_-LRB- #_# -RSB-_-RRB- ._.
These_DT formalisms_NNS seek_VBP to_TO explicitly_RB model_VB the_DT structure_NN that_WDT is_VBZ often_RB present_JJ in_IN real-world_JJ problems_NNS by_IN decomposing_VBG the_DT situation_NN into_IN chance_NN and_CC decision_NN variables_NNS ,_, and_CC the_DT dependencies_NNS between_IN the_DT variables_NNS ._.
MAIDs_NNS provide_VBP an_DT alternative_NN to_TO normal_JJ and_CC extensive_JJ game_NN forms_NNS using_VBG a_DT graphical_JJ formalism_NN to_TO represent_VB games_NNS of_IN imperfect_JJ information_NN with_IN a_DT decision_NN node_NN for_IN each_DT agent_NN ''_'' s_NNS actions_NNS and_CC chance_NN nodes_NNS capturing_VBG the_DT agent_NN ''_'' s_NNS private_JJ information_NN ._.
MAIDs_NNS objectively_RB analyze_VBP the_DT game_NN ,_, efficiently_RB computing_VBG the_DT Nash_NNP equilibrium_NN profile_NN by_IN exploiting_VBG the_DT independence_NN structure_NN ._.
NIDs_NNS extend_VBP MAIDs_NNS to_TO include_VB agents_NNS ''_'' uncertainty_NN over_IN the_DT game_NN being_VBG played_VBN and_CC over_IN models_NNS of_IN the_DT other_JJ agents_NNS ._.
Each_DT model_NN is_VBZ a_DT MAID_NN and_CC the_DT network_NN of_IN MAIDs_NNS is_VBZ collapsed_VBN ,_, bottom_NN up_RB ,_, into_IN a_DT single_JJ MAID_NN for_IN computing_VBG the_DT equilibrium_NN of_IN the_DT game_NN keeping_VBG in_IN mind_NN the_DT different_JJ models_NNS of_IN each_DT agent_NN ._.
Graphical_JJ formalisms_NNS such_JJ as_IN MAIDs_NNS and_CC NIDs_NNS open_VBP up_RP a_DT promising_JJ area_NN of_IN research_NN that_WDT aims_VBZ to_TO represent_VB multiagent_JJ interactions_NNS more_RBR transparently_RB ._.
However_RB ,_, MAIDs_NNS provide_VBP an_DT analysis_NN of_IN the_DT game_NN from_IN an_DT external_JJ viewpoint_NN and_CC the_DT applicability_NN of_IN both_CC is_VBZ limited_VBN to_TO static_JJ single_JJ play_NN games_NNS ._.
Matters_NNS are_VBP more_RBR complex_JJ when_WRB we_PRP consider_VBP interactions_NNS that_WDT are_VBP extended_VBN over_IN time_NN ,_, where_WRB predictions_NNS about_IN others_NNS ''_'' future_JJ actions_NNS must_MD be_VB made_VBN using_VBG models_NNS that_WDT change_VBP as_IN the_DT agents_NNS act_VBP and_CC observe_VBP ._.
I-DIDs_JJ address_NN this_DT gap_NN by_IN allowing_VBG the_DT representation_NN of_IN other_JJ agents_NNS ''_'' models_NNS as_IN the_DT values_NNS of_IN a_DT special_JJ model_NN node_NN ._.
Both_DT ,_, other_JJ agents_NNS ''_'' models_NNS and_CC the_DT original_JJ agent_NN ''_'' s_NNS beliefs_NNS over_IN these_DT models_NNS are_VBP updated_VBN over_IN time_NN using_VBG special-purpose_JJ implementations_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP improve_VBP on_IN the_DT previous_JJ preliminary_JJ representation_NN of_IN the_DT I-DID_NN shown_VBN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- by_IN using_VBG the_DT insight_NN that_IN the_DT static_JJ I-ID_NN is_VBZ a_DT type_NN of_IN NID_NN ._.
Thus_RB ,_, we_PRP may_MD utilize_VB NID-specific_JJ language_NN constructs_NNS such_JJ as_IN multiplexers_NNS to_TO represent_VB the_DT model_NN node_NN ,_, and_CC subsequently_RB the_DT I-ID_NN ,_, more_JJR transparently_RB ._.
Furthermore_RB ,_, we_PRP clarify_VBP the_DT semantics_NNS of_IN the_DT special_JJ purpose_NN policy_NN link_NN introduced_VBN in_IN the_DT representation_NN of_IN I-DID_NN by_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ,_, and_CC show_VBP that_IN it_PRP could_MD be_VB replaced_VBN by_IN traditional_JJ dependency_NN links_NNS ._.
In_IN the_DT previous_JJ representation_NN of_IN the_DT I-DID_NN ,_, the_DT update_VB of_IN the_DT agent_NN ''_'' s_NNS belief_NN over_IN the_DT models_NNS of_IN others_NNS as_IN the_DT agents_NNS act_VBP and_CC receive_VBP observations_NNS was_VBD denoted_VBN using_VBG a_DT special_JJ link_NN called_VBD the_DT model_NN update_VBP link_NN that_WDT connected_VBD the_DT model_NN nodes_NNS over_IN time_NN ._.
We_PRP explicate_VBP the_DT semantics_NNS of_IN this_DT link_NN by_IN showing_VBG how_WRB it_PRP can_MD be_VB implemented_VBN using_VBG the_DT traditional_JJ dependency_NN links_NNS between_IN the_DT chance_NN nodes_NNS that_WDT constitute_VBP the_DT model_NN nodes_NNS ._.
The_DT net_JJ result_NN is_VBZ a_DT representation_NN of_IN I-DID_NN that_WDT is_VBZ significantly_RB more_RBR transparent_JJ ,_, semantically_RB clear_JJ ,_, and_CC capable_JJ of_IN being_VBG implemented_VBN using_VBG the_DT standard_JJ algorithms_NNS for_IN solving_VBG DIDs_NNS ._.
We_PRP show_VBP how_WRB IDIDs_NNS may_MD be_VB used_VBN to_TO model_VB an_DT agent_NN ''_'' s_NNS uncertainty_NN over_IN others_NNS ''_'' models_NNS ,_, that_WDT may_MD themselves_PRP be_VB I-DIDs_NNS ._.
Solution_NN to_TO the_DT I-DID_NN is_VBZ a_DT policy_NN that_WDT prescribes_VBZ what_WP the_DT agent_NN should_MD do_VB over_IN time_NN ,_, given_VBN its_PRP$ beliefs_NNS over_IN the_DT physical_JJ state_NN and_CC others_NNS ''_'' models_NNS ._.
Analogous_JJ to_TO DIDs_NNS ,_, I-DIDs_NNS may_MD be_VB used_VBN to_TO compute_VB the_DT policy_NN of_IN an_DT agent_NN online_NN as_IN the_DT agent_NN acts_VBZ and_CC observes_VBZ in_IN a_DT setting_NN that_WDT is_VBZ populated_VBN by_IN other_JJ interacting_VBG agents_NNS ._.
2_LS ._.
BACKGROUND_NN :_: FINITELY_NNP NESTED_NNP IPOMDPS_NNP Interactive_JJ POMDPs_NNS generalize_VBP POMDPs_NNS to_TO multiagent_JJ settings_NNS by_IN including_VBG other_JJ agents_NNS ''_'' models_NNS as_IN part_NN of_IN the_DT state_NN space_NN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Since_IN other_JJ agents_NNS may_MD also_RB reason_VB about_IN others_NNS ,_, the_DT interactive_JJ state_NN space_NN is_VBZ strategically_RB nested_JJ ;_: it_PRP contains_VBZ beliefs_NNS about_IN other_JJ agents_NNS ''_'' models_NNS and_CC their_PRP$ beliefs_NNS about_IN others_NNS ._.
For_IN simplicity_NN of_IN presentation_NN we_PRP consider_VBP an_DT agent_NN ,_, i_FW ,_, that_DT is_VBZ interacting_VBG with_IN one_CD other_JJ agent_NN ,_, j_NN ._.
A_DT finitely_JJ nested_JJ I-POMDP_NN of_IN agent_NN i_FW with_IN a_DT strategy_NN level_NN l_NN is_VBZ defined_VBN as_IN the_DT tuple_NN :_: I-POMDPi_NN ,_, l_NN =_JJ ISi_NN ,_, l_NN ,_, A_NN ,_, Ti_NNP ,_, i_FW ,_, Oi_NN ,_, Ri_NNP where_WRB :_: ISi_NNS ,_, l_NN denotes_VBZ a_DT set_NN of_IN interactive_JJ states_NNS defined_VBN as_IN ,_, ISi_NNP ,_, l_NN =_JJ S_NN Mj_NN ,_, l1_NN ,_, where_WRB Mj_NNP ,_, l1_NN =_JJ -LCB-_-LRB- j_NN ,_, l1_NN SMj_NN -RCB-_-RRB- ,_, for_IN l_NN #_# ,_, and_CC ISi_NNP ,_, #_# =_JJ S_NN ,_, where_WRB S_NN is_VBZ the_DT set_NN of_IN states_NNS of_IN the_DT physical_JJ environment_NN ._.
j_NN ,_, l1_NN is_VBZ the_DT set_NN of_IN computable_JJ intentional_JJ models_NNS of_IN agent_NN j_NN :_: j_NN ,_, l1_NN =_JJ bj_NN ,_, l1_NN ,_, j_NN where_WRB the_DT frame_NN ,_, j_NN =_JJ A_NN ,_, j_NN ,_, Tj_NN ,_, Oj_NN ,_, Rj_NN ,_, OCj_NN ._.
Here_RB ,_, j_NN is_VBZ Bayes_NNP rational_JJ and_CC OCj_NN is_VBZ j_NN ''_'' s_NNS optimality_NN criterion_NN ._.
SMj_NN is_VBZ the_DT set_NN of_IN subintentional_JJ models_NNS of_IN j_NN ._.
Simple_JJ examples_NNS of_IN subintentional_JJ models_NNS include_VBP a_DT no-information_NN model_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- and_CC a_DT fictitious_JJ play_NN model_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, both_DT of_IN which_WDT are_VBP history_NN independent_JJ ._.
We_PRP give_VBP a_DT recursive_JJ bottom-up_JJ construction_NN of_IN the_DT interactive_JJ state_NN space_NN below_IN ._.
ISi_NNP ,_, #_# =_JJ S_NN ,_, j_NN ,_, #_# =_SYM -LCB-_-LRB- bj_NN ,_, #_# ,_, j_NN |_CD bj_NN ,_, #_# -LRB-_-LRB- ISj_NNP ,_, #_# -RRB-_-RRB- -RCB-_-RRB- ISi_NNP ,_, #_# =_JJ S_NN -LCB-_-LRB- j_NN ,_, #_# SMj_FW -RCB-_-RRB- ,_, j_NN ,_, #_# =_SYM -LCB-_-LRB- bj_NN ,_, #_# ,_, j_NN |_CD bj_NN ,_, #_# -LRB-_-LRB- ISj_NNP ,_, #_# -RRB-_-RRB- -RCB-_-RRB- ._. ._. ._. ._. ._. ._.
ISi_NNP ,_, l_NN =_JJ S_NN -LCB-_-LRB- j_NN ,_, l1_NN SMj_NN -RCB-_-RRB- ,_, j_NN ,_, l_NN =_JJ -LCB-_-LRB- bj_NN ,_, l_NN ,_, j_NN |_CD bj_NN ,_, l_NN -LRB-_-LRB- ISj_NN ,_, l_NN -RRB-_-RRB- -RCB-_-RRB- Similar_JJ formulations_NNS of_IN nested_JJ spaces_NNS have_VBP appeared_VBN in_IN -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ._.
A_DT =_JJ Ai_NNP Aj_NNP is_VBZ the_DT set_NN of_IN joint_JJ actions_NNS of_IN all_DT agents_NNS in_IN the_DT environment_NN ;_: Ti_NNP :_: S_NN AS_JJ -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- ,_, describes_VBZ the_DT effect_NN of_IN the_DT joint_JJ actions_NNS on_IN the_DT physical_JJ states_NNS of_IN the_DT environment_NN ;_: i_LS is_VBZ the_DT set_NN of_IN observations_NNS of_IN agent_NN i_FW ;_: Oi_NNP :_: S_NN A_DT i_FW -LSB-_-LRB- #_# ,_, #_# -RSB-_-RRB- gives_VBZ the_DT likelihood_NN of_IN the_DT observations_NNS given_VBN the_DT physical_JJ state_NN and_CC joint_JJ action_NN ;_: Ri_NNP :_: ISi_NNP A_NNP R_NN describes_VBZ agent_NN i_FW ''_'' s_VBZ preferences_NNS over_IN its_PRP$ interactive_JJ states_NNS ._.
Usually_RB only_RB the_DT physical_JJ states_NNS will_MD matter_VB ._.
Agent_NNP i_FW ''_'' s_VBZ policy_NN is_VBZ the_DT mapping_NN ,_, i_FW -LRB-_-LRB- Ai_NN -RRB-_-RRB- ,_, where_WRB i_FW is_VBZ the_DT set_NN of_IN all_DT observation_NN histories_NNS of_IN agent_NN i_FW ._.
Since_IN belief_NN over_IN the_DT interactive_JJ states_NNS forms_VBZ a_DT sufficient_JJ statistic_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, the_DT policy_NN can_MD also_RB be_VB represented_VBN as_IN a_DT mapping_NN from_IN the_DT set_NN of_IN all_DT beliefs_NNS of_IN agent_NN i_FW to_TO a_DT distribution_NN over_IN its_PRP$ actions_NNS ,_, -LRB-_-LRB- ISi_NNP -RRB-_-RRB- -LRB-_-LRB- Ai_NN -RRB-_-RRB- ._.
2_LS ._.
#_# Belief_NNP Update_NNP Analogous_JJ to_TO POMDPs_NNS ,_, an_DT agent_NN within_IN the_DT I-POMDP_NN framework_NN updates_NNS its_PRP$ belief_NN as_IN it_PRP acts_VBZ and_CC observes_VBZ ._.
However_RB ,_, there_EX are_VBP two_CD differences_NNS that_WDT complicate_VBP the_DT belief_NN update_VBP in_IN multiagent_JJ settings_NNS when_WRB compared_VBN to_TO single_JJ agent_NN ones_NNS ._.
First_RB ,_, since_IN the_DT state_NN of_IN the_DT physical_JJ environment_NN depends_VBZ on_IN the_DT actions_NNS of_IN both_DT agents_NNS ,_, i_FW ''_'' s_VBZ prediction_NN of_IN how_WRB the_DT physical_JJ state_NN changes_NNS has_VBZ to_TO be_VB made_VBN based_VBN on_IN its_PRP$ prediction_NN of_IN j_NN ''_'' s_NNS actions_NNS ._.
Second_RB ,_, changes_NNS in_IN j_NN ''_'' s_NNS models_NNS have_VBP to_TO be_VB included_VBN in_IN i_FW ''_'' s_VBZ belief_NN update_VB ._.
Specifically_RB ,_, if_IN j_NN is_VBZ intentional_JJ then_RB an_DT update_VB of_IN j_NN ''_'' s_NNS beliefs_NNS due_JJ to_TO its_PRP$ action_NN and_CC observation_NN has_VBZ to_TO be_VB included_VBN ._.
In_IN other_JJ words_NNS ,_, i_FW has_VBZ to_TO update_VB its_PRP$ belief_NN based_VBN on_IN its_PRP$ prediction_NN of_IN what_WP j_NN would_MD observe_VB and_CC how_WRB j_NN would_MD update_VB its_PRP$ belief_NN ._.
If_IN j_NN ''_'' s_NNS model_NN is_VBZ subintentional_JJ ,_, then_RB j_NN ''_'' s_NNS probable_JJ observations_NNS are_VBP appended_VBN to_TO the_DT observation_NN history_NN contained_VBD in_IN the_DT model_NN ._.
Formally_RB ,_, we_PRP have_VBP :_: Pr_NN -LRB-_-LRB- ist_NN |_CD at1_NN i_FW ,_, bt1_NN i_FW ,_, l_NN -RRB-_-RRB- =_JJ ISt1_NN :_: mt1_NN j_NN =_JJ t_NN j_NN bt1_NN i_FW ,_, l_NN -LRB-_-LRB- ist1_NN -RRB-_-RRB- at1_NN j_NN Pr_NN -LRB-_-LRB- at1_NN j_NN |_CD t1_NN j_NN ,_, l1_NN -RRB-_-RRB- Oi_NN -LRB-_-LRB- st_NN ,_, at1_NN i_FW ,_, at1_NN j_NN ,_, ot_NN i_LS -RRB-_-RRB- Ti_NN -LRB-_-LRB- st1_NN ,_, at1_NN i_FW ,_, at1_NN j_NN ,_, st_NN -RRB-_-RRB- ot_NN j_NN Oj_NN -LRB-_-LRB- st_NN ,_, at1_NN i_FW ,_, at1_NN j_NN ,_, ot_NN j_NN -RRB-_-RRB- -LRB-_-LRB- SEt_NN j_NN -LRB-_-LRB- bt1_NN j_NN ,_, l1_NN ,_, at1_NN j_NN ,_, ot_NN j_NN -RRB-_-RRB- bt_NN j_NN ,_, l1_NN -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB is_VBZ the_DT normalizing_VBG constant_JJ ,_, is_VBZ #_# if_IN its_PRP$ argument_NN is_VBZ #_# otherwise_RB it_PRP is_VBZ #_# ,_, Pr_NN -LRB-_-LRB- at1_NN j_NN |_CD t1_NN j_NN ,_, l1_NN -RRB-_-RRB- is_VBZ the_DT probability_NN that_WDT at1_NN j_NN is_VBZ Bayes_NNP rational_JJ for_IN the_DT agent_NN described_VBN by_IN model_NN t1_NN j_NN ,_, l1_NN ,_, and_CC SE_NN -LRB-_-LRB- -RRB-_-RRB- is_VBZ an_DT abbreviation_NN for_IN the_DT belief_NN update_VBP ._.
For_IN a_DT version_NN of_IN the_DT belief_NN update_VBP when_WRB j_NN ''_'' s_NNS model_NN is_VBZ subintentional_JJ ,_, see_VB -LSB-_-LRB- #_# -RSB-_-RRB- ._.
If_IN agent_NN j_NN is_VBZ also_RB modeled_VBN as_IN an_DT I-POMDP_NN ,_, then_RB i_FW ''_'' s_VBZ belief_NN update_VB invokes_VBZ j_NN ''_'' s_NNS belief_NN update_VBP -LRB-_-LRB- via_IN the_DT term_NN SEt_NN j_NN -LRB-_-LRB- bt1_NN j_NN ,_, l1_NN ,_, at1_NN j_NN ,_, ot_NN j_NN -RRB-_-RRB- -RRB-_-RRB- ,_, which_WDT in_IN turn_NN could_MD invoke_VB i_FW ''_'' s_VBZ belief_NN update_VB and_CC so_RB on_IN ._.
This_DT recursion_NN in_IN belief_NN nesting_JJ bottoms_NNS out_RP at_IN the_DT 0th_JJ level_NN ._.
At_IN this_DT level_NN ,_, the_DT belief_NN update_VB of_IN the_DT agent_NN reduces_VBZ to_TO a_DT POMDP_NN belief_NN update_VBP ._.
#_# For_IN illustrations_NNS of_IN the_DT belief_NN update_VBP ,_, additional_JJ details_NNS on_IN I-POMDPs_NNS ,_, and_CC how_WRB they_PRP compare_VBP with_IN other_JJ multiagent_JJ frameworks_NNS ,_, see_VBP -LSB-_-LRB- #_# -RSB-_-RRB- ._.
2_LS ._.
#_# Value_NNP Iteration_NNP Each_DT belief_NN state_NN in_IN a_DT finitely_RB nested_JJ I-POMDP_NN has_VBZ an_DT associated_VBN value_NN reflecting_VBG the_DT maximum_NN payoff_NN the_DT agent_NN can_MD expect_VB in_IN this_DT belief_NN state_NN :_: Un_NN -LRB-_-LRB- bi_NN ,_, l_NN ,_, i_LS -RRB-_-RRB- =_JJ max_NN aiAi_NN isISi_NN ,_, l_NN ERi_NNS -LRB-_-LRB- is_VBZ ,_, ai_VBP -RRB-_-RRB- bi_NN ,_, l_NN -LRB-_-LRB- is_VBZ -RRB-_-RRB- +_CC oii_NN Pr_NN -LRB-_-LRB- oi_NN |_CD ai_VBP ,_, bi_VBP ,_, l_NN -RRB-_-RRB- Un1_NN -LRB-_-LRB- SEi_NN -LRB-_-LRB- bi_NN ,_, l_NN ,_, ai_VBP ,_, oi_VBP -RRB-_-RRB- ,_, i_LS -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- where_WRB ,_, ERi_NN -LRB-_-LRB- is_VBZ ,_, ai_VBP -RRB-_-RRB- =_JJ aj_NN Ri_NN -LRB-_-LRB- is_VBZ ,_, ai_VBP ,_, aj_VBP -RRB-_-RRB- Pr_NN -LRB-_-LRB- aj_NN |_CD mj_NN ,_, l1_NN -RRB-_-RRB- -LRB-_-LRB- since_IN is_VBZ =_JJ -LRB-_-LRB- s_NNS ,_, mj_NN ,_, l1_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Eq_NN ._.
#_# is_VBZ a_DT basis_NN for_IN value_NN iteration_NN in_IN I-POMDPs_NNS ._.
Agent_NNP i_FW ''_'' s_VBZ optimal_JJ action_NN ,_, a_DT i_FW ,_, for_IN the_DT case_NN of_IN finite_JJ horizon_NN with_IN discounting_NN ,_, is_VBZ an_DT element_NN of_IN the_DT set_NN of_IN optimal_JJ actions_NNS for_IN the_DT belief_NN state_NN ,_, OPT_NN -LRB-_-LRB- i_LS -RRB-_-RRB- ,_, defined_VBN as_IN :_: OPT_NN -LRB-_-LRB- bi_NN ,_, l_NN ,_, i_LS -RRB-_-RRB- =_JJ argmax_NN aiAi_NN isISi_NN ,_, l_NN ERi_NNS -LRB-_-LRB- is_VBZ ,_, ai_VBP -RRB-_-RRB- bi_NN ,_, l_NN -LRB-_-LRB- is_VBZ -RRB-_-RRB- +_CC oii_NN Pr_NN -LRB-_-LRB- oi_NN |_CD ai_VBP ,_, bi_VBP ,_, l_NN -RRB-_-RRB- Un_NN -LRB-_-LRB- SEi_NN -LRB-_-LRB- bi_NN ,_, l_NN ,_, ai_VBP ,_, oi_VBP -RRB-_-RRB- ,_, i_LS -RRB-_-RRB- -LRB-_-LRB- #_# -RRB-_-RRB- 3_CD ._.
INTERACTIVEINFLUENCEDIAGRAMS_NNP A_NNP naive_JJ extension_NN of_IN influence_NN diagrams_NNS -LRB-_-LRB- IDs_NNS -RRB-_-RRB- to_TO settings_NNS populated_VBN by_IN multiple_JJ agents_NNS is_VBZ possible_JJ by_IN treating_VBG other_JJ agents_NNS as_IN automatons_NNS ,_, represented_VBN using_VBG chance_NN nodes_NNS ._.
However_RB ,_, this_DT approach_NN assumes_VBZ that_IN the_DT agents_NNS ''_'' actions_NNS are_VBP controlled_VBN using_VBG a_DT probability_NN distribution_NN that_WDT does_VBZ not_RB change_VB over_IN time_NN ._.
Interactive_JJ influence_NN diagrams_NNS -LRB-_-LRB- I-IDs_NNS -RRB-_-RRB- adopt_VBP a_DT more_RBR sophisticated_JJ approach_NN by_IN generalizing_VBG IDs_NNS to_TO make_VB them_PRP applicable_JJ to_TO settings_NNS shared_VBN with_IN other_JJ agents_NNS who_WP may_MD act_VB and_CC observe_VB ,_, and_CC update_VB their_PRP$ beliefs_NNS ._.
3_LS ._.
#_# Syntax_FW In_IN addition_NN to_TO the_DT usual_JJ chance_NN ,_, decision_NN ,_, and_CC utility_NN nodes_NNS ,_, IIDs_NNS include_VBP a_DT new_JJ type_NN of_IN node_NN called_VBD the_DT model_NN node_NN ._.
We_PRP show_VBP a_DT general_JJ level_NN l_NN I-ID_NN in_IN Fig_NN ._.
#_# -LRB-_-LRB- a_DT -RRB-_-RRB- ,_, where_WRB the_DT model_NN node_NN -LRB-_-LRB- Mj_NN ,_, l1_NN -RRB-_-RRB- is_VBZ denoted_VBN using_VBG a_DT hexagon_NN ._.
We_PRP note_VBP that_IN the_DT probability_NN distribution_NN over_IN the_DT chance_NN node_NN ,_, S_NN ,_, and_CC the_DT model_NN node_NN together_RB represents_VBZ agent_NN i_FW ''_'' s_VBZ belief_NN over_IN its_PRP$ interactive_JJ states_NNS ._.
In_IN addition_NN to_TO the_DT model_NN 1_CD The_DT 0th_JJ level_NN model_NN is_VBZ a_DT POMDP_NN :_: Other_JJ agent_NN ''_'' s_NNS actions_NNS are_VBP treated_VBN as_IN exogenous_JJ events_NNS and_CC folded_VBN into_IN the_DT T_NN ,_, O_NN ,_, and_CC R_NN functions_NNS ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD Figure_NNP #_# :_: -LRB-_-LRB- a_LS -RRB-_-RRB- A_DT generic_JJ level_NN l_NN I-ID_NN for_IN agent_NN i_FW situated_VBN with_IN one_CD other_JJ agent_NN j_NN ._.
The_DT hexagon_NN is_VBZ the_DT model_NN node_NN -LRB-_-LRB- Mj_NN ,_, l1_NN -RRB-_-RRB- whose_WP$ structure_NN we_PRP show_VBP in_IN -LRB-_-LRB- b_NN -RRB-_-RRB- ._.
Members_NNS of_IN the_DT model_NN node_NN are_VBP I-IDs_JJ themselves_PRP -LRB-_-LRB- m1_NN j_NN ,_, l1_NN ,_, m2_NN j_NN ,_, l1_NN ;_: diagrams_NNS not_RB shown_VBN here_RB for_IN simplicity_NN -RRB-_-RRB- whose_WP$ decision_NN nodes_NNS are_VBP mapped_VBN to_TO the_DT corresponding_JJ chance_NN nodes_NNS -LRB-_-LRB- A1_NN j_NN ,_, A2_NN j_NN -RRB-_-RRB- ._.
Depending_VBG on_IN the_DT value_NN of_IN the_DT node_NN ,_, Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- ,_, the_DT distribution_NN of_IN each_DT of_IN the_DT chance_NN nodes_NNS is_VBZ assigned_VBN to_TO the_DT node_NN Aj_NN ._.
-LRB-_-LRB- c_NN -RRB-_-RRB- The_DT transformed_VBN I-ID_NN with_IN the_DT model_NN node_NN replaced_VBN by_IN the_DT chance_NN nodes_NNS and_CC the_DT relationships_NNS between_IN them_PRP ._.
node_NN ,_, I-IDs_JJ differ_VBP from_IN IDs_NNS by_IN having_VBG a_DT dashed_VBN link_NN -LRB-_-LRB- called_VBN the_DT policy_NN link_NN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- -RRB-_-RRB- between_IN the_DT model_NN node_NN and_CC a_DT chance_NN node_NN ,_, Aj_NNP ,_, that_IN represents_VBZ the_DT distribution_NN over_IN the_DT other_JJ agent_NN ''_'' s_NNS actions_NNS given_VBN its_PRP$ model_NN ._.
In_IN the_DT absence_NN of_IN other_JJ agents_NNS ,_, the_DT model_NN node_NN and_CC the_DT chance_NN node_NN ,_, Aj_NNP ,_, vanish_VBP and_CC I-IDs_JJ collapse_NN into_IN traditional_JJ IDs_NNS ._.
The_DT model_NN node_NN contains_VBZ the_DT alternative_JJ computational_JJ models_NNS ascribed_VBN by_IN i_FW to_TO the_DT other_JJ agent_NN from_IN the_DT set_NN ,_, j_NN ,_, l1_NN SMj_NN ,_, where_WRB j_NN ,_, l1_NN and_CC SMj_NN were_VBD defined_VBN previously_RB in_IN Section_NN #_# ._.
Thus_RB ,_, a_DT model_NN in_IN the_DT model_NN node_NN may_MD itself_PRP be_VB an_DT I-ID_NN or_CC ID_NN ,_, and_CC the_DT recursion_NN terminates_VBZ when_WRB a_DT model_NN is_VBZ an_DT ID_NN or_CC subintentional_JJ ._.
Because_IN the_DT model_NN node_NN contains_VBZ the_DT alternative_JJ models_NNS of_IN the_DT other_JJ agent_NN as_IN its_PRP$ values_NNS ,_, its_PRP$ representation_NN is_VBZ not_RB trivial_JJ ._.
In_IN particular_JJ ,_, some_DT of_IN the_DT models_NNS within_IN the_DT node_NN are_VBP I-IDs_NNS that_WDT when_WRB solved_VBN generate_VBP the_DT agent_NN ''_'' s_NNS optimal_JJ policy_NN in_IN their_PRP$ decision_NN nodes_NNS ._.
Each_DT decision_NN node_NN is_VBZ mapped_VBN to_TO the_DT corresponding_JJ chance_NN node_NN ,_, say_VBP A1_NN j_NN ,_, in_IN the_DT following_JJ way_NN :_: if_IN OPT_NN is_VBZ the_DT set_NN of_IN optimal_JJ actions_NNS obtained_VBN by_IN solving_VBG the_DT I-ID_NN -LRB-_-LRB- or_CC ID_NN -RRB-_-RRB- ,_, then_RB Pr_NN -LRB-_-LRB- aj_NN A1_NN j_NN -RRB-_-RRB- =_JJ #_# |_CD OP_NN T_NN |_CD if_IN aj_NN OPT_NN ,_, #_# otherwise_RB ._.
Borrowing_NN insights_NNS from_IN previous_JJ work_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, we_PRP observe_VBP that_IN the_DT model_NN node_NN and_CC the_DT dashed_VBN policy_NN link_NN that_WDT connects_VBZ it_PRP to_TO the_DT chance_NN node_NN ,_, Aj_NNP ,_, could_MD be_VB represented_VBN as_IN shown_VBN in_IN Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- ._.
The_DT decision_NN node_NN of_IN each_DT level_NN l_NN #_# I-ID_NN is_VBZ transformed_VBN into_IN a_DT chance_NN node_NN ,_, as_IN we_PRP mentioned_VBD previously_RB ,_, so_IN that_IN the_DT actions_NNS with_IN the_DT largest_JJS value_NN in_IN the_DT decision_NN node_NN are_VBP assigned_VBN uniform_JJ probabilities_NNS in_IN the_DT chance_NN node_NN while_IN the_DT rest_NN are_VBP assigned_VBN zero_CD probability_NN ._.
The_DT different_JJ chance_NN nodes_NNS -LRB-_-LRB- A1_NN j_NN ,_, A2_NN j_NN -RRB-_-RRB- ,_, one_CD for_IN each_DT model_NN ,_, and_CC additionally_RB ,_, the_DT chance_NN node_NN labeled_VBN Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- form_VBP the_DT parents_NNS of_IN the_DT chance_NN node_NN ,_, Aj_NN ._.
Thus_RB ,_, there_EX are_VBP as_RB many_JJ action_NN nodes_NNS -LRB-_-LRB- A1_NN j_NN ,_, A2_NN j_NN -RRB-_-RRB- in_IN Mj_NNP ,_, l1_NN as_IN the_DT number_NN of_IN models_NNS in_IN the_DT support_NN of_IN agent_NN i_FW ''_'' s_VBZ beliefs_NNS ._.
The_DT conditional_JJ probability_NN table_NN of_IN the_DT chance_NN node_NN ,_, Aj_NNP ,_, is_VBZ a_DT multiplexer_NN that_WDT assumes_VBZ the_DT distribution_NN of_IN each_DT of_IN the_DT action_NN nodes_NNS -LRB-_-LRB- A1_NN j_NN ,_, A2_NN j_NN -RRB-_-RRB- depending_VBG on_IN the_DT value_NN of_IN Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- ._.
The_DT values_NNS of_IN Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- denote_VBP the_DT different_JJ models_NNS of_IN j_NN ._.
In_IN other_JJ words_NNS ,_, when_WRB Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- has_VBZ the_DT value_NN m1_NN j_NN ,_, l1_NN ,_, the_DT chance_NN node_NN Aj_NN assumes_VBZ the_DT distribution_NN of_IN the_DT node_NN A1_NN j_NN ,_, and_CC Aj_NNP assumes_VBZ the_DT distribution_NN of_IN A2_NN j_NN when_WRB Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- has_VBZ the_DT value_NN m2_NN j_NN ,_, l1_NN ._.
The_DT distribution_NN over_IN the_DT node_NN ,_, Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- ,_, is_VBZ the_DT agent_NN i_FW ''_'' s_VBZ belief_NN over_IN the_DT models_NNS of_IN j_NN given_VBN a_DT physical_JJ state_NN ._.
For_IN more_JJR agents_NNS ,_, we_PRP will_MD have_VB as_RB many_JJ model_NN nodes_NNS as_IN there_EX are_VBP agents_NNS ._.
Notice_NNP that_IN Fig_NNP ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- clarifies_VBZ the_DT semantics_NNS of_IN the_DT policy_NN link_NN ,_, and_CC shows_VBZ how_WRB it_PRP can_MD be_VB represented_VBN using_VBG the_DT traditional_JJ dependency_NN links_NNS ._.
In_IN Fig_NN ._.
#_# -LRB-_-LRB- c_NN -RRB-_-RRB- ,_, we_PRP show_VBP the_DT transformed_VBN I-ID_NN when_WRB the_DT model_NN node_NN is_VBZ replaced_VBN by_IN the_DT chance_NN nodes_NNS and_CC relationships_NNS between_IN them_PRP ._.
In_IN contrast_NN to_TO the_DT representation_NN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ,_, there_EX are_VBP no_DT special-purpose_JJ policy_NN links_NNS ,_, rather_RB the_DT I-ID_NN is_VBZ composed_VBN of_IN only_RB those_DT types_NNS of_IN nodes_NNS that_WDT are_VBP found_VBN in_IN traditional_JJ IDs_NNS and_CC dependency_NN relationships_NNS between_IN the_DT nodes_NNS ._.
This_DT allows_VBZ I-IDs_NNS to_TO be_VB represented_VBN and_CC implemented_VBN using_VBG conventional_JJ application_NN tools_NNS that_WDT target_VBP IDs_NNS ._.
Note_VB that_IN we_PRP may_MD view_VB the_DT level_NN l_NN I-ID_NN as_IN a_DT NID_NN ._.
Specifically_RB ,_, each_DT of_IN the_DT level_NN l_NN #_# models_NNS within_IN the_DT model_NN node_NN are_VBP blocks_VBZ in_IN the_DT NID_NN ._.
If_IN the_DT level_NN l_NN =_JJ #_# ,_, each_DT block_NN is_VBZ a_DT traditional_JJ ID_NNP ,_, otherwise_RB if_IN l_NN >_JJR #_# ,_, each_DT block_NN within_IN the_DT NID_NN may_MD itself_PRP be_VB a_DT NID_NN ._.
Note_VB that_DT within_IN the_DT I-IDs_NNS -LRB-_-LRB- or_CC IDs_NNS -RRB-_-RRB- at_IN each_DT level_NN ,_, there_EX is_VBZ only_RB a_DT single_JJ decision_NN node_NN ._.
Thus_RB ,_, our_PRP$ NID_NN does_VBZ not_RB contain_VB any_DT MAIDs_NNS ._.
Figure_NNP #_# :_: A_DT level_NN l_NN I-ID_NN represented_VBN as_IN a_DT NID_NN ._.
The_DT probabilities_NNS assigned_VBN to_TO the_DT blocks_NNS of_IN the_DT NID_NN are_VBP i_FW ''_'' s_VBZ beliefs_NNS over_IN j_NN ''_'' s_NNS models_NNS conditioned_VBN on_IN a_DT physical_JJ state_NN ._.
3_LS ._.
#_# Solution_NN The_DT solution_NN of_IN an_DT I-ID_JJ proceeds_NNS in_IN a_DT bottom-up_JJ manner_NN ,_, and_CC is_VBZ implemented_VBN recursively_RB ._.
We_PRP start_VBP by_IN solving_VBG the_DT level_NN #_# models_NNS ,_, which_WDT ,_, if_IN intentional_JJ ,_, are_VBP traditional_JJ IDs_NNS ._.
Their_PRP$ solutions_NNS provide_VBP probability_NN distributions_NNS over_IN the_DT other_JJ agents_NNS ''_'' actions_NNS ,_, which_WDT are_VBP entered_VBN in_IN the_DT corresponding_JJ chance_NN nodes_NNS found_VBN in_IN the_DT model_NN node_NN of_IN the_DT level_NN #_# I-ID_NN ._.
The_DT mapping_NN from_IN the_DT level_NN #_# models_NNS ''_'' decision_NN nodes_NNS to_TO the_DT chance_NN nodes_NNS is_VBZ carried_VBN out_RP so_RB that_IN actions_NNS with_IN the_DT largest_JJS value_NN in_IN the_DT decision_NN node_NN are_VBP assigned_VBN uniform_JJ probabilities_NNS in_IN the_DT chance_NN node_NN while_IN the_DT rest_NN are_VBP assigned_VBN zero_CD probability_NN ._.
Given_VBN the_DT distributions_NNS over_IN the_DT actions_NNS within_IN the_DT different_JJ chance_NN nodes_NNS -LRB-_-LRB- one_CD for_IN each_DT model_NN of_IN the_DT other_JJ agent_NN -RRB-_-RRB- ,_, the_DT level_NN #_# I-ID_NN is_VBZ transformed_VBN as_IN shown_VBN in_IN Fig_NN ._.
#_# -LRB-_-LRB- c_NN -RRB-_-RRB- ._.
During_IN the_DT transformation_NN ,_, the_DT conditional_JJ probability_NN table_NN -LRB-_-LRB- CPT_NN -RRB-_-RRB- of_IN the_DT node_NN ,_, Aj_NNP ,_, is_VBZ populated_JJ such_JJ that_IN the_DT node_NN assumes_VBZ the_DT distribution_NN of_IN each_DT of_IN the_DT chance_NN nodes_NNS depending_VBG on_IN the_DT value_NN of_IN the_DT node_NN ,_, Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- ._.
As_IN we_PRP mentioned_VBD previously_RB ,_, the_DT values_NNS of_IN the_DT node_NN Mod_NN -LSB-_-LRB- Mj_NN -RSB-_-RRB- denote_VBP the_DT different_JJ models_NNS of_IN the_DT other_JJ agent_NN ,_, and_CC its_PRP$ distribution_NN is_VBZ the_DT agent_NN i_FW ''_'' s_VBZ belief_NN over_IN the_DT models_NNS of_IN j_NN conditioned_VBN on_IN the_DT physical_JJ state_NN ._.
The_DT transformed_VBN level_NN #_# I-ID_NN is_VBZ a_DT traditional_JJ ID_NN that_WDT may_MD be_VB solved_VBN us816_JJ The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- -LRB-_-LRB- a_DT -RRB-_-RRB- -LRB-_-LRB- b_NN -RRB-_-RRB- Figure_NN #_# :_: -LRB-_-LRB- a_LS -RRB-_-RRB- A_DT generic_JJ two_CD time-slice_JJ level_NN l_NN I-DID_NN for_IN agent_NN i_FW in_IN a_DT setting_NN with_IN one_CD other_JJ agent_NN j_NN ._.
Notice_NNP the_DT dotted_VBN model_NN update_VBP link_NN that_WDT denotes_VBZ the_DT update_VB of_IN the_DT models_NNS of_IN j_NN and_CC the_DT distribution_NN over_IN the_DT models_NNS over_IN time_NN ._.
-LRB-_-LRB- b_NN -RRB-_-RRB- The_DT semantics_NNS of_IN the_DT model_NN update_VBP link_NN ._.
ing_VBG the_DT standard_NN expected_VBN utility_NN maximization_NN method_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ._.
This_DT procedure_NN is_VBZ carried_VBN out_RP up_IN to_TO the_DT level_NN l_NN I-ID_NN whose_WP$ solution_NN gives_VBZ the_DT non-empty_JJ set_NN of_IN optimal_JJ actions_NNS that_IN the_DT agent_NN should_MD perform_VB given_VBN its_PRP$ belief_NN ._.
Notice_NNP that_WDT analogous_JJ to_TO IDs_NNS ,_, I-IDs_NNS are_VBP suitable_JJ for_IN online_JJ decision-making_NN when_WRB the_DT agent_NN ''_'' s_NNS current_JJ belief_NN is_VBZ known_VBN ._.
4_LS ._.
INTERACTIVE_JJ DYNAMIC_NNP INFLUENCE_NNP DIAGRAMS_NNP Interactive_JJ dynamic_JJ influence_NN diagrams_NNS -LRB-_-LRB- I-DIDs_NNS -RRB-_-RRB- extend_VBP I-IDs_NNS -LRB-_-LRB- and_CC NIDs_NNS -RRB-_-RRB- to_TO allow_VB sequential_JJ decision-making_NN over_IN several_JJ time_NN steps_NNS ._.
Just_RB as_IN DIDs_NNS are_VBP structured_VBN graphical_JJ representations_NNS of_IN POMDPs_NNS ,_, I-DIDs_NNS are_VBP the_DT graphical_JJ online_NN analogs_NNS for_IN finitely_RB nested_JJ I-POMDPs_NNS ._.
I-DIDs_NNS may_MD be_VB used_VBN to_TO optimize_VB over_IN a_DT finite_JJ look-ahead_JJ given_VBN initial_JJ beliefs_NNS while_IN interacting_VBG with_IN other_JJ ,_, possibly_RB similar_JJ ,_, agents_NNS ._.
4_LS ._.
#_# Syntax_FW We_PRP depict_VBP a_DT general_JJ two_CD time-slice_JJ I-DID_NN in_IN Fig_NN ._.
#_# -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
In_IN addition_NN to_TO the_DT model_NN nodes_NNS and_CC the_DT dashed_VBN policy_NN link_NN ,_, what_WDT differentiates_VBZ an_DT I-DID_NN from_IN a_DT DID_NN is_VBZ the_DT model_NN update_VBP link_NN shown_VBN as_IN a_DT dotted_VBN arrow_NN in_IN Fig_NN ._.
#_# -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
We_PRP explained_VBD the_DT semantics_NNS of_IN the_DT model_NN node_NN and_CC the_DT policy_NN link_NN in_IN the_DT previous_JJ section_NN ;_: we_PRP describe_VBP the_DT model_NN updates_NNS next_JJ ._.
The_DT update_VBP of_IN the_DT model_NN node_NN over_IN time_NN involves_VBZ two_CD steps_NNS :_: First_NNP ,_, given_VBN the_DT models_NNS at_IN time_NN t_NN ,_, we_PRP identify_VBP the_DT updated_VBN set_NN of_IN models_NNS that_WDT reside_VBP in_IN the_DT model_NN node_NN at_IN time_NN t_NN +_CC #_# ._.
Recall_VB from_IN Section_NN #_# that_IN an_DT agent_NN ''_'' s_NNS intentional_JJ model_NN includes_VBZ its_PRP$ belief_NN ._.
Because_IN the_DT agents_NNS act_VBP and_CC receive_VBP observations_NNS ,_, their_PRP$ models_NNS are_VBP updated_VBN to_TO reflect_VB their_PRP$ changed_VBN beliefs_NNS ._.
Since_IN the_DT set_NN of_IN optimal_JJ actions_NNS for_IN a_DT model_NN could_MD include_VB all_PDT the_DT actions_NNS ,_, and_CC the_DT agent_NN may_MD receive_VB any_DT one_CD of_IN |_CD j_NN |_CD possible_JJ observations_NNS ,_, the_DT updated_VBN set_NN at_IN time_NN step_NN t_NN +_CC #_# will_MD have_VB at_IN most_JJS |_CD Mt_NN j_NN ,_, l1_NN |_CD |_CD Aj_NNP |_CD |_CD j_NN |_CD models_NNS ._.
Here_RB ,_, |_CD Mt_NN j_NN ,_, l1_NN |_NN is_VBZ the_DT number_NN of_IN models_NNS at_IN time_NN step_NN t_NN ,_, |_CD Aj_NNP |_CD and_CC |_CD j_NN |_NNS are_VBP the_DT largest_JJS spaces_NNS of_IN actions_NNS and_CC observations_NNS respectively_RB ,_, among_IN all_PDT the_DT models_NNS ._.
Second_RB ,_, we_PRP compute_VBP the_DT new_JJ distribution_NN over_IN the_DT updated_VBN models_NNS given_VBN the_DT original_JJ distribution_NN and_CC the_DT probability_NN of_IN the_DT agent_NN performing_VBG the_DT action_NN and_CC receiving_VBG the_DT observation_NN that_WDT led_VBD to_TO the_DT updated_VBN model_NN ._.
These_DT steps_NNS are_VBP a_DT part_NN of_IN agent_NN i_FW ''_'' s_VBZ belief_NN update_VB formalized_VBN using_VBG Eq_NN ._.
#_# ._.
In_IN Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- ,_, we_PRP show_VBP how_WRB the_DT dotted_VBN model_NN update_VBP link_NN is_VBZ implemented_VBN in_IN the_DT I-DID_NN ._.
If_IN each_DT of_IN the_DT two_CD level_NN l_NN #_# models_NNS ascribed_VBN to_TO j_VB at_IN time_NN step_NN t_NN results_VBZ in_IN one_CD action_NN ,_, and_CC j_NN could_MD make_VB one_CD of_IN two_CD possible_JJ observations_NNS ,_, then_RB the_DT model_NN node_NN at_IN time_NN step_NN t_NN +_CC #_# contains_VBZ four_CD updated_VBN models_NNS -LRB-_-LRB- mt_NN +_CC #_# ,_, #_# j_FW ,_, l1_NN ,_, mt_NN +_CC #_# ,_, #_# j_FW ,_, l1_NN ,_, mt_NN +_CC #_# ,_, #_# j_FW ,_, l1_NN ,_, and_CC mt_NN +_CC #_# ,_, #_# j_FW ,_, l1_NN -RRB-_-RRB- ._.
These_DT models_NNS differ_VBP in_IN their_PRP$ initial_JJ beliefs_NNS ,_, each_DT of_IN which_WDT is_VBZ the_DT result_NN of_IN j_NN updating_VBG its_PRP$ beliefs_NNS due_JJ to_TO its_PRP$ action_NN and_CC a_DT possible_JJ observation_NN ._.
The_DT decision_NN nodes_NNS in_IN each_DT of_IN the_DT I-DIDs_NNS or_CC DIDs_NNS that_WDT represent_VBP the_DT lower_JJR level_NN models_NNS are_VBP mapped_VBN to_TO the_DT corresponding_JJ Figure_NN #_# :_: Transformed_VBN I-DID_NN with_IN the_DT model_NN nodes_NNS and_CC model_NN update_VBP link_NN replaced_VBN with_IN the_DT chance_NN nodes_NNS and_CC the_DT relationships_NNS -LRB-_-LRB- in_IN bold_JJ -RRB-_-RRB- ._.
chance_NN nodes_NNS ,_, as_IN mentioned_VBN previously_RB ._.
Next_RB ,_, we_PRP describe_VBP how_WRB the_DT distribution_NN over_IN the_DT updated_VBN set_NN of_IN models_NNS -LRB-_-LRB- the_DT distribution_NN over_IN the_DT chance_NN node_NN Mod_NN -LSB-_-LRB- Mt_NN +_CC #_# j_NN -RSB-_-RRB- in_IN Mt_NN +_CC #_# j_FW ,_, l1_NN -RRB-_-RRB- is_VBZ computed_VBN ._.
The_DT probability_NN that_IN j_NN ''_'' s_NNS updated_VBN model_NN is_VBZ ,_, say_VBP mt_NN +_CC #_# ,_, #_# j_FW ,_, l1_NN ,_, depends_VBZ on_IN the_DT probability_NN of_IN j_NN performing_VBG the_DT action_NN and_CC receiving_VBG the_DT observation_NN that_WDT led_VBD to_TO this_DT model_NN ,_, and_CC the_DT prior_JJ distribution_NN over_IN the_DT models_NNS at_IN time_NN step_NN t_NN ._.
Because_IN the_DT chance_NN node_NN At_IN j_NN assumes_VBZ the_DT distribution_NN of_IN each_DT of_IN the_DT action_NN nodes_NNS based_VBN on_IN the_DT value_NN of_IN Mod_NN -LSB-_-LRB- Mt_NN j_NN -RSB-_-RRB- ,_, the_DT probability_NN of_IN the_DT action_NN is_VBZ given_VBN by_IN this_DT chance_NN node_NN ._.
In_IN order_NN to_TO obtain_VB the_DT probability_NN of_IN j_NN ''_'' s_NNS possible_JJ observation_NN ,_, we_PRP introduce_VBP the_DT chance_NN node_NN Oj_NN ,_, which_WDT depending_VBG on_IN the_DT value_NN of_IN Mod_NN -LSB-_-LRB- Mt_NN j_NN -RSB-_-RRB- assumes_VBZ the_DT distribution_NN of_IN the_DT observation_NN node_NN in_IN the_DT lower_JJR level_NN model_NN denoted_VBN by_IN Mod_NN -LSB-_-LRB- Mt_NN j_NN -RSB-_-RRB- ._.
Because_IN the_DT probability_NN of_IN j_NN ''_'' s_NNS observations_NNS depends_VBZ on_IN the_DT physical_JJ state_NN and_CC the_DT joint_JJ actions_NNS of_IN both_DT agents_NNS ,_, the_DT node_NN Oj_NN is_VBZ linked_VBN with_IN St_NNP +_CC #_# ,_, At_IN j_NN ,_, and_CC At_IN i_FW ._.
#_# Analogous_JJ to_TO At_IN j_NN ,_, the_DT conditional_JJ probability_NN table_NN of_IN Oj_NN is_VBZ also_RB a_DT multiplexer_NN modulated_VBN by_IN Mod_NN -LSB-_-LRB- Mt_NN j_NN -RSB-_-RRB- ._.
Finally_RB ,_, the_DT distribution_NN over_IN the_DT prior_JJ models_NNS at_IN time_NN t_NN is_VBZ obtained_VBN from_IN the_DT chance_NN node_NN ,_, Mod_NN -LSB-_-LRB- Mt_NN j_NN -RSB-_-RRB- in_IN Mt_NN j_NN ,_, l1_NN ._.
Consequently_RB ,_, the_DT chance_NN nodes_NNS ,_, Mod_NN -LSB-_-LRB- Mt_NN j_NN -RSB-_-RRB- ,_, At_IN j_NN ,_, and_CC Oj_NN ,_, form_VBP the_DT parents_NNS of_IN Mod_NN -LSB-_-LRB- Mt_NN +_CC #_# j_NN -RSB-_-RRB- in_IN Mt_NN +_CC #_# j_FW ,_, l1_NN ._.
Notice_NNP that_IN the_DT model_NN update_VBP link_NN may_MD be_VB replaced_VBN by_IN the_DT dependency_NN links_NNS between_IN the_DT chance_NN nodes_NNS that_WDT constitute_VBP the_DT model_NN nodes_NNS in_IN the_DT two_CD time_NN slices_NNS ._.
In_IN Fig_NN ._.
#_# we_PRP show_VBP the_DT two_CD time-slice_JJ I-DID_NN with_IN the_DT model_NN nodes_NNS replaced_VBN by_IN the_DT chance_NN nodes_NNS and_CC the_DT relationships_NNS between_IN them_PRP ._.
Chance_NN nodes_NNS and_CC dependency_NN links_NNS that_WDT not_RB in_IN bold_JJ are_VBP standard_JJ ,_, usually_RB found_VBN in_IN DIDs_NNS ._.
Expansion_NN of_IN the_DT I-DID_NN over_IN more_JJR time_NN steps_NNS requires_VBZ the_DT repetition_NN of_IN the_DT two_CD steps_NNS of_IN updating_VBG the_DT set_NN of_IN models_NNS that_WDT form_VBP the_DT 2_CD Note_NN that_WDT Oj_NN represents_VBZ j_NN ''_'' s_NNS observation_NN at_IN time_NN t_NN +_CC #_# ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD values_NNS of_IN the_DT model_NN node_NN and_CC adding_VBG the_DT relationships_NNS between_IN the_DT chance_NN nodes_NNS ,_, as_IN many_JJ times_NNS as_IN there_EX are_VBP model_JJ update_VBP links_NNS ._.
We_PRP note_VBP that_IN the_DT possible_JJ set_NN of_IN models_NNS of_IN the_DT other_JJ agent_NN j_NN grows_VBZ exponentially_RB with_IN the_DT number_NN of_IN time_NN steps_NNS ._.
For_IN example_NN ,_, after_IN T_NN steps_NNS ,_, there_EX may_MD be_VB at_IN most_JJS |_NNS Mt_NN =_JJ #_# j_FW ,_, l1_NN |_NN -LRB-_-LRB- |_CD Aj_NNP |_CD |_CD j_NN |_NN -RRB-_-RRB- T_NN #_# candidate_NN models_NNS residing_VBG in_IN the_DT model_NN node_NN ._.
4_LS ._.
#_# Solution_NN Analogous_JJ to_TO I-IDs_NNS ,_, the_DT solution_NN to_TO a_DT level_NN l_NN I-DID_NN for_IN agent_NN i_FW expanded_VBN over_IN T_NN time_NN steps_NNS may_MD be_VB carried_VBN out_RP recursively_RB ._.
For_IN the_DT purpose_NN of_IN illustration_NN ,_, let_VB l_NN =_JJ #_# and_CC T_NN =_JJ #_# ._.
The_DT solution_NN method_NN uses_VBZ the_DT standard_JJ look-ahead_JJ technique_NN ,_, projecting_VBG the_DT agent_NN ''_'' s_NNS action_NN and_CC observation_NN sequences_NNS forward_RB from_IN the_DT current_JJ belief_NN state_NN -LSB-_-LRB- ##_CD -RSB-_-RRB- ,_, and_CC finding_VBG the_DT possible_JJ beliefs_NNS that_WDT i_FW could_MD have_VB in_IN the_DT next_JJ time_NN step_NN ._.
Because_IN agent_NN i_FW has_VBZ a_DT belief_NN over_IN j_NN ''_'' s_NNS models_NNS as_RB well_RB ,_, the_DT lookahead_NN includes_VBZ finding_VBG out_RP the_DT possible_JJ models_NNS that_WDT j_VBP could_MD have_VB in_IN the_DT future_NN ._.
Consequently_RB ,_, each_DT of_IN j_NN ''_'' s_NNS subintentional_JJ or_CC level_NN #_# models_NNS -LRB-_-LRB- represented_VBN using_VBG a_DT standard_JJ DID_NN -RRB-_-RRB- in_IN the_DT first_JJ time_NN step_NN must_MD be_VB solved_VBN to_TO obtain_VB its_PRP$ optimal_JJ set_NN of_IN actions_NNS ._.
These_DT actions_NNS are_VBP combined_VBN with_IN the_DT set_NN of_IN possible_JJ observations_NNS that_WDT j_VBP could_MD make_VB in_IN that_DT model_NN ,_, resulting_VBG in_IN an_DT updated_VBN set_NN of_IN candidate_NN models_NNS -LRB-_-LRB- that_WDT include_VBP the_DT updated_VBN beliefs_NNS -RRB-_-RRB- that_WDT could_MD describe_VB the_DT behavior_NN of_IN j_NN ._.
Beliefs_NNS over_IN this_DT updated_VBN set_NN of_IN candidate_NN models_NNS are_VBP calculated_VBN using_VBG the_DT standard_JJ inference_NN methods_NNS using_VBG the_DT dependency_NN relationships_NNS between_IN the_DT model_NN nodes_NNS as_IN shown_VBN in_IN Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- ._.
We_PRP note_VBP the_DT recursive_JJ nature_NN of_IN this_DT solution_NN :_: in_IN solving_VBG agent_NN i_FW ''_'' s_VBZ level_NN #_# I-DID_NN ,_, j_NN ''_'' s_NNS level_NN #_# DIDs_NNS must_MD be_VB solved_VBN ._.
If_IN the_DT nesting_JJ of_IN models_NNS is_VBZ deeper_JJR ,_, all_DT models_NNS at_IN all_DT levels_NNS starting_VBG from_IN #_# are_VBP solved_VBN in_IN a_DT bottom-up_JJ manner_NN ._.
We_PRP briefly_RB outline_VBP the_DT recursive_JJ algorithm_NN for_IN solving_VBG agent_NN i_FW ''_'' s_VBZ Algorithm_NNP for_IN solving_VBG I-DID_NNP Input_NNP :_: level_NN l_NN #_# I-ID_NN or_CC level_NN #_# ID_NNP ,_, T_NN Expansion_NN Phase_NN 1_CD ._.
For_IN t_NN from_IN #_# to_TO T_NN #_# do_VBP 2_CD ._.
If_IN l_NN #_# then_RB Populate_VB Mt_NN +_CC #_# j_FW ,_, l1_NN 3_CD ._.
For_IN each_DT mt_NN j_NN in_IN Range_NNP -LRB-_-LRB- Mt_NNP j_NN ,_, l1_NN -RRB-_-RRB- do_VBP 4_CD ._.
Recursively_RB call_VB algorithm_NN with_IN the_DT l_NN #_# I-ID_NN -LRB-_-LRB- or_CC ID_NN -RRB-_-RRB- that_WDT represents_VBZ mt_NN j_NN and_CC the_DT horizon_NN ,_, T_NN t_NN +_CC #_# 5_CD ._.
Map_NN the_DT decision_NN node_NN of_IN the_DT solved_VBN I-ID_NN -LRB-_-LRB- or_CC ID_NN -RRB-_-RRB- ,_, OPT_NN -LRB-_-LRB- mt_NN j_NN -RRB-_-RRB- ,_, to_TO a_DT chance_NN node_NN Aj_NN 6_CD ._.
For_IN each_DT aj_NN in_IN OPT_NN -LRB-_-LRB- mt_NN j_NN -RRB-_-RRB- do_VBP 7_CD ._.
For_IN each_DT oj_NN in_IN Oj_NN -LRB-_-LRB- part_NN of_IN mt_NN j_NN -RRB-_-RRB- do_VBP 8_CD ._.
Update_NNP j_NN ''_'' s_NNS belief_NN ,_, bt_NN +_CC #_# j_FW SE_NN -LRB-_-LRB- bt_NN j_NN ,_, aj_NN ,_, oj_NN -RRB-_-RRB- 9_CD ._.
mt_NN +_CC #_# j_FW New_JJ I-ID_NN -LRB-_-LRB- or_CC ID_NN -RRB-_-RRB- with_IN bt_NN +_CC #_# j_FW as_IN the_DT initial_JJ belief_NN 10_CD ._.
Range_NNP -LRB-_-LRB- Mt_NNP +_CC #_# j_FW ,_, l1_NN -RRB-_-RRB- -LCB-_-LRB- mt_NN +_CC #_# j_FW -RCB-_-RRB- 11_CD ._.
Add_VB the_DT model_NN node_NN ,_, Mt_NN +_CC #_# j_FW ,_, l1_NN ,_, and_CC the_DT dependency_NN links_NNS between_IN Mt_NN j_NN ,_, l1_NN and_CC Mt_NN +_CC #_# j_FW ,_, l1_NN -LRB-_-LRB- shown_VBN in_IN Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- -RRB-_-RRB- 12_CD ._.
Add_VB the_DT chance_NN ,_, decision_NN ,_, and_CC utility_NN nodes_NNS for_IN t_NN +_CC #_# time_NN slice_NN and_CC the_DT dependency_NN links_NNS between_IN them_PRP 13_CD ._.
Establish_VB the_DT CPTs_NNS for_IN each_DT chance_NN node_NN and_CC utility_NN node_NN Look-Ahead_NNP Phase_NN 14_CD ._.
Apply_VB the_DT standard_JJ look-ahead_JJ and_CC backup_NN method_NN to_TO solve_VB the_DT expanded_VBN I-DID_NN Figure_NN #_# :_: Algorithm_NN for_IN solving_VBG a_DT level_NN l_NN #_# I-DID_NN ._.
level_NN l_NN I-DID_NN expanded_VBN over_IN T_NN time_NN steps_NNS with_IN one_CD other_JJ agent_NN j_NN in_IN Fig_NN ._.
#_# ._.
We_PRP adopt_VBP a_DT two-phase_JJ approach_NN :_: Given_VBN an_DT I-ID_NN of_IN level_NN l_NN -LRB-_-LRB- described_VBN previously_RB in_IN Section_NN #_# -RRB-_-RRB- with_IN all_DT lower_JJR level_NN models_NNS also_RB represented_VBD as_IN I-IDs_NNS or_CC IDs_NNS -LRB-_-LRB- if_IN level_NN #_# -RRB-_-RRB- ,_, the_DT first_JJ step_NN is_VBZ to_TO expand_VB the_DT level_NN l_NN I-ID_NN over_IN T_NN time_NN steps_NNS adding_VBG the_DT dependency_NN links_NNS and_CC the_DT conditional_JJ probability_NN tables_NNS for_IN each_DT node_NN ._.
We_PRP particularly_RB focus_VBP on_IN establishing_VBG and_CC populating_VBG the_DT model_NN nodes_NNS -LRB-_-LRB- lines_NNS 3-11_CD -RRB-_-RRB- ._.
Note_VB that_DT Range_NNP -LRB-_-LRB- -RRB-_-RRB- returns_VBZ the_DT values_NNS -LRB-_-LRB- lower_JJR level_NN models_NNS -RRB-_-RRB- of_IN the_DT random_JJ variable_JJ given_VBN as_IN input_NN -LRB-_-LRB- model_NN node_NN -RRB-_-RRB- ._.
In_IN the_DT second_JJ phase_NN ,_, we_PRP use_VBP a_DT standard_JJ look-ahead_JJ technique_NN projecting_VBG the_DT action_NN and_CC observation_NN sequences_NNS over_IN T_NN time_NN steps_NNS in_IN the_DT future_NN ,_, and_CC backing_VBG up_RP the_DT utility_NN values_NNS of_IN the_DT reachable_JJ beliefs_NNS ._.
Similar_JJ to_TO I-IDs_NNS ,_, the_DT I-DIDs_JJ reduce_VB to_TO DIDs_NNS in_IN the_DT absence_NN of_IN other_JJ agents_NNS ._.
As_IN we_PRP mentioned_VBD previously_RB ,_, the_DT 0-th_JJ level_NN models_NNS are_VBP the_DT traditional_JJ DIDs_NNS ._.
Their_PRP$ solutions_NNS provide_VBP probability_NN distributions_NNS over_IN actions_NNS of_IN the_DT agent_NN modeled_VBD at_IN that_DT level_NN to_TO I-DIDs_NNS at_IN level_NN #_# ._.
Given_VBN probability_NN distributions_NNS over_IN other_JJ agent_NN ''_'' s_NNS actions_NNS the_DT level_NN #_# IDIDs_NNS can_MD themselves_PRP be_VB solved_VBN as_IN DIDs_NNS ,_, and_CC provide_VBP probability_NN distributions_NNS to_TO yet_RB higher_JJR level_NN models_NNS ._.
Assume_VB that_IN the_DT number_NN of_IN models_NNS considered_VBN at_IN each_DT level_NN is_VBZ bound_VBN by_IN a_DT number_NN ,_, M_NN ._.
Solving_VBG an_DT I-DID_NN of_IN level_NN l_NN in_IN then_RB equivalent_JJ to_TO solving_VBG O_NN -LRB-_-LRB- Ml_NN -RRB-_-RRB- DIDs_NNS ._.
5_CD ._.
EXAMPLE_NN APPLICATIONS_NNS To_TO illustrate_VB the_DT usefulness_NN of_IN I-DIDs_NNS ,_, we_PRP apply_VBP them_PRP to_TO three_CD problem_NN domains_NNS ._.
We_PRP describe_VBP ,_, in_IN particular_JJ ,_, the_DT formulation_NN of_IN the_DT I-DID_NN and_CC the_DT optimal_JJ prescriptions_NNS obtained_VBN on_IN solving_VBG it_PRP ._.
5_CD ._.
#_# Followership-Leadership_NN in_IN the_DT Multiagent_JJ Tiger_NNP Problem_NNP We_PRP begin_VBP our_PRP$ illustrations_NNS of_IN using_VBG I-IDs_NNS and_CC I-DIDs_NNS with_IN a_DT slightly_RB modified_VBN version_NN of_IN the_DT multiagent_JJ tiger_NN problem_NN discussed_VBN in_IN -LSB-_-LRB- #_# -RSB-_-RRB- ._.
The_DT problem_NN has_VBZ two_CD agents_NNS ,_, each_DT of_IN which_WDT can_MD open_VB the_DT right_JJ door_NN -LRB-_-LRB- OR_NN -RRB-_-RRB- ,_, the_DT left_JJ door_NN -LRB-_-LRB- OL_NN -RRB-_-RRB- or_CC listen_VB -LRB-_-LRB- L_NN -RRB-_-RRB- ._.
In_IN addition_NN to_TO hearing_VBG growls_VBZ -LRB-_-LRB- from_IN the_DT left_NN -LRB-_-LRB- GL_NN -RRB-_-RRB- or_CC from_IN the_DT right_NN -LRB-_-LRB- GR_NN -RRB-_-RRB- -RRB-_-RRB- when_WRB they_PRP listen_VBP ,_, the_DT agents_NNS also_RB hear_VBP creaks_NNS -LRB-_-LRB- from_IN the_DT left_NN -LRB-_-LRB- CL_NN -RRB-_-RRB- ,_, from_IN the_DT right_NN -LRB-_-LRB- CR_NN -RRB-_-RRB- ,_, or_CC no_DT creaks_NNS -LRB-_-LRB- S_NN -RRB-_-RRB- -RRB-_-RRB- ,_, which_WDT noisily_RB indicate_VBP the_DT other_JJ agent_NN ''_'' s_NNS opening_VBG one_CD of_IN the_DT doors_NNS ._.
When_WRB any_DT door_NN is_VBZ opened_VBN ,_, the_DT tiger_NN persists_VBZ in_IN its_PRP$ original_JJ location_NN with_IN a_DT probability_NN of_IN ##_CD %_NN ._.
Agent_NNP i_FW hears_VBZ growls_VBZ with_IN a_DT reliability_NN of_IN ##_CD %_NN and_CC creaks_VBZ with_IN a_DT reliability_NN of_IN ##_CD %_NN ._.
Agent_NNP j_NN ,_, on_IN the_DT other_JJ hand_NN ,_, hears_VBZ growls_VBZ with_IN a_DT reliability_NN of_IN ##_CD %_NN ._.
Thus_RB ,_, the_DT setting_NN is_VBZ such_JJ that_IN agent_NN i_FW hears_VBZ agent_NN j_NN opening_VBG doors_NNS more_RBR reliably_RB than_IN the_DT tiger_NN ''_'' s_NNS growls_VBZ ._.
This_DT suggests_VBZ that_IN i_FW could_MD use_VB j_NN ''_'' s_NNS actions_NNS as_IN an_DT indication_NN of_IN the_DT location_NN of_IN the_DT tiger_NN ,_, as_IN we_PRP discuss_VBP below_IN ._.
Each_DT agent_NN ''_'' s_NNS preferences_NNS are_VBP as_IN in_IN the_DT single_JJ agent_NN game_NN discussed_VBN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ._.
The_DT transition_NN ,_, observation_NN ,_, and_CC reward_NN functions_NNS are_VBP shown_VBN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ._.
A_DT good_JJ indicator_NN of_IN the_DT usefulness_NN of_IN normative_JJ methods_NNS for_IN decision-making_NN like_IN I-DIDs_NNS is_VBZ the_DT emergence_NN of_IN realistic_JJ social_JJ behaviors_NNS in_IN their_PRP$ prescriptions_NNS ._.
In_IN settings_NNS of_IN the_DT persistent_JJ multiagent_JJ tiger_NN problem_NN that_WDT reflect_VBP real_JJ world_NN situations_NNS ,_, we_PRP demonstrate_VBP followership_NN between_IN the_DT agents_NNS and_CC ,_, as_IN shown_VBN in_IN -LSB-_-LRB- ##_NN -RSB-_-RRB- ,_, deception_NN among_IN agents_NNS who_WP believe_VBP that_IN they_PRP are_VBP in_IN a_DT follower-leader_JJ type_NN of_IN relationship_NN ._.
In_IN particular_JJ ,_, we_PRP analyze_VBP the_DT situational_JJ and_CC epistemological_JJ conditions_NNS sufficient_JJ for_IN their_PRP$ emergence_NN ._.
The_DT followership_NN behavior_NN ,_, for_IN example_NN ,_, results_VBZ from_IN the_DT agent_NN knowing_VBG its_PRP$ own_JJ weaknesses_NNS ,_, assessing_VBG the_DT strengths_NNS ,_, preferences_NNS ,_, and_CC possible_JJ behaviors_NNS of_IN the_DT other_JJ ,_, and_CC realizing_VBG that_IN its_PRP$ best_JJS for_IN it_PRP to_TO follow_VB the_DT other_JJ ''_'' s_NNS actions_NNS in_IN order_NN to_TO maximize_VB its_PRP$ payoffs_NNS ._.
Let_VB us_PRP consider_VB a_DT particular_JJ setting_NN of_IN the_DT tiger_NN problem_NN in_IN which_WDT agent_NN i_FW believes_VBZ that_IN j_NN ''_'' s_NNS preferences_NNS are_VBP aligned_VBN with_IN its_PRP$ own_JJ -_: both_DT of_IN them_PRP just_RB want_VBP to_TO get_VB the_DT gold_NN -_: and_CC j_NN ''_'' s_NNS hearing_NN is_VBZ more_RBR reliable_JJ in_IN comparison_NN to_TO itself_PRP ._.
As_IN an_DT example_NN ,_, suppose_VBP that_IN j_NN ,_, on_IN listening_VBG can_MD discern_VB the_DT tiger_NN ''_'' s_NNS location_NN ##_CD %_NN of_IN the_DT times_NNS compared_VBN to_TO i_FW ''_'' s_VBZ 65_CD %_NN accuracy_NN ._.
Additionally_RB ,_, agent_NN i_FW does_VBZ not_RB have_VB any_DT initial_JJ information_NN about_IN the_DT tiger_NN ''_'' s_NNS location_NN ._.
In_IN other_JJ words_NNS ,_, i_FW ''_'' s_VBZ single-level_JJ nested_JJ belief_NN ,_, bi_NN ,_, #_# ,_, assigns_VBZ #_# ._.
#_# to_TO each_DT of_IN the_DT two_CD locations_NNS of_IN the_DT tiger_NN ._.
In_IN addition_NN ,_, i_FW considers_VBZ two_CD models_NNS of_IN j_NN ,_, which_WDT differ_VBP in_IN j_NN ''_'' s_NNS flat_JJ level_NN #_# initial_JJ beliefs_NNS ._.
This_DT is_VBZ represented_VBN in_IN the_DT level_NN #_# I-ID_NN shown_VBN in_IN Fig_NN ._.
#_# -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
According_VBG to_TO one_CD model_NN ,_, j_NN assigns_VBZ a_DT probability_NN of_IN #_# ._.
#_# that_IN the_DT tiger_NN is_VBZ behind_IN the_DT left_JJ door_NN ,_, while_IN the_DT other_JJ 818_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- Figure_NNP #_# :_: -LRB-_-LRB- a_LS -RRB-_-RRB- Level_NN #_# I-ID_NN of_IN agent_NN i_FW ,_, -LRB-_-LRB- b_LS -RRB-_-RRB- two_CD level_NN #_# IDs_NNS of_IN agent_NN j_NN whose_WP$ decision_NN nodes_NNS are_VBP mapped_VBN to_TO the_DT chance_NN nodes_NNS ,_, A1_NN j_NN ,_, A2_NN j_NN ,_, in_IN -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
model_NN assigns_VBZ #_# ._.
#_# to_TO that_DT location_NN -RRB-_-RRB- ._.
Agent_NNP i_FW is_VBZ undecided_JJ on_IN these_DT two_CD models_NNS of_IN j_NN ._.
If_IN we_PRP vary_VBP i_FW ''_'' s_VBZ hearing_NN ability_NN ,_, and_CC solve_VB the_DT corresponding_JJ level_NN #_# I-ID_NN expanded_VBN over_IN three_CD time_NN steps_NNS ,_, we_PRP obtain_VBP the_DT normative_JJ behavioral_JJ policies_NNS shown_VBN in_IN Fig_NN #_# that_WDT exhibit_VBP followership_NN behavior_NN ._.
If_IN i_FW ''_'' s_VBZ probability_NN of_IN correctly_RB hearing_VBG the_DT growls_VBZ is_VBZ #_# ._.
##_NN ,_, then_RB as_IN shown_VBN in_IN the_DT policy_NN in_IN Fig_NN ._.
#_# -LRB-_-LRB- a_DT -RRB-_-RRB- ,_, i_FW begins_VBZ to_TO conditionally_RB follow_VB j_NN ''_'' s_NNS actions_NNS :_: i_FW opens_VBZ the_DT same_JJ door_NN that_WDT j_VBD opened_VBN previously_RB iff_FW i_FW ''_'' s_VBZ own_JJ assessment_NN of_IN the_DT tiger_NN ''_'' s_NNS location_NN confirms_VBZ j_NN ''_'' s_NNS pick_VBP ._.
If_IN i_FW loses_VBZ the_DT ability_NN to_TO correctly_RB interpret_VB the_DT growls_VBZ completely_RB ,_, it_PRP blindly_RB follows_VBZ j_NN and_CC opens_VBZ the_DT same_JJ door_NN that_WDT j_VBD opened_VBN previously_RB -LRB-_-LRB- Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Figure_NNP #_# :_: Emergence_NN of_IN -LRB-_-LRB- a_DT -RRB-_-RRB- conditional_JJ followership_NN ,_, and_CC -LRB-_-LRB- b_LS -RRB-_-RRB- blind_JJ followership_NN in_IN the_DT tiger_NN problem_NN ._.
Behaviors_NNS of_IN interest_NN are_VBP in_IN bold_JJ ._.
*_SYM is_VBZ a_DT wildcard_NN ,_, and_CC denotes_VBZ any_DT one_CD of_IN the_DT observations_NNS ._.
We_PRP observed_VBD that_IN a_DT single_JJ level_NN of_IN belief_NN nesting_JJ -_: beliefs_NNS about_IN the_DT other_JJ ''_'' s_NNS models_NNS -_: was_VBD sufficient_JJ for_IN followership_NN to_TO emerge_VB in_IN the_DT tiger_NN problem_NN ._.
However_RB ,_, the_DT epistemological_JJ requirements_NNS for_IN the_DT emergence_NN of_IN leadership_NN are_VBP more_RBR complex_JJ ._.
For_IN an_DT agent_NN ,_, say_VBP j_NN ,_, to_TO emerge_VB as_IN a_DT leader_NN ,_, followership_NN must_MD first_RB emerge_VB in_IN the_DT other_JJ agent_NN i_FW ._.
As_IN we_PRP mentioned_VBD previously_RB ,_, if_IN i_FW is_VBZ certain_JJ that_IN its_PRP$ preferences_NNS are_VBP identical_JJ to_TO those_DT of_IN j_NN ,_, and_CC believes_VBZ that_IN j_NN has_VBZ a_DT better_JJR sense_NN of_IN hearing_NN ,_, i_FW will_MD follow_VB j_NN ''_'' s_NNS actions_NNS over_IN time_NN ._.
Agent_NNP j_NN emerges_VBZ as_IN a_DT leader_NN if_IN it_PRP believes_VBZ that_IN i_FW will_MD follow_VB it_PRP ,_, which_WDT implies_VBZ that_IN j_NN ''_'' s_NNS belief_NN must_MD be_VB nested_JJ two_CD levels_NNS deep_JJ to_TO enable_VB it_PRP to_TO recognize_VB its_PRP$ leadership_NN role_NN ._.
Realizing_VBG that_DT i_FW will_MD follow_VB presents_NNS j_NN with_IN an_DT opportunity_NN to_TO influence_VB i_FW ''_'' s_VBZ actions_NNS in_IN the_DT benefit_NN of_IN the_DT collective_JJ good_JJ or_CC its_PRP$ self-interest_NN alone_RB ._.
For_IN example_NN ,_, in_IN the_DT tiger_NN problem_NN ,_, let_VB us_PRP consider_VB a_DT setting_NN in_IN which_WDT if_IN both_DT i_FW and_CC j_FW open_VB the_DT correct_JJ door_NN ,_, then_RB each_DT gets_VBZ a_DT payoff_NN of_IN ##_NN that_WDT is_VBZ double_JJ the_DT original_JJ ._.
If_IN j_NN alone_RB selects_VBZ the_DT correct_JJ door_NN ,_, it_PRP gets_VBZ the_DT payoff_NN of_IN ##_NN ._.
On_IN the_DT other_JJ hand_NN ,_, if_IN both_DT agents_NNS pick_VBP the_DT wrong_JJ door_NN ,_, their_PRP$ penalties_NNS are_VBP cut_VBN in_IN half_NN ._.
In_IN this_DT setting_NN ,_, it_PRP is_VBZ in_IN both_CC j_NN ''_'' s_NNS best_JJS interest_NN as_RB well_RB as_IN the_DT collective_JJ betterment_NN for_IN j_NN to_TO use_VB its_PRP$ expertise_NN in_IN selecting_VBG the_DT correct_JJ door_NN ,_, and_CC thus_RB be_VB a_DT good_JJ leader_NN ._.
However_RB ,_, consider_VB a_DT slightly_RB different_JJ problem_NN in_IN which_WDT j_NN gains_NNS from_IN i_FW ''_'' s_VBZ loss_NN and_CC is_VBZ penalized_VBN if_IN i_FW gains_NNS ._.
Specifically_RB ,_, let_VB i_FW ''_'' s_VBZ payoff_NN be_VB subtracted_VBN from_IN j_NN ''_'' s_NNS ,_, indicating_VBG that_IN j_NN is_VBZ antagonistic_JJ toward_IN i_LS -_: if_IN j_NN picks_VBZ the_DT correct_JJ door_NN and_CC i_FW the_DT wrong_JJ one_CD ,_, then_RB i_FW ''_'' s_VBZ loss_NN of_IN ###_CD becomes_VBZ j_NN ''_'' s_NNS gain_NN ._.
Agent_NNP j_NN believes_VBZ that_IN i_FW incorrectly_RB thinks_VBZ that_IN j_NN ''_'' s_NNS preferences_NNS are_VBP those_DT that_WDT promote_VBP the_DT collective_JJ good_JJ and_CC that_IN it_PRP starts_VBZ off_RP by_IN believing_VBG with_IN ##_CD %_NN confidence_NN where_WRB the_DT tiger_NN is_VBZ ._.
Because_IN i_FW believes_VBZ that_IN its_PRP$ preferences_NNS are_VBP similar_JJ to_TO those_DT of_IN j_NN ,_, and_CC that_IN j_NN starts_VBZ by_IN believing_VBG almost_RB surely_RB that_IN one_CD of_IN the_DT two_CD is_VBZ the_DT correct_JJ location_NN -LRB-_-LRB- two_CD level_NN 0_CD models_NNS of_IN j_NN -RRB-_-RRB- ,_, i_FW will_MD start_VB by_IN following_VBG j_NN ''_'' s_NNS actions_NNS ._.
We_PRP show_VBP i_FW ''_'' s_VBZ normative_JJ policy_NN on_IN solving_VBG its_PRP$ singly-nested_JJ I-DID_NN over_IN three_CD time_NN steps_NNS in_IN Fig_NN ._.
#_# -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
The_DT policy_NN demonstrates_VBZ that_IN i_FW will_MD blindly_RB follow_VB j_NN ''_'' s_NNS actions_NNS ._.
Since_IN the_DT tiger_NN persists_VBZ in_IN its_PRP$ original_JJ location_NN with_IN a_DT probability_NN of_IN #_# ._.
##_NN ,_, i_FW will_MD select_VB the_DT same_JJ door_NN again_RB ._.
If_IN j_NN begins_VBZ the_DT game_NN with_IN a_DT ##_CD %_NN probability_NN that_IN the_DT tiger_NN is_VBZ on_IN the_DT right_NN ,_, solving_VBG j_NN ''_'' s_NNS I-DID_JJ nested_JJ two_CD levels_NNS deep_RB ,_, results_VBZ in_IN the_DT policy_NN shown_VBN in_IN Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- ._.
Even_RB though_IN j_NN is_VBZ almost_RB certain_JJ that_IN OL_NN is_VBZ the_DT correct_JJ action_NN ,_, it_PRP will_MD start_VB by_IN selecting_VBG OR_NN ,_, followed_VBN by_IN OL_NN ._.
Agent_NNP j_NN ''_'' s_NNS intention_NN is_VBZ to_TO deceive_VB i_FW who_WP ,_, it_PRP believes_VBZ ,_, will_MD follow_VB j_NN ''_'' s_NNS actions_NNS ,_, so_RB as_IN to_TO gain_VB $_$ ###_CD in_IN the_DT second_JJ time_NN step_NN ,_, which_WDT is_VBZ more_RBR than_IN what_WP j_NN would_MD gain_VB if_IN it_PRP were_VBD to_TO be_VB honest_JJ ._.
Figure_NNP #_# :_: Emergence_NN of_IN deception_NN between_IN agents_NNS in_IN the_DT tiger_NN problem_NN ._.
Behaviors_NNS of_IN interest_NN are_VBP in_IN bold_JJ ._.
*_SYM denotes_VBZ as_IN before_RB ._.
-LRB-_-LRB- a_DT -RRB-_-RRB- Agent_NNP i_FW ''_'' s_VBZ policy_NN demonstrating_NN that_IN it_PRP will_MD blindly_RB follow_VB j_NN ''_'' s_NNS actions_NNS ._.
-LRB-_-LRB- b_NN -RRB-_-RRB- Even_RB though_IN j_NN is_VBZ almost_RB certain_JJ that_IN the_DT tiger_NN is_VBZ on_IN the_DT right_NN ,_, it_PRP will_MD start_VB by_IN selecting_VBG OR_NN ,_, followed_VBN by_IN OL_NN ,_, in_IN order_NN to_TO deceive_VB i_LS ._.
5_CD ._.
#_# Altruism_NNP and_CC Reciprocity_NNP in_IN the_DT Public_NNP Good_JJ Problem_NNP The_DT public_JJ good_JJ -LRB-_-LRB- PG_NN -RRB-_-RRB- problem_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, consists_VBZ of_IN a_DT group_NN of_IN M_NN agents_NNS ,_, each_DT of_IN whom_WP must_MD either_RB contribute_VB some_DT resource_NN to_TO a_DT public_JJ pot_NN or_CC keep_VB it_PRP for_IN themselves_PRP ._.
Since_IN resources_NNS contributed_VBD to_TO the_DT public_JJ pot_NN are_VBP shared_VBN among_IN all_PDT the_DT agents_NNS ,_, they_PRP are_VBP less_RBR valuable_JJ to_TO the_DT agent_NN when_WRB in_IN the_DT public_JJ pot_NN ._.
However_RB ,_, if_IN all_DT agents_NNS choose_VB to_TO contribute_VB their_PRP$ resources_NNS ,_, then_RB the_DT payoff_NN to_TO each_DT agent_NN is_VBZ more_JJR than_IN if_IN no_DT one_NN contributes_VBZ ._.
Since_IN an_DT agent_NN gets_VBZ its_PRP$ share_NN of_IN the_DT public_JJ pot_NN irrespective_RB of_IN whether_IN it_PRP has_VBZ contributed_VBN or_CC not_RB ,_, the_DT dominating_VBG action_NN is_VBZ for_IN each_DT agent_NN to_TO not_RB contribute_VB ,_, and_CC instead_RB free_JJ ride_NN on_IN others_NNS ''_'' contributions_NNS ._.
However_RB ,_, behaviors_NNS of_IN human_JJ players_NNS in_IN empirical_JJ simulations_NNS of_IN the_DT PG_NN problem_NN differ_VBP from_IN the_DT normative_JJ predictions_NNS ._.
The_DT experiments_NNS reveal_VBP that_IN many_JJ players_NNS initially_RB contribute_VBP a_DT large_JJ amount_NN to_TO the_DT public_JJ pot_NN ,_, and_CC continue_VBP to_TO contribute_VB when_WRB the_DT PG_NN problem_NN is_VBZ played_VBN repeatedly_RB ,_, though_IN in_IN decreasing_VBG amounts_NNS -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Many_JJ of_IN these_DT experiments_NNS -LSB-_-LRB- #_# -RSB-_-RRB- report_NN that_IN a_DT small_JJ core_NN group_NN of_IN players_NNS persistently_RB contributes_VBZ to_TO the_DT public_JJ pot_NN even_RB when_WRB all_DT others_NNS are_VBP defecting_VBG ._.
These_DT experiments_NNS also_RB reveal_VBP that_IN players_NNS who_WP persistently_RB contribute_VBP have_VBP altruistic_JJ or_CC reciprocal_JJ preferences_NNS matching_VBG expected_VBN cooperation_NN of_IN others_NNS ._.
For_IN simplicity_NN ,_, we_PRP assume_VBP that_IN the_DT game_NN is_VBZ played_VBN between_IN M_NN =_JJ 2_CD agents_NNS ,_, i_FW and_CC j_FW ._.
Let_VB each_DT agent_NN be_VB initially_RB endowed_JJ with_IN XT_NN amount_NN of_IN resources_NNS ._.
While_IN the_DT classical_JJ PG_NN game_NN formulation_NN permits_VBZ each_DT agent_NN to_TO contribute_VB any_DT quantity_NN of_IN resources_NNS -LRB-_-LRB- XT_NN -RRB-_-RRB- to_TO the_DT public_JJ pot_NN ,_, we_PRP simplify_VBP the_DT action_NN space_NN by_IN allowing_VBG two_CD possible_JJ actions_NNS ._.
Each_DT agent_NN may_MD choose_VB to_TO either_CC contribute_VBP -LRB-_-LRB- C_NN -RRB-_-RRB- a_DT fixed_VBN amount_NN of_IN the_DT resources_NNS ,_, or_CC not_RB contribute_VB ._.
The_DT latter_JJ action_NN is_VBZ deThe_NNP Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD noted_VBD as_IN defect_NN -LRB-_-LRB- D_NN -RRB-_-RRB- ._.
We_PRP assume_VBP that_IN the_DT actions_NNS are_VBP not_RB observable_JJ to_TO others_NNS ._.
The_DT value_NN of_IN resources_NNS in_IN the_DT public_JJ pot_NN is_VBZ discounted_VBN by_IN ci_NN for_IN each_DT agent_NN i_FW ,_, where_WRB ci_NN is_VBZ the_DT marginal_JJ private_JJ return_NN ._.
We_PRP assume_VBP that_IN ci_NN <_JJR #_# so_IN that_IN the_DT agent_NN does_VBZ not_RB benefit_VB enough_JJ that_IN it_PRP contributes_VBZ to_TO the_DT public_JJ pot_NN for_IN private_JJ gain_NN ._.
Simultaneously_RB ,_, ciM_NNP >_JJR #_# ,_, making_VBG collective_JJ contribution_NN pareto_FW optimal_JJ ._.
i_LS /_: j_NN C_NN D_NN C_NN 2ciXT_NN ,_, 2cjXT_NN ciXT_NN cp_NN ,_, XT_NN +_CC cjXT_NN P_NN D_NN XT_NN +_CC ciXT_NN P_NN ,_, cjXT_NN cp_NN XT_NNP ,_, XT_NNP Table_NNP #_# :_: The_DT one-shot_JJ PG_NN game_NN with_IN punishment_NN ._.
In_IN order_NN to_TO encourage_VB contributions_NNS ,_, the_DT contributing_VBG agents_NNS punish_VB free_JJ riders_NNS but_CC incur_VBP a_DT small_JJ cost_NN for_IN administering_VBG the_DT punishment_NN ._.
Let_VB P_NN be_VB the_DT punishment_NN meted_VBD out_RP to_TO the_DT defecting_VBG agent_NN and_CC cp_NN the_DT non-zero_JJ cost_NN of_IN punishing_VBG for_IN the_DT contributing_VBG agent_NN ._.
For_IN simplicity_NN ,_, we_PRP assume_VBP that_IN the_DT cost_NN of_IN punishing_VBG is_VBZ same_JJ for_IN both_CC the_DT agents_NNS ._.
The_DT one-shot_JJ PG_NN game_NN with_IN punishment_NN is_VBZ shown_VBN in_IN Table_NNP ._.
#_# ._.
Let_VB ci_NN =_JJ cj_NN ,_, cp_NN >_JJR #_# ,_, and_CC if_IN P_NN >_JJR XT_NNP ciXT_NNP ,_, then_RB defection_NN is_VBZ no_RB longer_RB a_DT dominating_VBG action_NN ._.
If_IN P_NN <_JJR XT_CD ciXT_NN ,_, then_RB defection_NN is_VBZ the_DT dominating_VBG action_NN for_IN both_DT ._.
If_IN P_NN =_JJ XT_NN ciXT_NN ,_, then_RB the_DT game_NN is_VBZ not_RB dominance-solvable_JJ ._.
Figure_NNP #_# :_: -LRB-_-LRB- a_LS -RRB-_-RRB- Level_NN #_# I-ID_NN of_IN agent_NN i_FW ,_, -LRB-_-LRB- b_LS -RRB-_-RRB- level_NN #_# IDs_NNS of_IN agent_NN j_NN with_IN decision_NN nodes_NNS mapped_VBN to_TO the_DT chance_NN nodes_NNS ,_, A1_NN j_NN and_CC A2_NN j_NN ,_, in_IN -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
We_PRP formulate_VBP a_DT sequential_JJ version_NN of_IN the_DT PG_NN problem_NN with_IN punishment_NN from_IN the_DT perspective_NN of_IN agent_NN i_FW ._.
Though_NNP in_IN the_DT repeated_VBN PG_NN game_NN ,_, the_DT quantity_NN in_IN the_DT public_JJ pot_NN is_VBZ revealed_VBN to_TO all_PDT the_DT agents_NNS after_IN each_DT round_NN of_IN actions_NNS ,_, we_PRP assume_VBP in_IN our_PRP$ formulation_NN that_IN it_PRP is_VBZ hidden_VBN from_IN the_DT agents_NNS ._.
Each_DT agent_NN may_MD contribute_VB a_DT fixed_JJ amount_NN ,_, xc_NN ,_, or_CC defect_NN ._.
An_DT agent_NN on_IN performing_VBG an_DT action_NN receives_VBZ an_DT observation_NN of_IN plenty_NN -LRB-_-LRB- PY_NN -RRB-_-RRB- or_CC meager_JJ -LRB-_-LRB- MR_NN -RRB-_-RRB- symbolizing_VBG the_DT state_NN of_IN the_DT public_JJ pot_NN ._.
Notice_NNP that_IN the_DT observations_NNS are_VBP also_RB indirectly_RB indicative_JJ of_IN agent_NN j_NN ''_'' s_NNS actions_NNS because_IN the_DT state_NN of_IN the_DT public_JJ pot_NN is_VBZ influenced_VBN by_IN them_PRP ._.
The_DT amount_NN of_IN resources_NNS in_IN agent_NN i_FW ''_'' s_VBZ private_JJ pot_NN ,_, is_VBZ perfectly_RB observable_JJ to_TO i_LS ._.
The_DT payoffs_NNS are_VBP analogous_JJ to_TO Table_NNP ._.
#_# ._.
Borrowing_NN from_IN the_DT empirical_JJ investigations_NNS of_IN the_DT PG_NN problem_NN -LSB-_-LRB- #_# -RSB-_-RRB- ,_, we_PRP construct_VBP level_NN #_# IDs_NNS for_IN j_NN that_IN model_NN altruistic_JJ and_CC non-altruistic_JJ types_NNS -LRB-_-LRB- Fig_NN ._.
#_# -LRB-_-LRB- b_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Specifically_RB ,_, our_PRP$ altruistic_JJ agent_NN has_VBZ a_DT high_JJ marginal_JJ private_JJ return_NN -LRB-_-LRB- cj_NN is_VBZ close_JJ to_TO #_# -RRB-_-RRB- and_CC does_VBZ not_RB punish_VB others_NNS who_WP defect_NN ._.
Let_VB xc_NN =_JJ #_# and_CC the_DT level_NN #_# agent_NN be_VB punished_VBN half_PDT the_DT times_NNS it_PRP defects_VBZ ._.
With_IN one_CD action_NN remaining_VBG ,_, both_DT types_NNS of_IN agents_NNS choose_VB to_TO contribute_VB to_TO avoid_VB being_VBG punished_VBN ._.
With_IN two_CD actions_NNS to_TO go_VB ,_, the_DT altruistic_JJ type_NN chooses_VBZ to_TO contribute_VB ,_, while_IN the_DT other_JJ defects_NNS ._.
This_DT is_VBZ because_IN cj_NN for_IN the_DT altruistic_JJ type_NN is_VBZ close_JJ to_TO #_# ,_, thus_RB the_DT expected_JJ punishment_NN ,_, #_# ._.
5P_NN >_JJR -LRB-_-LRB- #_# cj_NN -RRB-_-RRB- ,_, which_WDT the_DT altruistic_JJ type_NN avoids_VBZ ._.
Because_IN cj_NN for_IN the_DT non-altruistic_JJ type_NN is_VBZ less_RBR ,_, it_PRP prefers_VBZ not_RB to_TO contribute_VB ._.
With_IN three_CD steps_NNS to_TO go_VB ,_, the_DT altruistic_JJ agent_NN contributes_VBZ to_TO avoid_VB punishment_NN -LRB-_-LRB- #_# ._.
5P_NN >_JJR #_# -LRB-_-LRB- #_# cj_NN -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC the_DT non-altruistic_JJ type_NN defects_NNS ._.
For_IN greater_JJR than_IN three_CD steps_NNS ,_, while_IN the_DT altruistic_JJ agent_NN continues_VBZ to_TO contribute_VB to_TO the_DT public_JJ pot_NN depending_VBG on_IN how_WRB close_JJ its_PRP$ marginal_JJ private_JJ return_NN is_VBZ to_TO #_# ,_, the_DT non-altruistic_JJ type_NN prescribes_VBZ defection_NN ._.
We_PRP analyzed_VBD the_DT decisions_NNS of_IN an_DT altruistic_JJ agent_NN i_FW modeled_VBN using_VBG a_DT level_NN #_# I-DID_NN expanded_VBN over_IN #_# time_NN steps_NNS ._.
i_LS ascribes_VBZ the_DT two_CD level_NN 0_CD models_NNS ,_, mentioned_VBN previously_RB ,_, to_TO j_NN ._.
If_IN i_FW believes_VBZ with_IN a_DT probability_NN #_# that_WDT j_NN is_VBZ altruistic_JJ ,_, i_FW chooses_VBZ to_TO contribute_VB for_IN each_DT of_IN the_DT three_CD steps_NNS ._.
This_DT behavior_NN persists_VBZ when_WRB i_FW is_VBZ unaware_JJ of_IN whether_IN j_NN is_VBZ altruistic_JJ -LRB-_-LRB- Fig_NN ._.
##_NN -LRB-_-LRB- a_DT -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC when_WRB i_FW assigns_VBZ a_DT high_JJ probability_NN to_TO j_NN being_VBG the_DT non-altruistic_JJ type_NN ._.
However_RB ,_, when_WRB i_FW believes_VBZ with_IN a_DT probability_NN #_# that_WDT j_NN is_VBZ non-altruistic_JJ and_CC will_MD thus_RB surely_RB defect_NN ,_, i_FW chooses_VBZ to_TO defect_NN to_TO avoid_VB being_VBG punished_VBN and_CC because_IN its_PRP$ marginal_JJ private_JJ return_NN is_VBZ less_JJR than_IN #_# ._.
These_DT results_NNS demonstrate_VBP that_IN the_DT behavior_NN of_IN our_PRP$ altruistic_JJ type_NN resembles_VBZ that_IN found_VBN experimentally_RB ._.
The_DT non-altruistic_JJ level_NN #_# agent_NN chooses_VBZ to_TO defect_NN regardless_RB of_IN how_WRB likely_JJ it_PRP believes_VBZ the_DT other_JJ agent_NN to_TO be_VB altruistic_JJ ._.
We_PRP analyzed_VBD the_DT behavior_NN of_IN a_DT reciprocal_JJ agent_NN type_NN that_WDT matches_VBZ expected_VBN cooperation_NN or_CC defection_NN ._.
The_DT reciprocal_JJ type_NN ''_'' s_NNS marginal_JJ private_JJ return_NN is_VBZ similar_JJ to_TO that_DT of_IN the_DT non-altruistic_JJ type_NN ,_, however_RB ,_, it_PRP obtains_VBZ a_DT greater_JJR payoff_NN when_WRB its_PRP$ action_NN is_VBZ similar_JJ to_TO that_DT of_IN the_DT other_JJ ._.
We_PRP consider_VBP the_DT case_NN when_WRB the_DT reciprocal_JJ agent_NN i_FW is_VBZ unsure_JJ of_IN whether_IN j_NN is_VBZ altruistic_JJ and_CC believes_VBZ that_IN the_DT public_JJ pot_NN is_VBZ likely_JJ to_TO be_VB half_JJ full_JJ ._.
For_IN this_DT prior_JJ belief_NN ,_, i_FW chooses_VBZ to_TO defect_NN ._.
On_IN receiving_VBG an_DT observation_NN of_IN plenty_NN ,_, i_FW decides_VBZ to_TO contribute_VB ,_, while_IN an_DT observation_NN of_IN meager_JJ makes_VBZ it_PRP defect_NN -LRB-_-LRB- Fig_NN ._.
##_NN -LRB-_-LRB- b_NN -RRB-_-RRB- -RRB-_-RRB- ._.
This_DT is_VBZ because_IN an_DT observation_NN of_IN plenty_JJ signals_NNS that_IN the_DT pot_NN is_VBZ likely_JJ to_TO be_VB greater_JJR than_IN half_DT full_JJ ,_, which_WDT results_VBZ from_IN j_NN ''_'' s_NNS action_NN to_TO contribute_VB ._.
Thus_RB ,_, among_IN the_DT two_CD models_NNS ascribed_VBN to_TO j_NN ,_, its_PRP$ type_NN is_VBZ likely_JJ to_TO be_VB altruistic_JJ making_VBG it_PRP likely_JJ that_IN j_NN will_MD contribute_VB again_RB in_IN the_DT next_JJ time_NN step_NN ._.
Agent_NNP i_FW therefore_RB chooses_VBZ to_TO contribute_VB to_TO reciprocate_VB j_NN ''_'' s_NNS action_NN ._.
An_DT analogous_JJ reasoning_NN leads_VBZ i_FW to_TO defect_NN when_WRB it_PRP observes_VBZ a_DT meager_JJ pot_NN ._.
With_IN one_CD action_NN to_TO go_VB ,_, i_FW believing_VBG that_IN j_NN contributes_VBZ ,_, will_MD choose_VB to_TO contribute_VB too_RB to_TO avoid_VB punishment_NN regardless_RB of_IN its_PRP$ observations_NNS ._.
Figure_NNP ##_NNP :_: -LRB-_-LRB- a_LS -RRB-_-RRB- An_DT altruistic_JJ level_NN #_# agent_NN always_RB contributes_VBZ ._.
-LRB-_-LRB- b_NN -RRB-_-RRB- A_DT reciprocal_JJ agent_NN i_FW starts_VBZ off_RP by_IN defecting_VBG followed_VBN by_IN choosing_VBG to_TO contribute_VB or_CC defect_NN based_VBN on_IN its_PRP$ observation_NN of_IN plenty_NN -LRB-_-LRB- indicating_VBG that_IN j_NN is_VBZ likely_JJ altruistic_JJ -RRB-_-RRB- or_CC meager_JJ -LRB-_-LRB- j_NN is_VBZ non-altruistic_JJ -RRB-_-RRB- ._.
5_CD ._.
#_# Strategies_NNS in_IN Two-Player_NNP Poker_NNP Poker_NNP is_VBZ a_DT popular_JJ zero_CD sum_NN card_NN game_NN that_WDT has_VBZ received_VBN much_JJ attention_NN among_IN the_DT AI_NNP research_NN community_NN as_IN a_DT testbed_JJ -LSB-_-LRB- #_# -RSB-_-RRB- ._.
Poker_NNP is_VBZ played_VBN among_IN M_NN #_# players_NNS in_IN which_WDT each_DT player_NN receives_VBZ a_DT hand_NN of_IN cards_NNS from_IN a_DT deck_NN ._.
While_IN several_JJ flavors_NNS of_IN Poker_NNP with_IN varying_VBG complexity_NN exist_VBP ,_, we_PRP consider_VBP a_DT simple_JJ version_NN in_IN which_WDT each_DT player_NN has_VBZ three_CD plys_NNS during_IN which_WDT the_DT player_NN may_MD either_RB exchange_VB a_DT card_NN -LRB-_-LRB- E_NN -RRB-_-RRB- ,_, keep_VB the_DT existing_VBG hand_NN -LRB-_-LRB- K_NN -RRB-_-RRB- ,_, fold_NN -LRB-_-LRB- F_NN -RRB-_-RRB- and_CC withdraw_VB from_IN the_DT game_NN ,_, or_CC call_NN -LRB-_-LRB- C_NN -RRB-_-RRB- ,_, requiring_VBG all_DT players_NNS to_TO show_VB their_PRP$ hands_NNS ._.
To_TO keep_VB matters_NNS simple_JJ ,_, let_VB M_NN =_JJ #_# ,_, and_CC each_DT player_NN receive_VB a_DT hand_NN consisting_VBG of_IN a_DT single_JJ card_NN drawn_VBN from_IN the_DT same_JJ suit_NN ._.
Thus_RB ,_, during_IN a_DT showdown_NN ,_, the_DT player_NN who_WP has_VBZ the_DT numerically_RB larger_JJR card_NN -LRB-_-LRB- #_# is_VBZ the_DT lowest_JJS ,_, ace_NN is_VBZ the_DT highest_JJS -RRB-_-RRB- wins_VBZ the_DT pot_NN ._.
During_IN an_DT exchange_NN of_IN cards_NNS ,_, the_DT discarded_VBN card_NN is_VBZ placed_VBN either_CC in_IN the_DT L_NN pile_NN ,_, indicating_VBG to_TO the_DT other_JJ agent_NN that_IN it_PRP was_VBD a_DT low_JJ numbered_VBD card_NN less_JJR than_IN #_# ,_, or_CC in_IN the_DT 820_CD The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- H_NN pile_NN ,_, indicating_VBG that_IN the_DT card_NN had_VBD a_DT rank_NN greater_JJR than_IN or_CC equal_JJ to_TO 8_CD ._.
Notice_NNP that_IN ,_, for_IN example_NN ,_, if_IN a_DT lower_JJR numbered_VBD card_NN is_VBZ discarded_VBN ,_, the_DT probability_NN of_IN receiving_VBG a_DT low_JJ card_NN in_IN exchange_NN is_VBZ now_RB reduced_VBN ._.
We_PRP show_VBP the_DT level_NN #_# I-ID_NN for_IN the_DT simplified_VBN two-player_JJ Poker_NNP in_IN Fig_NNP ._.
##_NN ._.
We_PRP considered_VBD two_CD models_NNS -LRB-_-LRB- personality_NN types_NNS -RRB-_-RRB- of_IN agent_NN j_NN ._.
The_DT conservative_JJ type_NN believes_VBZ that_IN it_PRP is_VBZ likely_JJ that_IN its_PRP$ opponent_NN has_VBZ a_DT high_JJ numbered_VBD card_NN in_IN its_PRP$ hand_NN ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT aggressive_JJ agent_NN j_NN believes_VBZ with_IN a_DT high_JJ probability_NN that_IN its_PRP$ opponent_NN has_VBZ a_DT lower_JJR numbered_VBD card_NN ._.
Thus_RB ,_, the_DT two_CD types_NNS differ_VBP in_IN their_PRP$ beliefs_NNS over_IN their_PRP$ opponent_NN ''_'' s_NNS hand_NN ._.
In_IN both_DT these_DT level_NN #_# models_NNS ,_, the_DT opponent_NN is_VBZ assumed_VBN to_TO perform_VB its_PRP$ actions_NNS following_VBG a_DT fixed_VBN ,_, uniform_JJ distribution_NN ._.
With_IN three_CD actions_NNS to_TO go_VB ,_, regardless_RB of_IN its_PRP$ hand_NN -LRB-_-LRB- unless_IN it_PRP is_VBZ an_DT ace_NN -RRB-_-RRB- ,_, the_DT aggressive_JJ agent_NN chooses_VBZ to_TO exchange_VB its_PRP$ card_NN ,_, with_IN the_DT intent_NN of_IN improving_VBG on_IN its_PRP$ current_JJ hand_NN ._.
This_DT is_VBZ because_IN it_PRP believes_VBZ the_DT other_JJ to_TO have_VB a_DT low_JJ card_NN ,_, which_WDT improves_VBZ its_PRP$ chances_NNS of_IN getting_VBG a_DT high_JJ card_NN during_IN the_DT exchange_NN ._.
The_DT conservative_JJ agent_NN chooses_VBZ to_TO keep_VB its_PRP$ card_NN ,_, no_DT matter_NN its_PRP$ hand_NN because_IN its_PRP$ chances_NNS of_IN getting_VBG a_DT high_JJ card_NN are_VBP slim_JJ as_IN it_PRP believes_VBZ that_IN its_PRP$ opponent_NN has_VBZ one_CD ._.
Figure_NNP ##_NNP :_: -LRB-_-LRB- a_LS -RRB-_-RRB- Level_NN #_# I-ID_NN of_IN agent_NN i_FW ._.
The_DT observation_NN reveals_VBZ information_NN about_IN j_NN ''_'' s_NNS hand_NN of_IN the_DT previous_JJ time_NN step_NN ,_, -LRB-_-LRB- b_LS -RRB-_-RRB- level_NN #_# IDs_NNS of_IN agent_NN j_NN whose_WP$ decision_NN nodes_NNS are_VBP mapped_VBN to_TO the_DT chance_NN nodes_NNS ,_, A1_NN j_NN ,_, A2_NN j_NN ,_, in_IN -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
The_DT policy_NN of_IN a_DT level_NN #_# agent_NN i_FW who_WP believes_VBZ that_IN each_DT card_NN except_IN its_PRP$ own_JJ has_VBZ an_DT equal_JJ likelihood_NN of_IN being_VBG in_IN j_NN ''_'' s_NNS hand_NN -LRB-_-LRB- neutral_JJ personality_NN type_NN -RRB-_-RRB- and_CC j_NN could_MD be_VB either_CC an_DT aggressive_JJ or_CC conservative_JJ type_NN ,_, is_VBZ shown_VBN in_IN Fig_NN ._.
##_NN ._.
i_FW ''_'' s_VBZ own_JJ hand_NN contains_VBZ the_DT card_NN numbered_VBD #_# ._.
The_DT agent_NN starts_VBZ by_IN keeping_VBG its_PRP$ card_NN ._.
On_IN seeing_VBG that_IN j_NN did_VBD not_RB exchange_VB a_DT card_NN -LRB-_-LRB- N_NN -RRB-_-RRB- ,_, i_FW believes_VBZ with_IN probability_NN #_# that_WDT j_NN is_VBZ conservative_JJ and_CC hence_RB will_MD keep_VB its_PRP$ cards_NNS ._.
i_LS responds_VBZ by_IN either_CC keeping_VBG its_PRP$ card_NN or_CC exchanging_VBG it_PRP because_IN j_NN is_VBZ equally_RB likely_JJ to_TO have_VB a_DT lower_JJR or_CC higher_JJR card_NN ._.
If_IN i_FW observes_VBZ that_IN j_NN discarded_VBD its_PRP$ card_NN into_IN the_DT L_NN or_CC H_NN pile_NN ,_, i_FW believes_VBZ that_IN j_NN is_VBZ aggressive_JJ ._.
On_IN observing_VBG L_NN ,_, i_FW realizes_VBZ that_IN j_NN had_VBD a_DT low_JJ card_NN ,_, and_CC is_VBZ likely_JJ to_TO have_VB a_DT high_JJ card_NN after_IN its_PRP$ exchange_NN ._.
Because_IN the_DT probability_NN of_IN receiving_VBG a_DT low_JJ card_NN is_VBZ high_JJ now_RB ,_, i_FW chooses_VBZ to_TO keep_VB its_PRP$ card_NN ._.
On_IN observing_VBG H_NN ,_, believing_VBG that_IN the_DT probability_NN of_IN receiving_VBG a_DT high_JJ numbered_VBD card_NN is_VBZ high_JJ ,_, i_FW chooses_VBZ to_TO exchange_VB its_PRP$ card_NN ._.
In_IN the_DT final_JJ step_NN ,_, i_FW chooses_VBZ to_TO call_VB regardless_RB of_IN its_PRP$ observation_NN history_NN because_IN its_PRP$ belief_NN that_IN j_NN has_VBZ a_DT higher_JJR card_NN is_VBZ not_RB sufficiently_RB high_JJ to_TO conclude_VB that_IN its_PRP$ better_JJR to_TO fold_VB and_CC relinquish_VB the_DT payoff_NN ._.
This_DT is_VBZ partly_RB due_JJ to_TO the_DT fact_NN that_IN an_DT observation_NN of_IN ,_, say_VB ,_, L_NN resets_VBZ the_DT agent_NN i_FW ''_'' s_VBZ previous_JJ time_NN step_NN beliefs_NNS over_IN j_NN ''_'' s_NNS hand_NN to_TO the_DT low_JJ numbered_VBD cards_NNS only_RB ._.
6_CD ._.
DISCUSSION_NN We_PRP showed_VBD how_WRB DIDs_NNS may_MD be_VB extended_VBN to_TO I-DIDs_NNS that_WDT enable_VBP online_JJ sequential_JJ decision-making_NN in_IN uncertain_JJ multiagent_JJ settings_NNS ._.
Our_PRP$ graphical_JJ representation_NN of_IN I-DIDs_NN improves_VBZ on_IN the_DT previous_JJ Figure_NN ##_NN :_: A_DT level_NN #_# agent_NN i_FW ''_'' s_VBZ three_CD step_NN policy_NN in_IN the_DT Poker_NNP problem_NN ._.
i_LS starts_VBZ by_IN believing_VBG that_IN j_NN is_VBZ equally_RB likely_JJ to_TO be_VB aggressive_JJ or_CC conservative_JJ and_CC could_MD have_VB any_DT card_NN in_IN its_PRP$ hand_NN with_IN equal_JJ probability_NN ._.
work_NN significantly_RB by_IN being_VBG more_RBR transparent_JJ ,_, semantically_RB clear_JJ ,_, and_CC capable_JJ of_IN being_VBG solved_VBN using_VBG standard_JJ algorithms_NNS that_WDT target_VBP DIDs_NNS ._.
I-DIDs_NNS extend_VBP NIDs_NNS to_TO allow_VB sequential_JJ decision-making_NN over_IN multiple_JJ time_NN steps_NNS in_IN the_DT presence_NN of_IN other_JJ interacting_VBG agents_NNS ._.
I-DIDs_NNS may_MD be_VB seen_VBN as_IN concise_JJ graphical_JJ representations_NNS for_IN IPOMDPs_NNS providing_VBG a_DT way_NN to_TO exploit_VB problem_NN structure_NN and_CC carry_VB out_RP online_JJ decision-making_NN as_IN the_DT agent_NN acts_VBZ and_CC observes_VBZ given_VBN its_PRP$ prior_JJ beliefs_NNS ._.
We_PRP are_VBP currently_RB investigating_VBG ways_NNS to_TO solve_VB I-DIDs_JJ approximately_RB with_IN provable_JJ bounds_NNS on_IN the_DT solution_NN quality_NN ._.
Acknowledgment_NNP :_: We_PRP thank_VBP Piotr_NNP Gmytrasiewicz_NNP for_IN some_DT useful_JJ discussions_NNS related_VBN to_TO this_DT work_NN ._.
The_DT first_JJ author_NN would_MD like_VB to_TO acknowledge_VB the_DT support_NN of_IN a_DT UGARF_NNP grant_NN ._.
7_CD ._.
REFERENCES_NNS -LSB-_-LRB- #_# -RSB-_-RRB- R_NN ._.
J_NN ._.
Aumann_NNP ._.
Interactive_JJ epistemology_NN i_FW :_: Knowledge_NN ._.
International_NNP Journal_NNP of_IN Game_NNP Theory_NNP ,_, ##_CD :_: 263-300_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Billings_NNS ,_, A_DT ._.
Davidson_NNP ,_, J_NNP ._.
Schaeffer_NNP ,_, and_CC D_NN ._.
Szafron_NNP ._.
The_DT challenge_NN of_IN poker_NN ._.
AIJ_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- A_DT ._.
Brandenburger_NNP and_CC E_NNP ._.
Dekel_NNP ._.
Hierarchies_NNS of_IN beliefs_NNS and_CC common_JJ knowledge_NN ._.
Journal_NNP of_IN Economic_NNP Theory_NNP ,_, ##_CD :_: 189-198_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- C_NN ._.
Camerer_NNP ._.
Behavioral_JJ Game_NN Theory_NNP :_: Experiments_NNS in_IN Strategic_NNP Interaction_NN ._.
Princeton_NNP University_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- E_NN ._.
Fehr_NNP and_CC S_NN ._.
Gachter_NNP ._.
Cooperation_NN and_CC punishment_NN in_IN public_JJ goods_NNS experiments_NNS ._.
American_JJ Economic_NNP Review_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 980-994_CD ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Fudenberg_NNP and_CC D_NNP ._.
K_NN ._.
Levine_NNP ._.
The_DT Theory_NNP of_IN Learning_NNP in_IN Games_NNPS ._.
MIT_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- D_NN ._.
Fudenberg_NNP and_CC J_NNP ._.
Tirole_NNP ._.
Game_NNP Theory_NNP ._.
MIT_NNP Press_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- Y_NN ._.
Gal_NNP and_CC A_NNP ._.
Pfeffer_NNP ._.
A_DT language_NN for_IN modeling_NN agent_NN ''_'' s_NNS decision-making_JJ processes_NNS in_IN games_NNS ._.
In_IN AAMAS_NNP ,_, ####_CD ._.
-LSB-_-LRB- #_# -RSB-_-RRB- P_NN ._.
Gmytrasiewicz_NNP and_CC P_NN ._.
Doshi_NNP ._.
A_DT framework_NN for_IN sequential_JJ planning_NN in_IN multiagent_JJ settings_NNS ._.
JAIR_NNP ,_, ##_CD :_: 49-79_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- P_NN ._.
Gmytrasiewicz_NNP and_CC E_NNP ._.
Durfee_NNP ._.
Rational_JJ coordination_NN in_IN multi-agent_JJ environments_NNS ._.
JAAMAS_NNP ,_, #_# -LRB-_-LRB- #_# -RRB-_-RRB- :_: 319-350_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- J_NN ._.
C_NN ._.
Harsanyi_NNP ._.
Games_NNPS with_IN incomplete_JJ information_NN played_VBN by_IN bayesian_JJ players_NNS ._.
Management_NNP Science_NNP ,_, ##_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 159-182_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- R_NN ._.
A_DT ._.
Howard_NNP and_CC J_NNP ._.
E_NN ._.
Matheson_NNP ._.
Influence_NN diagrams_NNS ._.
In_IN R_NN ._.
A_DT ._.
Howard_NNP and_CC J_NNP ._.
E_NN ._.
Matheson_NNP ,_, editors_NNS ,_, The_DT Principles_NNS and_CC Applications_NNS of_IN Decision_NN Analysis_NN ._.
Strategic_NNP Decisions_NNS Group_NNP ,_, Menlo_NNP Park_NNP ,_, CA_NNP 94025_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- L_NN ._.
Kaelbling_NNP ,_, M_NN ._.
Littman_NNP ,_, and_CC A_NN ._.
Cassandra_NNP ._.
Planning_NNP and_CC acting_VBG in_IN partially_RB observable_JJ stochastic_JJ domains_NNS ._.
Artificial_JJ Intelligence_NNP Journal_NNP ,_, #_# ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
Koller_NNP and_CC B_NNP ._.
Milch_NNP ._.
Multi-agent_JJ influence_NN diagrams_NNS for_IN representing_VBG and_CC solving_VBG games_NNS ._.
In_IN IJCAI_NNP ,_, pages_NNS 1027-1034_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- K_NN ._.
Polich_NNP and_CC P_NN ._.
Gmytrasiewicz_NNP ._.
Interactive_JJ dynamic_JJ influence_NN diagrams_NNS ._.
In_IN GTDT_NNP Workshop_NNP ,_, AAMAS_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- B_NN ._.
Rathnas_NNP ._.
,_, P_NN ._.
Doshi_NNP ,_, and_CC P_NN ._.
J_NN ._.
Gmytrasiewicz_NNP ._.
Exact_JJ solutions_NNS to_TO interactive_JJ pomdps_NNS using_VBG behavioral_JJ equivalence_JJ ._.
In_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNPS Conference_NN -LRB-_-LRB- AAMAS_NN -RRB-_-RRB- ,_, ####_NN ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- S_NN ._.
Russell_NNP and_CC P_NN ._.
Norvig_NNP ._.
Artificial_JJ Intelligence_NNP :_: A_NNP Modern_NNP Approach_NNP -LRB-_-LRB- Second_NNP Edition_NNP -RRB-_-RRB- ._.
Prentice_NNP Hall_NNP ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- R_NN ._.
D_NN ._.
Shachter_NNP ._.
Evaluating_VBG influence_NN diagrams_NNS ._.
Operations_NNP Research_NNP ,_, 34_CD -LRB-_-LRB- #_# -RRB-_-RRB- :_: 871-882_CD ,_, ####_CD ._.
-LSB-_-LRB- ##_NN -RSB-_-RRB- D_NN ._.
Suryadi_NNP and_CC P_NN ._.
Gmytrasiewicz_NNP ._.
Learning_NNP models_NNS of_IN other_JJ agents_NNS using_VBG influence_NN diagrams_NNS ._.
In_IN UM_NNP ,_, ####_CD ._.
The_DT Sixth_NNP Intl_NNP ._.
Joint_NNP Conf_NNP ._.
on_IN Autonomous_NNP Agents_NNPS and_CC Multi-Agent_NNP Systems_NNP -LRB-_-LRB- AAMAS_NNP ##_CD -RRB-_-RRB- ###_CD
